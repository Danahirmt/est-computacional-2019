[
["index.html", "Estadística Computacional Información del curso", " Estadística Computacional María Teresa Ortiz Información del curso Notas del curso Estadística Computacional de los programas de maestría en Ciencia de Datos y en Computación del ITAM. Las notas fueron desarrolladas en 2014 por Teresa Ortiz quien las actualiza anualmente. En caso de encontrar errores o tener sugerencias del material se agradece la propuesta de correcciones mediante pull requests. Ligas Notas: https://tereom.github.io/est-computacional-2019/ Correo: teresa.ortiz.mancera@gmail.com GitHub: https://github.com/tereom/est-computacional-2019 Agradecimientos Se agradecen las contriubuciones a estas notas de @felipegonzalez y @mkokotchikova. Este trabajo está bajo una Licencia Creative Commons Atribución 4.0 Internacional. "],
["temario.html", "Temario", " Temario Manipulación y visualización de datos Principios de visualización. Reestructura y manipulación de datos. Temas selectos de programación en R: programación funcional, evaluación no estándar. Referencias: Tufte (2006), Cleveland (1993), Wickham and Grolemund (2017), Wickham (2019). Inferencia y remuestreo Repaso de probabilidad. Muestreo y probabilidad. Inferencia. El principio del plug-in. Bootstrap Cálculo de errores estándar e intervalos de confianza. Estructuras de datos complejos. Introducción a modelos probabilísticos. Referencias: Ross (1998), Efron and Tibshirani (1993). Modelos de probabilidad y simulación Variables aleatorias y modelos probabilísticos. Familias importantes: discretas y continuas. Teoría básica de simulación El generador uniforme de números aleatorios. El método de la transformación inversa. Simulación de variables aleatorias discretas con soporte finito. Otras variables aleatorias. Simulación para modelos gráficos Modelos probabilíticos gráficos. Simulación (e.g. ANOVA, regresión simple). Inferencia paramétrica y remuestreo Modelos paramétricos. Bootsrap paramétrico. Inferencia de gráficas Referencias: Gelman and Hill (2007). Métodos computacionales e inferencia Bayesiana Inferencia bayesiana. Métodos diretos Familias conjugadas. Aproximación por cuadrícula. Aceptación y rechazo. MCMC Cadenas de Markov. Metropolis-Hastings. Muestreador de Gibbs. Monte Carlo Hamiltoniano. Diagnósticos de convergencia. Referencias: Kruschke (2015), Gelman et al. (2013). Calificación Tareas 20% (se envían por correo con título EstComp-TareaXX). Exámen parcial (proyecto y exámen en clase) 40%. Examen final 40%. Software R: https://www.r-project.org RStudio: https://www.rstudio.com Stan: http://mc-stan.org Otros recursos Socrative (Room ESTCOMP): Para encuestas y ejercicios en clase. Lista de correos: Suscribete si quieres recibir noticias del curso o una suscripción a DataCamp (6 meses con acceso a todos los recursos de forma gratuita). Noticias Los dos premios más importantes en estadística se entregaron en 2019 a Hadley Whickham y a Bradley Efron, gran parte de nuestro curso se desarrolla en torno a las contribuciones de estos dos estadísticos: Hadley Wickham cuyos paquetes, libros y artículos son los recursos esenciales para la primera parte del curso, ganó en 2019 el reconocido premio COPSS: “Por la importancia de su trabajo en el computo estadístico, visualización, gráficas y análisis de datos; por desarrollar e implementar una extensa ifraestructura computacional para el análisis de datos a través del software R; por hacer el pensamiento estadístico y el cómputo accesible a una gran audiencia; y por realzar el importante papel de la estadística entre los científicos de datos.” (2019 Presidents’ Award) Bradley Efron creador del bootsrtap, que estudiaremos como segunda sección del curso, fue seleccionado en 2018 para recibir el premio internacional en estadística como reconocimiento al bootstrap, un método que desarrolló en 1977 para calcular incertidumbre en resultados científicos y que ha tenido un impacto extraordinario en muchos ámbitos. “A pesar de que la estadística no ofrece una píldora mágica para la investigación científica cuantitativa, el bootstrap es el mejor analgésico jamás producido” (Xiao-Li Meng, proff. at Harvard University.) Referencias "],
["principios-visualizacion.html", "Sección 1 Principios visualización", " Sección 1 Principios visualización “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey El cuarteto de Ascombe En 1971 un estadístico llamado Frank Anscombe (fundador del departamento de Estadística de la Universidad de Yale) publicó cuatro conjuntos de datos, cada uno consiste de 11 observaciones y tienen las mismas propiedades estadísticas. Sin embargo, cuando analizamos los datos de manera gráfica en un histograma encontramos rápidamente que los conjuntos de datos son muy distintos. Media de \\(x\\): 9 Varianza muestral de \\(x\\): 11 Media de \\(y\\): 7.50 Varianza muestral de \\(y\\): 4.12 Correlación entre \\(x\\) y \\(y\\): 0.816 Línea de regresión lineal: \\(y = 3.00 + 0.500x\\) En la gráfica del primer conjunto de datos, se ven datos como los que se tendrían en una relación lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gráfica (arriba a la derecha) muestra unos datos que tienen una asociación pero definitivamente no es lineal. En la tercera gráfica (abajo a la izquierda) están puntos alineados perfectamente en una línea recta, excepto por uno de ellos. En la última gráfica podemos ver un ejemplo en el cual basta tener una observación atípica para que se produzca un coeficiente de correlación alto aún cuando en realidad no existe una asociación lineal entre las dos variables. El cuarteto de Ascombe inspiró una técnica para crear datos que comparten las propiedades estadísticas al igual que en el cuarteto, pero que producen gráficas muy distintas (Matejka, Fitzmaurice). "],
["introduccion.html", "1.1 Introducción", " 1.1 Introducción La visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo o ayudar a una persona “que no entiende mucho” a entender ideas complejas. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. El siguiente ejemplo de (Tufte 2006), ilustra claramente la diferencia entre estos dos enfoques. A la izquierda están gráficas (más o menos típicas de Powerpoint) basadas en la filosofía de simplificar, de intentar no “ahogar” al lector con datos. El resultado es una colección incoherente, de bajo contenido, que no tiene mucho qué decir y que es, “indeferente al contenido y la evidencia”. A la derecha está una variación del rediseño de Tufte en forma de tabla, que en este caso particular es una manera eficiente de mostrar claramente los patrones que hay en este conjunto simple de datos. ¿Qué principios son los que soportan la efectividad de esta tabla sobre la gráfica de la derecha? Veremos que hay dos conjuntos de principios importantes: unos relacionados con el diseño y otros con la naturaleza del análisis de datos, independientemente del método de visualización. Visualización de datos en la estadística El estándar científico para contestar preguntas o tomar decisiones es uno que se basa en el análisis de datos: para contestar preguntas o tomar decisiones es necesario, en primer lugar, reunir todos los datos disponibles que puedan contener o sugerir alguna guía para entender mejor la pregunta o la decisión a la que nos enfrentamos. Esta recopilación de datos -que pueden ser cualitativos, cuantitativos, o una mezcla de los dos, debe entonces ser analizada para extraer información relevante para nuestro problema. En análisis de datos existen dos distintos tipos de trabajo: El trabajo exploratorio o de detective: ¿cuáles son los aspectos importantes de estos datos? ¿qué indicaciones generales muestran los datos? ¿qué tareas de análisis debemos empezar haciendo? ¿cuáles son los caminos generales para formular con precisión y contestar algunas preguntas que nos interesen? El trabajo inferencial, confirmatorio, o de juez: ¿cómo evaluar el peso de la evidencia de los descubrimientos del paso anterior? ¿qué tan bien soportadas están las respuestas y conclusiones por nuestro conjunto de datos? Aunque en el proceso de inferencia las gráficas cada vez son más importantes, la visualización entra más claramente dentro del análisis exploratorio de datos. Y como en un principio no es claro como la visualización aporta al proceso de la inferencia, se le consideró por mucho tiempo como un área de poca importancia para la estadística: una herramienta que en todo caso sirve para comunicar ideas simples, de manera deficiente, y a personas poco sofisticadas. El peor lado de este punto de vista consiste en restringirse a el análisis estadístico rutinario Cleveland (1993): aplicar las recetas y negarse a ver los datos de distinta manera (¡incluso pensar que esto puede sesgar los resultados, o que nos podría engañar!). El siguiente ejemplo muestra un caso grave y real (no simulado) de este análisis estadístico rutinario (tomado de Cleveland (1994)). A la derecha mostramos los resultados de un experimento de agricultura. Se cultivaron diez variedades de cebada en seis sitios de Minnesota, en \\(1921\\) y \\(1932\\). Este es uno de los primeros ejemplos en el que se aplicaron las ideas de Fisher en cuanto a diseño de experimentos. En primer lugar, observamos: Los niveles generales de rendimiento varían mucho dependiendo del sitio: hay mejores y peores sitios. Los rendimientos son típicamente más altos en 1931 que en 1932. Sin embargo, Morris es anómalo en cuanto a que el patrón no es consistente con el resto de los sitios. Hay variación considerable de las variedades dentro de cada sitio. ¿Existe alguna variedad que sea mejor que otras? Notamos claramente la anomalía en las diferencias: en el sitio Morris, el año 1932 fue mejor que el de 1931. Estos datos fueron reanalizados desde la época en la que se recolectaron por muchos agrónomos. Hasta muy recientemente se detectó la anomalía en el comportamiento de los años en el sitio Morris, el cual es evidente en la gráfica. Investigación posterior ha mostrado que es muy plausible que en algún momento alguien volteó las etiquetas de los años en este sitio. Este ejemplo muestra, en primer lugar, que la visualización es crucial en el proceso de análisis de datos: sin ella estamos expuestos a no encontrar aspectos importantes de los datos (errores) que deben ser discutidos - aún cuando nuestra receta de análisis no considere estos aspectos. Ninguna receta puede aproximarse a describir todas las complejidades y detalles en un conjunto de datos de tamaño razonable (este ejemplo, en realidad, es chico). Sin embargo, la visualización de datos, por su enfoque menos estructurado, y el hecho de que se apoya en un medio con un “ancho de banda” mayor al que puede producir un cierto número de cantidades resumen, es ideal para investigar estos aspectos y detalles. Visualización popular de datos Publicaciones populares (periódicos, revistas, sitios internet) muchas veces incluyen visualización de datos como parte de sus artículos o reportajes. En general siguen el mismo patrón que en la visión tradicionalista de la estadística: sirven más para divertir que para explicar, tienden a explicar ideas simples y conjuntos chicos de datos, y se consideran como una “ayuda” para los “lectores menos sofisticados”. Casi siempre se trata de gráficas triviales (muchas veces con errores graves) que no aportan mucho a artículos que tienen un nivel de complejidad mucho mayor (es la filosofía: lo escrito para el adulto, lo graficado para el niño). Referencias "],
["teoria-de-visualizacion-de-datos.html", "1.2 Teoría de visualización de datos", " 1.2 Teoría de visualización de datos Existe teoría fundamentada acerca de la visualización. Después del trabajo pionero de Tukey, los principios e indicadores de Tufte se basan en un estudio de la historia de la graficación y ejercicios de muestreo de la práctica gráfica a lo largo de varias disciplinas (¿cuáles son las mejores gráficas? ¿por qué? El trabajo de Cleveland es orientado a la práctica del análisis de datos (¿cuáles gráficas nos han ayudado a mostrar claramente los resultados del análisis?), por una parte, y a algunos estudios de percepción visual. En resumen, hablaremos de las siguientes guías: Principios generales del diseño analítico Aplicables a una presentación o análisis completos, y como guía para construir nuevas visualizaciones (Tufte 2006). Principio 1. Muestra comparaciones, contrastes, diferencias. Principio 2. Muestra causalidad, mecanismo, explicación, estructura sistemática. Principio 3. Muestra datos multivariados, es decir, más de una o dos variables. Principio 4. Integra palabras, números, imágenes y diagramas. Principio 5. Describe la totalidad de la evidencia. Muestra fuentes usadas y problemas relevantes. Principio 6. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Técnicas de visualización Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar (Tukey (1977), Cleveland (1993), Cleveland (1994), Tufte (2006)). Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, banking 45, suavizamiento y bandas de confianza. Pequeños múltiplos Indicadores de calidad gráfica Aplicables a cualquier gráfica en particular. Estas son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica (Tufte 1986). Integridad Gráfica. El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk. Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos. Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. For non-data- ink, less is more. For data-ink, less is a bore. Densidad de datos. Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. Las gráficas se pueden encoger mucho. Percepción visual. Algunas tareas son más fáciles para el ojo humano que otras (Cleveland 1994). Referencias "],
["factor-de-engano-y-chartjunk.html", "1.3 Factor de engaño y Chartjunk", " 1.3 Factor de engaño y Chartjunk El factor de engaño es el cociente entre el efecto mostrado en una gráfica y el efecto correspondiente en los datos. Idealmente, el factor de engaño debe ser 1 (ninguna distorsión). El chartjunk son aquellos elementos gráficos que no corresponden a variación de datos, o que entorpecen la interpretación de una gráfica. Estos son los indicadores de calidad más fáciles de entender y aplicar, y afortunadamente cada vez son menos comunes. Un diseño popular que califica como chartjunk y además introduce factores de engaño es el pie de 3D. En la gráfica de la derecha, podemos ver como la rebanada C se ve más grande que la rebanada A, aunque claramente ese no es el caso (factor de engaño). La razón es la variación en la perspectiva que no corresponde a variación en los datos (chartjunk). Crítica gráfica: Gráfica de Pie Todavía elementos que pueden mejorar la comprensión de nuestra gráfica de pie: se trata de la decodificiación que hay que hacer categoría - color - cuantificación. Podemos agregar las etiquetas como se muestra en la serie de la derecha, pero entonces: ¿por qué no mostrar simplemente la tabla de datos? ¿qué agrega el pie a la interpretación? La deficiencias en el pie se pueden ver claramente al intentar graficar más categorías (13). En el primer pie no podemos distinguir realmente cuáles son las categorías grandes y cuáles las chicas, y es muy difícil tener una imagen mental clara de estos datos. Agregar los porcentajes ayuda, pero entonces, otra vez, preguntamos cuál es el propósito del pie. La tabla de la izquierda hace todo el trabajo (una vez que ordenamos las categrías de la más grande a la más chica). Es posible hacer una gráfica de barras como la de abajo a la izquierda. Hay otros tipos de chartjunk comunes: uno es la textura de barras, por ejemplo. El efecto es la producción de un efecto moiré que es desagradable y quita la atención de los datos, como en la gráfica de barras de abajo. Otro común son las rejillas, como mostramos en las gráficas de la izquierda. Nótese como en estos casos hay efectos ópticos no planeados que degradan la percepción de los patrones en los datos. "],
["pequenos-multiplos-y-densidad-grafica.html", "1.4 Pequeños múltiplos y densidad gráfica", " 1.4 Pequeños múltiplos y densidad gráfica La densidad de una gráfica es el tamaño del conjunto de datos que se grafica comparado con el área total de la gráfica. En el siguiente ejemplo, graficamos en logaritmo-10 de cabezas de ganado en Francia (cerdos, res, ovejas y caballos). La gráfica de la izquierda es pobre en densidad pues sólo representa 4 datos. La manera más fácil de mejorar la densidad es hacer más chica la gráfica: La razón de este encogimiento es una que tiene qué ver con las oportunidades perdidas de una gráfica grande. Si repetimos este mismo patrón (misma escala, mismos tipos de ganado) para distintos países obtenemos la siguiente gráfica: Esta es una gráfica de puntos. Es útil como sustituto de una gráfica de barras, y es superior en el sentido de que una mayor proporción de la tinta que se usa es tinta de datos. Otra vez, mayor proporción de tinta de datos representa más oportunidades que se pueden capitalizar, como muestra la gráfica de punto y líneas que mostramos al principio (rendimiento en campos de cebada). Más pequeños múltiplos Los pequeños múltiplos presentan oportunidades para mostrar más acerca de nuestro problema de interés. Consideramos por ejemplo la relación de radiación solar y niveles de ozono. Podemos ver que si incluimos una variable adicional (velocidad del viento) podemos entender más acerca de la relación de radiación solar y niveles de ozono: "],
["tinta-de-datos.html", "1.5 Tinta de datos", " 1.5 Tinta de datos Maximizar la proporción de tinta de datos en nuestras gráficas tiene beneficios inmediatos. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: ¿Por qué usar grises en lugar de negros? La respuesta tiene qué ver con el principio de tinta de datos: si marcamos las diferencias sutil pero claramente, tenemos más oportunidades abiertas para hacer énfasis en lo que nos interesa: a una gráfica o tabla saturada no se le puede hacer más - es difícil agregar elementos adicionales que ayuden a la comprensión. Si comenzamos marcando con sutileza, entonces se puede hacer más. Los mapas geográficos son un buen ejemplo de este principio. El espacio en blanco es suficientemente bueno para indicar las fronteras en una tabla, y facilita la lectura: Para un ejemplo del proceso de rediseño de una tabla, ver aquí. Finalmente, podemos ver un ejemplo que intenta incorporar los elementos del diseño analítico, incluyendo pequeños múltiplos: "],
["decoracion.html", "1.6 Decoración", " 1.6 Decoración "],
["percepcion-de-escala.html", "1.7 Percepción de escala", " 1.7 Percepción de escala Entre la percepción visual y la interpretación de una gráfica están implícitas tareas visuales específicas que las personas debemos realizar para ver correctamente la gráfica. En la década de los ochenta, William S. Cleveland y Robert McGill realizaron algunos experimentos identificando y clasificando estas tareas para diferentes tipos de gráficos (Cleveland and McGill 1984). En estos, se le pregunta a la persona que compare dos valores dentro de una gráfica, por ejemplo, en dos barras en una gráfica de barras, o dos rebanadas de una gráfica de pie. Los resultados de Cleveland y McGill fueron replicados por Heer y Bostock en 2010 y los resultados se muestran en las gráficas de la abajo: Imagen de Heer y Bostock, 2010 Referencias "],
["ejemplos-grafica-de-minard.html", "1.8 Ejemplos: gráfica de Minard", " 1.8 Ejemplos: gráfica de Minard Concluimos esta sección con una gráfica que, aunque poco común, ejemplifica los principios de una buena gráfica, y es reconocida como una de las mejores visualizaciones de la historia. Una gráfica excelente, presenta datos interesantes de forma bien diseñada: es una cuestión de fondo, de diseño, y estadística… [Se] compone de ideas complejas comunicadas con claridad, precisión y eficiencia. … [Es] lo que da al espectador la mayor cantidad de ideas, en el menor tiempo, con la menor cantidad de tinta, y en el espacio más pequeño. … Es casi siempre multivariado. … Una excelente gráfica debe decir la verdad acerca de los datos. (Tufte, 1983) La famosa visualización de Charles Joseph Minard de la marcha de Napoleón sobre Moscú, ilustra los principios de una buena gráfica. Tufte señala que esta imagen “bien podría ser el mejor gráfico estadístico jamás dibujado”, y sostiene que “cuenta una historia rica y coherente con sus datos multivariados, mucho más esclarecedora que un solo número que rebota en el tiempo”. Se representan seis variables: el tamaño del ejército, su ubicación en una superficie bidimensional, la dirección del movimiento del ejército y la temperatura en varias fechas durante la retirada de Moscú“. Hoy en día Minard es reconocido como uno de los principales contribuyentes a la teoría de análisis de datos y creación de infografías con un fundamento estadístico. Se grafican 6 variables: el número de tropas de Napoleón, la distancia, la temperatura, la ubicación (latitud y longitud), la dirección en que viajaban las tropas y la localización relativa a fechas específicas. La gráfica de Minard, como la describe E.J. Marey, parece “desafiar la pluma del historiador con su brutal elocuencia”, la combinación de datos del mapa, y la serie de tiempo, dibujados en 1869, “retratan una secuencia de pérdidas devastadoras que sufrieron las tropas de Napoleón en 1812”. Comienza en la izquierda, en la frontera de Polonia y Rusia, cerca del río Niemen. La línea gruesa dorada muestra el tamaño de la Gran Armada (422,000) en el momento en que invadía Rusia en junio de 1812. El ancho de esta banda indica el tamaño de la armada en cada punto del mapa. En septiembre, la armada llegó a Moscú, que ya había sido saqueada y dejada desértica, con sólo 100,000 hombres. El camino del retiro de Napoleón desde Moscú está representado por la línea oscuara (gris) que está en la parte inferior, que está relacionada a su vez con la temperatura y las fechas en el diagrama de abajo. Fue un invierno muy frío, y muchos se congelaron en su salida de Rusia. Como se muestra en el mapa, cruzar el río Berezina fue un desastre, y el ejército de Napoleón logró regresar a Polonia con tan sólo 10,000 hombres. También se muestran los movimientos de las tropas auxiliaries, que buscaban proteger por atrás y por la delantera mientras la armada avanzaba hacia Moscú. La gráfica de Minard cuenta una historia rica y cohesiva, coherente con datos multivariados y con los hechos históricos, y que puede ser más ilustrativa que tan sólo representar un número rebotando a lo largo del tiempo. "],
["introduccion-a-r-y-al-paquete-ggplot2.html", "Sección 2 Introducción a R y al paquete ggplot2", " Sección 2 Introducción a R y al paquete ggplot2 ¿Qué es R? R es un lenguaje de programación y un ambiente de cómputo estadístico R es software libre (no dice qué puedes o no hacer con el software), de código abierto (todo el código de R se puede inspeccionar - y se inspecciona). Cuando instalamos R, instala la base de R. Mucha de la funcionalidad adicional está en paquetes (conjunto de funciones y datos documentados) que la comunidad contribuye. ¿Cómo entender R? Hay una sesión de R corriendo. La consola de R es la interfaz entre R y nosotros. En la sesión hay objetos. Todo en R es un objeto: vectores, tablas, funciones, etc. Operamos aplicando funciones a los objetos y creando nuevos objetos. ¿Por qué R? R funciona en casi todas las plataformas (Mac, Windows, Linux e incluso en Playstation 3). R es un lenguaje de programación completo, permite desarrollo de DSLs. R promueve la investigación reproducible. R está actualizado gracias a que tiene una activa comunidad. Solo en CRAN hay cerca de \\(10,000\\) paquetes (funcionalidad adicional de R creadas creada por la comunidad). R se puede combinar con otras herramientas. R tiene capacidades gráficas muy sofisticadas. R es popular (Revolutions blog). "],
["r-primeros-pasos.html", "2.1 R: primeros pasos", " 2.1 R: primeros pasos Para comenzar se debe descargar R, esta descarga incluye R básico y un editor de textos para escribir código. Después de descargar R se recomienda descargar RStudio (gratis y libre). Rstudio es un ambiente de desarrollo integrado para R: incluye una consola, un editor de texto y un conjunto de herramientas para administrar el espacio de trabajo cuando se utiliza R. Algunos shortcuts útiles en RStudio son: En el editor command/ctrl + enter: enviar código a la consola ctrl + 2: mover el cursor a la consola En la consola flecha hacia arriba: recuperar comandos pasados ctrl + flecha hacia arriba: búsqueda en los comandos ctrl + 1: mover el cursor al editor R en análisis de datos El estándar científico para contestar preguntas o tomar decisiones es uno que se basa en el análisis de datos. Aquí consideramos técnicas cuantitativas: recolectar, organizar, entender, interpretar y extraer información de colecciones de datos predominantemente numéricos. Todas estas tareas son partes del análisis de datos, cuyo proceso podría resumirse con el siguiente diagrama: Es importante la forma en que nos movemos dentro de estos procesos en el análisis de datos y en este curso buscamos dar herramientas para facilitar cumplir los siguientes principios: Reproducibilidad. Debe ser posible reproducir el análisis en todos sus pasos, en cualquier momento. Claridad. Los pasos del análisis deben estar documentados apropiadamente, de manera que las decisiones importantes puedan ser entendidas y explicadas claramente. Dedicaremos las primeras sesiones a aprender herramientas básicas para poder movernos agilmente a lo largo de las etapas de análisis utilizando R y nos enfocaremos en los paquetes que forman parte del tidyverse. Paquetes y el Tidyverse La mejor manera de usar R para análisis de datos es aprovechando la gran cantidad de paquetes que aportan funcionalidad adicional. Desde Rstudio podemos instalar paquetes (Tools - &gt; Install packages o usar la función install.packages(&quot;nombre_paquete&quot;)). Las siguientes lineas instalan los paquetes devtools y readr. install.packages(&quot;devtools&quot;) install.packages(&quot;readr&quot;) Una vez instalados, podemos cargarlos a nuestra sesión de R mediante library. Por ejemplo, para cargar el paquete readr hacemos: print(read_csv) #&gt; function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), #&gt; na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, #&gt; trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, #&gt; n_max), progress = show_progress(), skip_empty_rows = TRUE) #&gt; { #&gt; tokenizer &lt;- tokenizer_csv(na = na, quoted_na = quoted_na, #&gt; quote = quote, comment = comment, trim_ws = trim_ws, #&gt; skip_empty_rows = skip_empty_rows) #&gt; read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, #&gt; locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, #&gt; comment = comment, n_max = n_max, guess_max = guess_max, #&gt; progress = progress) #&gt; } #&gt; &lt;bytecode: 0x846fe20&gt; #&gt; &lt;environment: namespace:readr&gt; library(readr) print(read_csv) #&gt; function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), #&gt; na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, #&gt; trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, #&gt; n_max), progress = show_progress(), skip_empty_rows = TRUE) #&gt; { #&gt; tokenizer &lt;- tokenizer_csv(na = na, quoted_na = quoted_na, #&gt; quote = quote, comment = comment, trim_ws = trim_ws, #&gt; skip_empty_rows = skip_empty_rows) #&gt; read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, #&gt; locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, #&gt; comment = comment, n_max = n_max, guess_max = guess_max, #&gt; progress = progress) #&gt; } #&gt; &lt;bytecode: 0x846fe20&gt; #&gt; &lt;environment: namespace:readr&gt; read_csv es una función que aporta el paquete readr, que a su vez está incluido en el tidyverse. El paquete de arriba se instaló de CRAN, pero podemos instalar paquetes que están en otros repositorios (por ejemplo BioConductor) o paquetes que están en GitHub. library(devtools) install_github(&quot;tereom/estcomp&quot;) Los paquetes se instalan una sola vez, sin embargo, se deben cargar (ejecutar library(readr)) en cada sesión de R que los ocupemos. En estas notas utilizaremos la colección de paquetes incluídos en el tidyverse. Estos paquetes de R están diseñados para ciencia de datos, y para funcionar juntos como parte de un flujo de trabajo. La siguiente imagen tomada de Why the tidyverse (Joseph Rickert) indica que paquetes del tidyverse se utilizan para cada etapa del análisis de datos. knitr::include_graphics(&quot;img/tidyverse.png&quot;) Recursos Existen muchos recursos gratuitos para aprender R, y resolver nuestras dudas: Buscar ayuda: Google, StackOverflow o RStudio Community. Para aprender más sobre un paquete o una función pueden visitar Rdocumentation.org. La referencia principal de estas notas es el libro R for Data Science de Hadley Wickham. RStudio tiene una Lista de recursos en línea. Para aprender programación avanzada en R, el libro gratuito Advanced R de Hadley Wickham es una buena referencia. En particular es conveniente leer la guía de estilo (para todos: principiantes, intermedios y avanzados). Para mantenerse al tanto de las noticias de la comunidad de R pueden seguir #rstats en Twitter. Para aprovechar la funcionalidad de RStudio. "],
["visualizacion-con-ggplot2.html", "2.2 Visualización con ggplot2", " 2.2 Visualización con ggplot2 Utilizaremos el paquete ggplot2, fue desarrollado por Hadley Wickham y es una implementación de la gramática de las gráficas (Wilkinson et al. 2005). Si no lo tienes instalado comienza instalando el paquete ggplot2 o el tidyverse que lo incluye. Gráficas de dispersión library(tidyverse) # Cargamos el paquete en nuestra sesión Usaremos el conjunto de datos election_sub_2012 que se incluye en el paquete estcomp, puedes encontrar información de esta base de datos tecleando ?election_sub_2012. library(estcomp) data(election_sub_2012) ?election_sub_2012 glimpse(election_sub_2012) #&gt; Observations: 1,500 #&gt; Variables: 23 #&gt; $ state_code &lt;chr&gt; &quot;19&quot;, &quot;30&quot;, &quot;09&quot;, &quot;07&quot;, &quot;09&quot;, &quot;27&quot;, &quot;20&quot;, &quot;15&quot;, … #&gt; $ state_name &lt;chr&gt; &quot;Nuevo León&quot;, &quot;Veracruz&quot;, &quot;Ciudad de México&quot;, &quot;C… #&gt; $ state_abbr &lt;chr&gt; &quot;NL&quot;, &quot;VER&quot;, &quot;CDMX&quot;, &quot;CHPS&quot;, &quot;CDMX&quot;, &quot;TAB&quot;, &quot;OAX… #&gt; $ district_loc_17 &lt;int&gt; 20, 30, 27, 5, 26, 21, 15, 43, 4, 19, 17, 9, 9, … #&gt; $ district_fed_17 &lt;int&gt; 7, 11, 22, 5, 15, 6, 3, 7, 4, 5, 6, 4, 1, 1, 3, … #&gt; $ polling_id &lt;int&gt; 90532, 134417, 32160, 15456, 31925, 122541, 9451… #&gt; $ section &lt;int&gt; 347, 1775, 2705, 1121, 4358, 502, 37, 826, 2207,… #&gt; $ region &lt;chr&gt; &quot;noreste&quot;, &quot;este&quot;, &quot;centrosur&quot;, &quot;suroeste&quot;, &quot;cen… #&gt; $ polling_type &lt;chr&gt; &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;,… #&gt; $ section_type &lt;chr&gt; &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;M&quot;, NA, &quot;U&quot;, &quot;M&quot;,… #&gt; $ pri_pvem &lt;int&gt; 150, 146, 103, 135, 108, 102, 121, 157, 134, 187… #&gt; $ pan &lt;int&gt; 111, 52, 43, 33, 95, 27, 60, 94, 40, 128, 53, 56… #&gt; $ panal &lt;int&gt; 12, 4, 10, 10, 4, 4, 8, 16, 4, 10, 0, 12, 8, 1, … #&gt; $ prd_pt_mc &lt;int&gt; 78, 226, 240, 237, 181, 290, 141, 158, 90, 63, 6… #&gt; $ otros &lt;int&gt; 1, 4, 4, 7, 6, 14, 1, 8, 8, 11, 2, 14, 7, 12, 48… #&gt; $ total &lt;int&gt; 352, 432, 400, 422, 394, 437, 331, 433, 276, 399… #&gt; $ nominal_list &lt;int&gt; 675, 636, 688, 672, 522, 698, 596, 716, 506, 584… #&gt; $ pri_pvem_pct &lt;dbl&gt; 43, 34, 26, 32, 27, 23, 37, 36, 49, 47, 42, 42, … #&gt; $ pan_pct &lt;dbl&gt; 32, 12, 11, 8, 24, 6, 18, 22, 14, 32, 50, 12, 16… #&gt; $ panal_pct &lt;dbl&gt; 3, 1, 2, 2, 1, 1, 2, 4, 1, 3, 0, 3, 3, 0, 28, 0,… #&gt; $ prd_pt_mc_pct &lt;dbl&gt; 22, 52, 60, 56, 46, 66, 43, 36, 33, 16, 6, 40, 5… #&gt; $ otros_pct &lt;dbl&gt; 0, 1, 1, 2, 2, 3, 0, 2, 3, 3, 2, 3, 2, 3, 13, 8,… #&gt; $ winner &lt;chr&gt; &quot;pri_pvem&quot;, &quot;prd_pt_mc&quot;, &quot;prd_pt_mc&quot;, &quot;prd_pt_mc… Comencemos con nuestra primera gráfica: ggplot(data = election_sub_2012) + geom_point(mapping = aes(x = total, y = prd_pt_mc)) En ggplot2 se inicia una gráfica con la instrucción ggplot(), debemos especificar explicitamente que base de datos usamos, este es el primer argumento en la función ggplot(). Una vez que creamos la base añadimos capas, y dentro de aes() escribimos las variables que queremos graficar y el atributo de la gráfica al que queremos mapearlas. La función geom_point() añade una capa de puntos, hay muchas funciones geometrías incluídas en ggplot2: geom_line(), geom_boxplot(), geom_histogram,… Cada una acepta distintos argumentos para mapear las variables en los datos a características estéticas de la gráfica. En el ejemplo de arriba mapeamos displ al eje x, prd_pt_mc al eje y, pero geom_point() nos permite representar más variables usando la forma, color y/o tamaño del punto. Esta flexibilidad nos permite entender o descubrir patrones más interesantes en los datos. ggplot(election_sub_2012) + geom_point(aes(x = total, y = prd_pt_mc, color = polling_type)) Experimenta con los aesthetics color (color), tamaño (size) y forma (shape).             ¿Qué diferencia hay entre las variables categóricas y las continuas?             ¿Qué ocurre cuando combinas varios aesthetics? El mapeo de las propiedades estéticas se denomina escalamiento y depende del tipo de variable, las variables discretas (por ejemplo, tipo de casilla, región, estado) se mapean a distintas escalas que las variables continuas (variables numéricas como voto por un partido, lista nominal, etc.), los defaults de escalamiento para algunos atributos son (los escalamientos se pueden modificar): aes Discreta Continua Color (color) Arcoiris de colores Gradiente de colores Tamaño (size) Escala discreta de tamaños Mapeo lineal entre el área y el valor Forma (shape) Distintas formas No aplica Transparencia (alpha) No aplica Mapeo lineal a la transparencia Los geoms controlan el tipo de gráfica p &lt;- ggplot(election_sub_2012, aes(x = total, y = prd_pt_mc)) p + geom_line() ¿Qué problema tiene la siguiente gráfica? p &lt;- ggplot(election_sub_2012, aes(x = pan_pct, y = prd_pt_mc_pct)) p + geom_point(size = 0.8) p + geom_jitter(size = 0.8) ¿Cómo podemos mejorar la siguiente gráfica? ggplot(election_sub_2012, aes(x = state_abbr, y = prd_pt_mc_pct)) + geom_point(size = 0.8) Intentemos reodenar los niveles de la variable clase ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_point(size = 0.8) Podemos probar otros geoms. ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_jitter(size = 0.8) ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_boxplot() También podemos usar más de un geom! ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_jitter(size = 0.8) + geom_boxplot() Y mejorar presentación: ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_jitter(alpha = 0.6, size = 0.8) + geom_boxplot(outlier.color = NA) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;Votos por casilla y estado&quot;, subtitle = &quot;PRD-PT-MC&quot;, x = &quot;estado&quot;, y = &quot;total de votos&quot;) Lee la ayuda de reorder y repite las gráficas anteriores ordenando por la mediana de prd_pt_mc.             ¿Cómo harías para graficar los puntos encima de las cajas de boxplot? Paneles Ahora veremos como hacer páneles de gráficas, la idea es hacer varios múltiplos de una gráfica donde cada múltiplo representa un subconjunto de los datos, es una práctica muy útil para explorar relaciones condicionales. En ggplot podemos usar facet_wrap() para hacer paneles dividiendo los datos de acuerdo a las categorías de una sola variable ggplot(election_sub_2012, aes(x = reorder(state_abbr, pri_pvem_pct, median), y = pri_pvem_pct)) + geom_boxplot() + facet_wrap(~ section_type) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Podemos eliminar los NA. Veremos la función filter() en la próxima sesión. ggplot(filter(election_sub_2012, !is.na(section_type)), aes(x = reorder(state_abbr, pri_pvem_pct, median), y = pri_pvem_pct)) + geom_boxplot() + facet_wrap(~ section_type) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) También podemos hacer una cuadrícula de \\(2\\) dimensiones usando facet\\_grid(filas~columnas) # Veremos como manipular datos en las próximas clases election_region_2012 &lt;- election_2012 %&gt;% group_by(region, section_type) %&gt;% summarise_at(vars(pri_pvem:total), sum) %&gt;% mutate_at(vars(pri_pvem:otros), .funs = ~ 100 * ./total) %&gt;% ungroup() %&gt;% mutate(region = reorder(region, pri_pvem)) %&gt;% gather(party, prop_votes, pri_pvem:otros) %&gt;% filter(!is.na(section_type)) ggplot(election_region_2012, aes(x = reorder(party, prop_votes), y = prop_votes, fill = reorder(party, -prop_votes))) + geom_col(show.legend = FALSE) + facet_grid(region ~ section_type) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Los páneles pueden ser muy útiles para entender relaciones en nuestros datos. En la siguiente gráfica es difícil entender si existe una relación entre radiación solar y ozono. data(airquality) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() #&gt; Warning: Removed 42 rows containing missing values (geom_point). Veamos que ocurre si realizamos páneles separando por velocidad del viento. library(Hmisc) airquality$Wind.cat &lt;- cut2(airquality$Wind, g = 3) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() + facet_wrap(~ Wind.cat) Podemos agregar un suavizador (loess) para ver mejor la relación de las variables en cada panel. ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() + facet_wrap(~ Wind.cat) + geom_smooth(method = &quot;lm&quot;) Como vimos en el caso de los resultados electorales por región, en ocasiones es necesario realizar transformaciones u obtener subconjuntos de los datos para poder responder preguntas de nuestro interés. library(dplyr) library(babynames) glimpse(babynames) #&gt; Observations: 1,924,665 #&gt; Variables: 5 #&gt; $ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880,… #&gt; $ sex &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;,… #&gt; $ name &lt;chr&gt; &quot;Mary&quot;, &quot;Anna&quot;, &quot;Emma&quot;, &quot;Elizabeth&quot;, &quot;Minnie&quot;, &quot;Margaret&quot;, … #&gt; $ n &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288,… #&gt; $ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843,… Supongamos que queremos ver la tendencia del nombre “John”, para ello debemos generar un subconjunto de la base de datos. ¿Qué ocurre en la siguiente gráfica? babynames_John &lt;- filter(babynames, name == &quot;Teresa&quot;) ggplot(babynames_John, aes(x = year, y = prop)) + geom_line() ggplot(babynames_John, aes(x = year, y = prop, color = sex)) + geom_line() La preparación de los datos es un aspecto muy importante del análisis y suele ser la fase que lleva más tiempo. Es por ello que el siguiente tema se enfocará en herramientas para hacer transformaciones de manera eficiente. Recursos El libro R for Data Science (Wickham and Grolemund 2017) tiene un capítulo de visualización. Documentación con ejemplos en la página de ggplot2. Otro recurso muy útil es el acordeón de ggplot. La teoría detrás de ggplot2 se explica en el libro de ggplot2 (Wickham 2009), Google, stackoverflow y RStudio Community tienen un tag para preguntas relacionadas con ggplot2. Referencias "],
["manipulacion-y-agrupacion-de-datos.html", "Sección 3 Manipulación y agrupación de datos", " Sección 3 Manipulación y agrupación de datos En esta sección continuamos con la introducción a R para análisis de datos, en particular mostraremos herramientas de manipulación y transformación de datos. Trataremos los siguientes puntos: Transformación de datos. Estrategia separa-aplica-combina. Reestructura de datos y el principio de los datos limpios. Es sabido que limpieza y preparación de datos ocupan gran parte del tiempo del análisis de datos (Dasu y Johnson, 2003 y NYT’s ‘Janitor Work’ Is Key Hurdle to Insights), es por ello que vale la pena dedicar un tiempo a aprender técnicas que faciliten estas tareas, y entender que estructura en los datos es más conveniente para trabajar. "],
["transformacion-de-datos.html", "3.1 Transformación de datos", " 3.1 Transformación de datos Es sorprendente que una gran variedad de necesidades de transformación de datos se pueden resolver con pocas funciones, en esta sección veremos 5 verbos que fueron diseñados para la tarea de transformación de datos y que comparten una filosofía en cuanto a su estructura. Estudiaremos las siguientes funciones: filter: obten un subconjunto de las filas de acuerdo a un criterio. select: selecciona columnas de acuerdo al nombre arrange: reordena las filas mutate: agrega nuevas variables summarise: reduce variables a valores (crear nuevas bases de datos con resúmenes de variables de la base original) Estas funciones trabajan de manera similar, el primer argumento que reciben es un data.frame, los argumentos que siguen indican que operación se va a efectuar y el resultado es un nuevo data.frame. Adicionalmente, se pueden usar con group_by() que veremos más adelante y que cambia el dominio de cada función, pasando de operar en el conjunto de datos completos a operar en grupos. Datos Usaremos datos de población municipal incluidos en el paquete mxmaps y datos de educación, situación conyugal y hogar incluídos en el estcomp, para tener acceso a ellos cargamos los paquetes correspondientes. library(tidyverse, warn.conflicts = FALSE, quietly = TRUE) library(mxmaps) library(estcomp) Una alternatica a instalar mxmaps es leer únicamente los datos, se descargan del repositorio de GitHub y se cargan con la función load(). download.file(&quot;https://github.com/diegovalle/mxmaps/blob/master/data/df_mxmunicipio.RData?raw=true&quot;, &quot;df_mxmunicipio.RData&quot;) load(&quot;df_mxmunicipio.RData&quot;) Observemos la estructura de los datos: df_mxmunicipio &lt;- as_tibble(df_mxmunicipio) glimpse(df_mxmunicipio) #&gt; Observations: 2,457 #&gt; Variables: 18 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;0… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;0… #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01002&quot;, &quot;01003&quot;, &quot;01004&quot;, &quot;01005&quot;,… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguasca… #&gt; $ state_name_official &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguasca… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;A… #&gt; $ state_abbr_official &lt;chr&gt; &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Asientos&quot;, &quot;Calvillo&quot;, &quot;C… #&gt; $ pop &lt;int&gt; 877190, 46464, 56048, 15577, 120405, 46473, … #&gt; $ pop_male &lt;int&gt; 425731, 22745, 27298, 7552, 60135, 22490, 26… #&gt; $ pop_female &lt;int&gt; 451459, 23719, 28750, 8025, 60270, 23983, 27… #&gt; $ afromexican &lt;dbl&gt; 532, 3, 10, 0, 32, 3, 13, 13, 4, 0, 43, 1139… #&gt; $ part_afromexican &lt;dbl&gt; 2791, 130, 167, 67, 219, 74, 578, 37, 59, 60… #&gt; $ indigenous &lt;dbl&gt; 104125, 1691, 7358, 2213, 8679, 6232, 6714, … #&gt; $ part_indigenous &lt;dbl&gt; 14209, 92, 2223, 191, 649, 251, 247, 84, 76,… #&gt; $ metro_area &lt;chr&gt; &quot;Aguascalientes&quot;, NA, NA, NA, &quot;Aguascaliente… #&gt; $ long &lt;dbl&gt; -102.29605, -102.08928, -102.71875, -102.300… #&gt; $ lat &lt;dbl&gt; 21.87982, 22.23832, 21.84691, 22.36641, 21.9… glimpse(df_edu) #&gt; Observations: 7,371 #&gt; Variables: 16 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;002&quot;, &quot;002&quot;, &quot;002&quot;, &quot;003&quot;, … #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01002&quot;, &quot;01002&quot;, &quot;010… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, … #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ sex &lt;chr&gt; &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mujeres&quot;, &quot;Total&quot;, &quot;Hombres&quot;… #&gt; $ pop_15 &lt;dbl&gt; 631064, 301714, 329350, 31013, 14991, 16022, 3867… #&gt; $ no_school &lt;dbl&gt; 2.662329, 2.355211, 2.943677, 4.011221, 4.389300,… #&gt; $ preschool &lt;dbl&gt; 0.17335801, 0.17466873, 0.17215728, 0.25795634, 0… #&gt; $ elementary &lt;dbl&gt; 20.15247, 18.60073, 21.57401, 33.77938, 35.48129,… #&gt; $ secondary &lt;dbl&gt; 29.31145, 30.37976, 28.33278, 39.21259, 37.45581,… #&gt; $ highschool &lt;dbl&gt; 23.31824, 22.84912, 23.74799, 16.07068, 15.67607,… #&gt; $ higher_edu &lt;dbl&gt; 24.291989, 25.560299, 23.130105, 6.355399, 6.3571… #&gt; $ other &lt;dbl&gt; 0.09016518, 0.08020841, 0.09928647, 0.31277206, 0… #&gt; $ schoolyrs &lt;dbl&gt; 10.211152, 10.380144, 10.056383, 7.854005, 7.6920… Filtrar Creamos una tabla de datos de juguete para mostrar el funcionamiento de cada instrucción: df_ej &lt;- tibble(genero = c(&quot;mujer&quot;, &quot;hombre&quot;, &quot;mujer&quot;, &quot;mujer&quot;, &quot;hombre&quot;), estatura = c(1.65, 1.80, 1.70, 1.60, 1.67)) df_ej #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 El primer argumento de filter() es el nombre del data frame, los subsecuentes son las expresiones que indican que filas filtrar. filter(df_ej, genero == &quot;mujer&quot;) #&gt; # A tibble: 3 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 mujer 1.7 #&gt; 3 mujer 1.6 filter(df_ej, estatura &gt; 1.65 &amp; estatura &lt; 1.75) #&gt; # A tibble: 2 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.7 #&gt; 2 hombre 1.67 Algunos operadores importantes para filtrar son: x &gt; 1 x &gt;= 1 x &lt; 1 x &lt;= 1 x != 1 x == 1 x %in% c(&quot;a&quot;, &quot;b&quot;) Crea un subconjunto de los datos df_mxmunicipio que contenga únicamente los municipios de la CDMX (state_abbr es CDMX)             Los municipios de Nuevo León con más de 200,000 habitantes.             Los municipios donde más de la mitad la población se autoidentifica como afromexicana o parte afromexicana. Observación == y operadores booleanos Debemos tener cuidado al usar ==, ¿qué devuelven las siguientes expresiones? sqrt(2) ^ 2 == 2 1/49 * 49 == 1 Los resultados de arriba se deben a que las computadoras usan aritmética de precisión finita: print(1/49 * 49, digits = 22) #&gt; [1] 0.9999999999999998889777 Para estos casos es útil usar la función near() near(sqrt(2) ^ 2, 2) #&gt; [1] TRUE near(1 / 49 * 49, 1) #&gt; [1] TRUE Los operadores booleanos también son convenientes para filtrar: # Conjuntos a | b # a o b a &amp; b # a y b a &amp; !b # a y no-b xor(a, b) El siguiente esquema nos ayuda a entender que hace cada operación, x está representada por el círculo del lado izquierdo y y por el círculo del lado derecho, la parte sombreada muestra las regiones que selecciona el operador: Figure 3.1: Operaciones booleanas, imagen del libro r4ds. Observación: faltantes NA Un caso común es cuando se desea eliminar o localizar los registros con faltantes en una o más columnas de las tablas de datos, en R los datos faltantes se expresan como NA, para seleccionar los registros con faltante en la variable schoolyrs de los datos df_edu resulta natural escribir: filter(df_edu, schoolyrs == NA) #&gt; # A tibble: 0 x 16 #&gt; # … with 16 variables: state_code &lt;chr&gt;, municipio_code &lt;chr&gt;, #&gt; # region &lt;chr&gt;, state_name &lt;chr&gt;, state_abbr &lt;chr&gt;, #&gt; # municipio_name &lt;chr&gt;, sex &lt;chr&gt;, pop_15 &lt;dbl&gt;, no_school &lt;dbl&gt;, #&gt; # preschool &lt;dbl&gt;, elementary &lt;dbl&gt;, secondary &lt;dbl&gt;, highschool &lt;dbl&gt;, #&gt; # higher_edu &lt;dbl&gt;, other &lt;dbl&gt;, schoolyrs &lt;dbl&gt; Y para eliminarlos filter(df_edu, schoolyrs != NA) #&gt; # A tibble: 0 x 16 #&gt; # … with 16 variables: state_code &lt;chr&gt;, municipio_code &lt;chr&gt;, #&gt; # region &lt;chr&gt;, state_name &lt;chr&gt;, state_abbr &lt;chr&gt;, #&gt; # municipio_name &lt;chr&gt;, sex &lt;chr&gt;, pop_15 &lt;dbl&gt;, no_school &lt;dbl&gt;, #&gt; # preschool &lt;dbl&gt;, elementary &lt;dbl&gt;, secondary &lt;dbl&gt;, highschool &lt;dbl&gt;, #&gt; # higher_edu &lt;dbl&gt;, other &lt;dbl&gt;, schoolyrs &lt;dbl&gt; en ambos casos nos devuelve una tabla vacía! El problema resulta de usar los operadores == y !=, pensemos ¿qué regresan las siguientes expresiones? 5 + NA NA / 2 sum(c(5, 4, NA)) mean(c(5, 4, NA)) NA &lt; 3 NA == 3 NA == NA Las expresiones anteriores regresan NA, el hecho que la media de un vector que incluye NAs o su suma regrese NAs se debe a que por defecto en R se propagan los valores faltantes, esto es, si deconozco el valor de una de las componentes de un vector, también desconozco la suma del mismo; sin embargo, muchas funciones tienen un argumento na.rm para eliminarlos, sum(c(5, 4, NA), na.rm = TRUE) #&gt; [1] 9 mean(c(5, 4, NA), na.rm = TRUE) #&gt; [1] 4.5 Aún queda pendiente como filtrarlos en una tabla, para esto veamos que el manejo de datos faltantes en R utiliza una lógica ternaria (como SQL): NA == NA #&gt; [1] NA La expresión anterior puede resultar confusa, una manera de pensar en esto es considerar los NA como no sé, por ejemplo si no se la edad de Juan y no se la edad de Esteban, la respuesta a ¿Juan tiene la misma edad que Esteban? es no sé (NA). edad_Juan &lt;- NA edad_Esteban &lt;- NA edad_Juan == edad_Esteban #&gt; [1] NA edad_Jose &lt;- 32 # Juan es menor que José? edad_Juan &lt; edad_Jose #&gt; [1] NA Por tanto para determinar si un valor es faltante usamos la instrucción is.na(). is.na(NA) #&gt; [1] TRUE Y finalmente podemos filtrar, filter(df_edu, is.na(schoolyrs)) Seleccionar Elegir columnas de un conjunto de datos. df_ej #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 select(df_ej, genero) #&gt; # A tibble: 5 x 1 #&gt; genero #&gt; &lt;chr&gt; #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre select(df_ej, -genero) #&gt; # A tibble: 5 x 1 #&gt; estatura #&gt; &lt;dbl&gt; #&gt; 1 1.65 #&gt; 2 1.8 #&gt; 3 1.7 #&gt; 4 1.6 #&gt; 5 1.67 select(df_ej, starts_with(&quot;g&quot;)) select(df_ej, contains(&quot;g&quot;)) Ve la ayuda de select (?select) y escribe tres maneras de seleccionar las variables del estado en los datos df_mxmunicipio. Ordenar Ordenar de acuerdo al valor de una o más variables: arrange(df_ej, genero) #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.8 #&gt; 2 hombre 1.67 #&gt; 3 mujer 1.65 #&gt; 4 mujer 1.7 #&gt; 5 mujer 1.6 arrange(df_ej, desc(estatura)) #&gt; # A tibble: 5 x 2 #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.8 #&gt; 2 mujer 1.7 #&gt; 3 hombre 1.67 #&gt; 4 mujer 1.65 #&gt; 5 mujer 1.6 Ordena los municipios por población, de mayor a menor.             ¿Cuáles son los municipios con mayor disparidad de sexo (a total)?             ¿Cuáles son los municipios con mayor disparidad de sexo (proporcional)?, elimina los municipios con menos de 5000 habitantes y repite. Mutar Mutar consiste en crear nuevas variables aplicando una función a columnas existentes: mutate(df_ej, estatura_cm = estatura * 100) #&gt; # A tibble: 5 x 3 #&gt; genero estatura estatura_cm #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 165 #&gt; 2 hombre 1.8 180 #&gt; 3 mujer 1.7 170 #&gt; 4 mujer 1.6 160 #&gt; 5 hombre 1.67 167 mutate(df_ej, estatura_cm = estatura * 100, estatura_in = estatura_cm * 0.3937) #&gt; # A tibble: 5 x 4 #&gt; genero estatura estatura_cm estatura_in #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 165 65.0 #&gt; 2 hombre 1.8 180 70.9 #&gt; 3 mujer 1.7 170 66.9 #&gt; 4 mujer 1.6 160 63.0 #&gt; 5 hombre 1.67 167 65.7 Calcula el porcentaje de población indígena de cada municipio y almacenalo en una nueva variable.             Crea una nueva variable que muestre el cociente entre la población femenina y masculina. Hay muchas funciones que podemos usar para crear nuevas variables con mutate(), éstas deben cumplir ser funciones vectorizadas, es decir, reciben un vector de valores y devuelven un vector de la misma dimensión, por ejemplo multiplicar columnas o por un escalar. ¿Cuáles de las siguientes funciones son adecuadas para mutate()? Notar que hay escenarios en los que nos puede interesar usar funciones no vectorizadas con mutate() pero vale la pena entender que es lo que regresan. * mean, pmin, max, *, ^, quantile df_ej_2 &lt;- add_column(df_ej, peso_actual = c(60, 80, 70, 50, 65), peso_anterior = c(66, 78, 73, 54, 61)) mutate(df_ej_2, peso_medio = mean(c(peso_actual, peso_anterior))) #&gt; # A tibble: 5 x 5 #&gt; genero estatura peso_actual peso_anterior peso_medio #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 60 66 65.7 #&gt; 2 hombre 1.8 80 78 65.7 #&gt; 3 mujer 1.7 70 73 65.7 #&gt; 4 mujer 1.6 50 54 65.7 #&gt; 5 hombre 1.67 65 61 65.7 mutate(df_ej_2, peso_menor = pmin(peso_actual, peso_anterior)) #&gt; # A tibble: 5 x 5 #&gt; genero estatura peso_actual peso_anterior peso_menor #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 60 66 60 #&gt; 2 hombre 1.8 80 78 78 #&gt; 3 mujer 1.7 70 73 70 #&gt; 4 mujer 1.6 50 54 50 #&gt; 5 hombre 1.67 65 61 61 mutate(df_ej_2, peso_mayor = max(peso_actual, peso_anterior)) #&gt; # A tibble: 5 x 5 #&gt; genero estatura peso_actual peso_anterior peso_mayor #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 60 66 80 #&gt; 2 hombre 1.8 80 78 80 #&gt; 3 mujer 1.7 70 73 80 #&gt; 4 mujer 1.6 50 54 80 #&gt; 5 hombre 1.67 65 61 80 mutate(df_ej_2, estatura_sq = estatura ^ 2) #&gt; # A tibble: 5 x 5 #&gt; genero estatura peso_actual peso_anterior estatura_sq #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 60 66 2.72 #&gt; 2 hombre 1.8 80 78 3.24 #&gt; 3 mujer 1.7 70 73 2.89 #&gt; 4 mujer 1.6 50 54 2.56 #&gt; 5 hombre 1.67 65 61 2.79 Summarise y resúmenes por grupo Summarise sirve para crear nuevas bases de datos con resúmenes o agregaciones de los datos originales. summarise(df_ej, promedio = mean(estatura)) #&gt; # A tibble: 1 x 1 #&gt; promedio #&gt; &lt;dbl&gt; #&gt; 1 1.68 Calcula la población total, indígena y afromexicana a total. summarise(df_mxmunicipio, indigeonous = sum(indigenous), afromexican = sum(afromexican)) #&gt; # A tibble: 1 x 2 #&gt; indigeonous afromexican #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 25694928 1381853 La mayor utlidad de summarise es cuando la combinamos con una variable de agrupación y esta combinación es la estrategia separa-aplica combina. Separa-aplica-combina (split-apply-combine) Muchos problemas de análisis de datos involucran la aplicación de la estrategia separa-aplica-combina (Wickham 2011), esta consiste en romper un problema en pedazos (de acuerdo a una variable de interés), operar sobre cada subconjunto de manera independiente (ej. calcular la media de cada grupo, ordenar observaciones por grupo, estandarizar por grupo) y después unir los pedazos nuevamente. El siguiente diagrama ejemplifiaca el paradigma de divide-aplica-combina: Separa la base de datos original. Aplica funciones a cada subconjunto. Combina los resultados en una nueva base de datos. Figure 3.2: Imagen de Software Carpentry con licencia CC-BY 4.0. Ahora, cuando pensamos como implementar la estrategia divide-aplica-combina es natural pensar en iteraciones, por ejemplo utilizar un ciclo for para recorrer cada grupo de interés y aplicar las funciones resumen, sin embargo la aplicación de ciclos for desemboca en código difícil de entender por lo que preferimos trabajar con funciones creadas para estas tareas, usaremos el paquete dplyr que además de ser más claro suele ser más veloz. Podemos hacer resúmenes por grupo, primero creamos una base de datos agrupada: by_genero &lt;- group_by(df_ej, genero) by_genero #&gt; # A tibble: 5 x 2 #&gt; # Groups: genero [2] #&gt; genero estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 y después operamos sobre cada grupo, creando un resumen a nivel grupo y uniendo los subconjuntos en una base nueva: summarise(by_genero, promedio = mean(estatura)) #&gt; # A tibble: 2 x 2 #&gt; genero promedio #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.74 #&gt; 2 mujer 1.65 Calcula la población total por estado. Calcula la población indígena y afromexicana por estado.             ¿Qué otros resúmenes puedes hacer para explorar los datos? Algunas funciones útiles con summarise son min(x), median(x), max(x), quantile(x, p), n(), sum(x), sum(x &gt; 1), mean(x &gt; 1), sd(x). Por ejemplo, para cada área metropolitana: cuántos municipios engloba (n()), la población total (sum()) y al estado al que pertenece (first()). by_metro_area &lt;- group_by(df_mxmunicipio, metro_area) no_miss &lt;- filter(by_metro_area, !is.na(metro_area)) pop_metro_area &lt;- summarise(no_miss, state = first(state_abbr), n_municipios = n(), pop_total = sum(pop)) head(pop_metro_area) #&gt; # A tibble: 6 x 4 #&gt; metro_area state n_municipios pop_total #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Acapulco GRO 2 886975 #&gt; 2 Acayucan VER 3 120340 #&gt; 3 Aguascalientes AGS 3 1044049 #&gt; 4 Cancún QROO 2 763121 #&gt; 5 Celaya GTO 3 635706 #&gt; 6 Chihuahua CHIH 3 918339 Operador pipeline En R, cuando uno hace varias operaciones es difícil leer y entender el código: library(estcomp) summarise(group_by(filter(election_2012, !is.na(section_type)), region, section_type), n = n(), pri_pvem = sum(pri_pvem), prd_pt_mc = sum(prd_pt_mc), pan = sum(pan)) #&gt; # A tibble: 24 x 6 #&gt; # Groups: region [8] #&gt; region section_type n pri_pvem prd_pt_mc pan #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 centronorte M 2071 331221 143225 228112 #&gt; 2 centronorte R 5049 651507 211524 447886 #&gt; 3 centronorte U 8940 1229241 653540 1171415 #&gt; 4 centrosur M 1839 324327 277470 126264 #&gt; 5 centrosur R 2541 495288 223978 181755 #&gt; 6 centrosur U 27515 3698793 4765575 1936586 #&gt; 7 este M 3158 462510 370352 306124 #&gt; 8 este R 6768 905078 521793 654839 #&gt; 9 este U 11403 1373876 1602217 1179497 #&gt; 10 noreste M 1259 176191 77062 169285 #&gt; # … with 14 more rows La dificultad radica en que usualmente los parámetros se asignan después del nombre de la función usando (). Una alternativa es ir almacenando las salidas en tablas de datos intermedias pero esto resulta poco práctico porque: 1) almacenamos en el mismo objeto sobreescribiendo ó 2) terminamos con muchos objetos con nombres poco significativos. El operador Forward Pipe (%&gt;%) cambia el orden en que se asignan los parámetros, de manera que un parámetro que precede a la función es enviado (&quot;piped&quot;) a la función: *x %&gt;% f(y)se vuelvef(x, y), *x %&gt;% f(y) %&gt;% g(z)se vuelveg(f(x, y), z)`. Es así que podemos reescribir el código para poder leer las operaciones que vamos aplicando de izquierda a derecha y de arriba hacia abajo. Veamos como cambia el código del ejemplo: election_2012 %&gt;% filter(!is.na(section_type)) %&gt;% group_by(region, section_type) %&gt;% summarise( n = n(), pri_pvem = sum(pri_pvem), prd_pt_mc = sum(prd_pt_mc), pan = sum(pan) ) #&gt; # A tibble: 24 x 6 #&gt; # Groups: region [8] #&gt; region section_type n pri_pvem prd_pt_mc pan #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 centronorte M 2071 331221 143225 228112 #&gt; 2 centronorte R 5049 651507 211524 447886 #&gt; 3 centronorte U 8940 1229241 653540 1171415 #&gt; 4 centrosur M 1839 324327 277470 126264 #&gt; 5 centrosur R 2541 495288 223978 181755 #&gt; 6 centrosur U 27515 3698793 4765575 1936586 #&gt; 7 este M 3158 462510 370352 306124 #&gt; 8 este R 6768 905078 521793 654839 #&gt; 9 este U 11403 1373876 1602217 1179497 #&gt; 10 noreste M 1259 176191 77062 169285 #&gt; # … with 14 more rows podemos leer %&gt;% como “después”. Siguiendo con los datos election_2012, ¿Qué estados tienen la mayor participación (esto es del total de votantes en la lista nominal que porcentaje asistió a votar)? Tip: debes eliminar las casillas especiales pues la lista nominal (ln) no está definida. Variables por grupo En ocasiones es conveniente crear variables por grupo, por ejemplo estandarizar dentro de cada grupo z = (x - mean(x)) / sd(x). Para esto usamos group_by() y mutate(). Veamos un ejemplo: z_prd_pt_mc_state &lt;- election_2012 %&gt;% filter(total &gt; 50, !is.na(section_type)) %&gt;% mutate(prd_pt_mc_pct = prd_pt_mc / total) %&gt;% group_by(state_abbr) %&gt;% mutate( n = n(), sd_prd_pt_mc = sd(prd_pt_mc_pct), mean_prd_pt_mc = mean(prd_pt_mc_pct), z_prd_pt_mc = (prd_pt_mc_pct - mean_prd_pt_mc) / sd_prd_pt_mc ) Verbos de dos tablas Muchas veces debemos reunir información que está almacenada a lo largo de muchas tablas, por ejemplo, si nos interesa conocer como se relaciona el año de escolaridad promedio (schoolyrs en el tibble df_edu) con el porcentaje de población indígena (indigenous en df_mxmunicipios), debemos poder pegar las dos tablas. Hay varias maneras de unir dos bases de datos y debemos pensar en el obejtivo: x &lt;- tibble(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Stuart&quot;, &quot;Pete&quot;), instrument = c(&quot;guitar&quot;, &quot;bass&quot;, &quot;guitar&quot;, &quot;drums&quot;, &quot;bass&quot;, &quot;drums&quot;)) y &lt;- tibble(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Brian&quot;), band = c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;)) x #&gt; # A tibble: 6 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar #&gt; 2 Paul bass #&gt; 3 George guitar #&gt; 4 Ringo drums #&gt; 5 Stuart bass #&gt; 6 Pete drums y #&gt; # A tibble: 5 x 2 #&gt; name band #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John TRUE #&gt; 2 Paul TRUE #&gt; 3 George TRUE #&gt; 4 Ringo TRUE #&gt; 5 Brian FALSE inner_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 4 x 3 #&gt; name instrument band #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar TRUE #&gt; 2 Paul bass TRUE #&gt; 3 George guitar TRUE #&gt; 4 Ringo drums TRUE left_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 6 x 3 #&gt; name instrument band #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar TRUE #&gt; 2 Paul bass TRUE #&gt; 3 George guitar TRUE #&gt; 4 Ringo drums TRUE #&gt; 5 Stuart bass &lt;NA&gt; #&gt; 6 Pete drums &lt;NA&gt; semi_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 4 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar #&gt; 2 Paul bass #&gt; 3 George guitar #&gt; 4 Ringo drums anti_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 2 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Stuart bass #&gt; 2 Pete drums Resumamos lo que observamos arriba: Tipo Acción inner Incluye únicamente las filas que aparecen tanto en x como en y left Incluye todas las filas en x y las filas de y que coincidan semi Incluye las filas de x que coincidan con y anti Incluye las filas de x que no coinciden con y Ahora tu turno, ¿cómo se relacionan los años de escolaridad con el porcentaje de población indígena. Utiliza los datos df_mxmunicipio y df_edu para explorar la relación. ¿cuál es el join adecuado? ¿de qué tamaño serán los datos finales? glimpse(df_edu) #&gt; Observations: 7,371 #&gt; Variables: 16 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;002&quot;, &quot;002&quot;, &quot;002&quot;, &quot;003&quot;, … #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01002&quot;, &quot;01002&quot;, &quot;010… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, … #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ sex &lt;chr&gt; &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mujeres&quot;, &quot;Total&quot;, &quot;Hombres&quot;… #&gt; $ pop_15 &lt;dbl&gt; 631064, 301714, 329350, 31013, 14991, 16022, 3867… #&gt; $ no_school &lt;dbl&gt; 2.662329, 2.355211, 2.943677, 4.011221, 4.389300,… #&gt; $ preschool &lt;dbl&gt; 0.17335801, 0.17466873, 0.17215728, 0.25795634, 0… #&gt; $ elementary &lt;dbl&gt; 20.15247, 18.60073, 21.57401, 33.77938, 35.48129,… #&gt; $ secondary &lt;dbl&gt; 29.31145, 30.37976, 28.33278, 39.21259, 37.45581,… #&gt; $ highschool &lt;dbl&gt; 23.31824, 22.84912, 23.74799, 16.07068, 15.67607,… #&gt; $ higher_edu &lt;dbl&gt; 24.291989, 25.560299, 23.130105, 6.355399, 6.3571… #&gt; $ other &lt;dbl&gt; 0.09016518, 0.08020841, 0.09928647, 0.31277206, 0… #&gt; $ schoolyrs &lt;dbl&gt; 10.211152, 10.380144, 10.056383, 7.854005, 7.6920… glimpse(df_mxmunicipio) #&gt; Observations: 2,457 #&gt; Variables: 18 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;0… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;0… #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01002&quot;, &quot;01003&quot;, &quot;01004&quot;, &quot;01005&quot;,… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguasca… #&gt; $ state_name_official &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguasca… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;A… #&gt; $ state_abbr_official &lt;chr&gt; &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Asientos&quot;, &quot;Calvillo&quot;, &quot;C… #&gt; $ pop &lt;int&gt; 877190, 46464, 56048, 15577, 120405, 46473, … #&gt; $ pop_male &lt;int&gt; 425731, 22745, 27298, 7552, 60135, 22490, 26… #&gt; $ pop_female &lt;int&gt; 451459, 23719, 28750, 8025, 60270, 23983, 27… #&gt; $ afromexican &lt;dbl&gt; 532, 3, 10, 0, 32, 3, 13, 13, 4, 0, 43, 1139… #&gt; $ part_afromexican &lt;dbl&gt; 2791, 130, 167, 67, 219, 74, 578, 37, 59, 60… #&gt; $ indigenous &lt;dbl&gt; 104125, 1691, 7358, 2213, 8679, 6232, 6714, … #&gt; $ part_indigenous &lt;dbl&gt; 14209, 92, 2223, 191, 649, 251, 247, 84, 76,… #&gt; $ metro_area &lt;chr&gt; &quot;Aguascalientes&quot;, NA, NA, NA, &quot;Aguascaliente… #&gt; $ long &lt;dbl&gt; -102.29605, -102.08928, -102.71875, -102.300… #&gt; $ lat &lt;dbl&gt; 21.87982, 22.23832, 21.84691, 22.36641, 21.9… Si queremos un mapa del ganador de las elecciones por estado debemos unir los datos de elecciones con datos geográficos, estos estan incluídos en mxmaps, son mxstate.map. election_2012_state &lt;- election_2012 %&gt;% group_by(state_code) %&gt;% summarise( pri_pvem = 100 * sum(pri_pvem) / sum(total), pan = 100 * sum(pan) / sum(total), prd_pt_mc = 100 * sum(prd_pt_mc) / sum(total) ) %&gt;% mutate(winner = case_when( pri_pvem &gt; pan &amp; pri_pvem &gt; prd_pt_mc ~ &quot;pri_pvem&quot;, pan &gt; pri_pvem &amp; pan &gt; prd_pt_mc ~ &quot;pan&quot;, TRUE ~ &quot;prd_pt_mc&quot;), winner_pct = pmax(pri_pvem, pan, prd_pt_mc)) election_map &lt;- mxstate.map %&gt;% left_join(election_2012_state, by = c(&quot;region&quot; = &quot;state_code&quot;)) ggplot(election_map, aes(long, lat, group=group)) + geom_polygon(aes(fill = winner)) + coord_map() Podemos especificar el color de cada categoría y la intensidad puede variar de acuerdo al porcentaje de votos que se llevó el partido/alianza ganador. library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine map_edo &lt;- ggplot(election_map, aes(long, lat, group = group)) + geom_polygon(aes(fill = winner, alpha = winner_pct), color = &quot;#666666&quot;, size = .05, show.legend = FALSE) + coord_map() + scale_fill_manual(values = c(&quot;prd_pt_mc&quot; = &quot;#FFCC00&quot;, &quot;pan&quot; = &quot;#3399FF&quot;, &quot;pri_pvem&quot; = &quot;#00CD66&quot;)) + theme_void() election_hexbinmap &lt;- mxhexbin.map %&gt;% left_join(election_2012_state, by = c(&quot;region&quot; = &quot;state_code&quot;)) state_labels_map &lt;- mxhexbin.map %&gt;% group_by(state_abbr) %&gt;% summarise(long = mean(long), lat = mean(lat), group = first(group)) hexbinmap_edo &lt;- ggplot(election_hexbinmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = winner, alpha = winner_pct), color = &quot;#666666&quot;, size = .05, show.legend = FALSE) + coord_map() + scale_fill_manual(values = c(&quot;prd_pt_mc&quot; = &quot;#FFCC00&quot;, &quot;pan&quot; = &quot;#3399FF&quot;, &quot;pri_pvem&quot; = &quot;#00CD66&quot;)) + geom_text(data = state_labels_map, aes(long, lat, label = state_abbr)) + theme_void() grid.arrange(map_edo, hexbinmap_edo, nrow=1) Genera un mapa a nivel municipo que muestre el porcentaje de la población casada a total (mayores de 12 años). Referencias "],
["datos-limpios.html", "3.2 Datos limpios", " 3.2 Datos limpios Una vez que importamos datos a R es conveniente limpiarlos, esto implica almacenarlos de una manera consisistente que nos permita enfocarnos en responder preguntas de los datos en lugar de estar luchando con los datos. Entonces, datos limpios son datos que facilitan las tareas del análisis de datos: Visualización: Resúmenes de datos usando gráficas, análisis exploratorio, o presentación de resultados. Manipulación: Manipulación de variables como agregar, filtrar, reordenar, transformar. Modelación: Ajustar modelos es sencillo si los datos están en la forma correcta. Los principios de datos limpios (Wickham 2014) proveen una manera estándar de organizar la información: Cada variable forma una columna. Cada observación forma un renglón. Cada tipo de unidad observacional forma una tabla. Vale la pena notar que los principios de los datos limpios se pueden ver como teoría de algebra relacional para estadísticos, estós principios equivalen a la tercera forma normal de Codd con enfoque en una sola tabla de datos en lugar de muchas conectadas en bases de datos relacionales. Veamos un ejemplo: La mayor parte de las bases de datos en estadística tienen forma rectangular, ¿cuántas variables tiene la siguiente tabla? tratamientoA tratamientoB Juan Aguirre - 2 Ana Bernal 16 11 José López 3 1 La tabla anterior también se puede estructurar de la siguiente manera: Juan Aguirre Ana Bernal José López tratamientoA - 16 3 tratamientoB 2 11 1 Si vemos los principios (cada variable forma una columna, cada observación forma un renglón, cada tipo de unidad observacional forma una tabla), ¿las tablas anteriores cumplen los principios? Para responder la pregunta identifiquemos primero cuáles son las variables y cuáles las observaciones de esta pequeña base. Las variables son: persona/nombre, tratamiento y resultado. Entonces, siguiendo los principios de datos limpios obtenemos la siguiente estructura: nombre tratamiento resultado Juan Aguirre a - Ana Bernal a 16 José López a 3 Juan Aguirre b 2 Ana Bernal b 11 José López b 1 Limpieza bases de datos Los principios de los datos limpios parecen obvios pero la mayor parte de los datos no los cumplen debido a: La mayor parte de la gente no está familiarizada con los principios y es difícil derivarlos por uno mismo. Los datos suelen estar organizados para facilitar otros aspectos que no son análisis, por ejemplo, la captura. Algunos de los problemas más comunes en las bases de datos que no están limpias son: Los encabezados de las columnas son valores y no nombres de variables. Más de una variable por columna. Las variables están organizadas tanto en filas como en columnas. Más de un tipo de observación en una tabla. Una misma unidad observacional está almacenada en múltiples tablas. La mayor parte de estos problemas se pueden arreglar con pocas herramientas, a continuación veremos como limpiar datos usando 2 funciones del paquete tidyr: gather: recibe múltiples columnas y las junta en pares de valores y nombres, convierte los datos anchos en largos. spread: recibe 2 columnas y las separa, haciendo los datos más anchos. Repasaremos los problemas más comunes que se encuentran en conjuntos de datos sucios y mostraremos como se puede manipular la tabla de datos (usando las funciones gather y spread) con el fin de estructurarla para que cumpla los principios de datos limpios. Los encabezados de las columanas son valores Usaremos ejemplos para entender los conceptos más facilmente. Comenzaremos con una tabla de datos que contiene las mediciones de partículas suspendidas PM2.5 de la red automática de monitoreo atmosférico (RAMA) para los primeros meses del 2019. data(pm25_2019) ¿Cuáles son las variables en estos datos? Esta base de datos tiene 4 variables: fecha, hora, estación y medición (en \\(\\mu g/m^3\\), microgramos por metro cúbico). Notemos que al alargar los datos desapareceran las columnas que se agrupan y dan lugar a dos nuevas columnas: la correspondiente a estación y la correspondiente a medición. Entonces, para alargar una base de datos usamos la función gather que recibe los argumentos: data: base de datos que vamos a reestructurar. key: nombre de la nueva variable que contiene lo que fueron los nombres de columnas que apilamos. value: nombre de la variable que almacenará los valores que corresponden a cada key. …: lo último que especificamos son las columnas que vamos a apilar, la notación para seleccionarlas es la misma que usamos con select(). pm25_2019_tidy &lt;- gather(pm25_2019, key = ESTACION, value = MEDICION, -FECHA, -HORA) head(pm25_2019_tidy) #&gt; # A tibble: 6 x 4 #&gt; FECHA HORA ESTACION MEDICION #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2019-01-01 00:00:00 1 AJM 19 #&gt; 2 2019-01-01 00:00:00 2 AJM 17 #&gt; 3 2019-01-01 00:00:00 3 AJM 14 #&gt; 4 2019-01-01 00:00:00 4 AJM 6 #&gt; 5 2019-01-01 00:00:00 5 AJM 4 #&gt; 6 2019-01-01 00:00:00 6 AJM 7 tail(pm25_2019_tidy) #&gt; # A tibble: 6 x 4 #&gt; FECHA HORA ESTACION MEDICION #&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2019-07-31 00:00:00 19 XAL 12 #&gt; 2 2019-07-31 00:00:00 20 XAL 11 #&gt; 3 2019-07-31 00:00:00 21 XAL 7 #&gt; 4 2019-07-31 00:00:00 22 XAL NA #&gt; 5 2019-07-31 00:00:00 23 XAL 7 #&gt; 6 2019-07-31 00:00:00 24 XAL 7 Observemos que en la tabla ancha teníamos bajo la columna &lt;$10k, en el renglón correspondiente a Agnostic un valor de 27, y podemos ver que este valor en la tabla larga se almacena bajo la columna frecuencia y corresponde a religión Agnostic, income &lt;$10k. También es importante ver que en este ejemplo especificamos las columnas a apilar identificando la que no vamos a alargar con un signo negativo: es decir apila todas las columnas menos religión. La nueva estructura de la base de datos nos permite, por ejemplo, hacer fácilmente una gráfica donde podemos comparar las diferencias en las frecuencias. Nota: En esta sección no explicaremos las funciones de graficación pues estas se cubren en las notas introductorias a R. En esta parte nos queremos concentrar en como limpiar datos y ejemplificar lo sencillo que es trabajar con datos limpios, esto es, una vez que los datos fueron reestructurados es fácil construir gráficas y resúmenes. pm25_2019_tidy %&gt;% mutate( FALTANTE = is.na(MEDICION), ESTACION = reorder(ESTACION, FALTANTE, sum) ) %&gt;% ggplot(aes(x = FECHA, y = HORA, fill = is.na(MEDICION))) + geom_raster(alpha = 0.8) + facet_wrap(~ ESTACION) + scale_fill_manual(&quot;faltante&quot;, values = c(&quot;TRUE&quot; = &quot;salmon&quot;, &quot;FALSE&quot; = &quot;gray&quot;)) Otro ejemplo, veamos los datos df_edu, ¿cuántas variables tenemos? df_edu #&gt; # A tibble: 7,371 x 16 #&gt; state_code municipio_code region state_name state_abbr municipio_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 01 001 01001 Aguascali… AGS Aguascalientes #&gt; 2 01 001 01001 Aguascali… AGS Aguascalientes #&gt; 3 01 001 01001 Aguascali… AGS Aguascalientes #&gt; 4 01 002 01002 Aguascali… AGS Asientos #&gt; 5 01 002 01002 Aguascali… AGS Asientos #&gt; 6 01 002 01002 Aguascali… AGS Asientos #&gt; 7 01 003 01003 Aguascali… AGS Calvillo #&gt; 8 01 003 01003 Aguascali… AGS Calvillo #&gt; 9 01 003 01003 Aguascali… AGS Calvillo #&gt; 10 01 004 01004 Aguascali… AGS Cosío #&gt; # … with 7,361 more rows, and 10 more variables: sex &lt;chr&gt;, pop_15 &lt;dbl&gt;, #&gt; # no_school &lt;dbl&gt;, preschool &lt;dbl&gt;, elementary &lt;dbl&gt;, secondary &lt;dbl&gt;, #&gt; # highschool &lt;dbl&gt;, higher_edu &lt;dbl&gt;, other &lt;dbl&gt;, schoolyrs &lt;dbl&gt; Notemos que el nivel de escolaridad esta guardado en 6 columnas (preschool, elementary, …, other), este tipo de almacenamiento no es limpio pero puede ser útil al momento de ingresar la información. Para tener datos limpios apilamos los niveles de escolaridad de manera que sea una sola columna (nuevamente alargamos los datos): df_edu_tidy &lt;- gather(data = df_edu, grade, percent, preschool:other, na.rm = TRUE) glimpse(df_edu_tidy) #&gt; Observations: 44,226 #&gt; Variables: 12 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;002&quot;, &quot;002&quot;, &quot;002&quot;, &quot;003&quot;, … #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01002&quot;, &quot;01002&quot;, &quot;010… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, … #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ sex &lt;chr&gt; &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mujeres&quot;, &quot;Total&quot;, &quot;Hombres&quot;… #&gt; $ pop_15 &lt;dbl&gt; 631064, 301714, 329350, 31013, 14991, 16022, 3867… #&gt; $ no_school &lt;dbl&gt; 2.662329, 2.355211, 2.943677, 4.011221, 4.389300,… #&gt; $ schoolyrs &lt;dbl&gt; 10.211152, 10.380144, 10.056383, 7.854005, 7.6920… #&gt; $ grade &lt;chr&gt; &quot;preschool&quot;, &quot;preschool&quot;, &quot;preschool&quot;, &quot;preschool… #&gt; $ percent &lt;dbl&gt; 0.17335801, 0.17466873, 0.17215728, 0.25795634, 0… Notemos que en esta ocasión especificamos las columnas que vamos a apilar indicando el nombre de la primera de ellas seguido de : y por último el nombre de la última variable a apilar. Por otra parte, el parámetro na.rm = TRUE se utiliza para eliminar los renglones con valores faltantes en la columna de porcentaje, esto es, eliminamos aquellas observaciones que tenían NA en la columnas de nivel de escolaridad de la tabla ancha. En este caso optamos por que los faltantes sean implícitos, la conveniencia de tenerlos implícitos/explícitos dependerá de la aplicación. Con los datos limpios es facil hacer manipulaciones y graficar. df_edu_cdmx &lt;- df_edu_tidy %&gt;% filter(state_abbr == &quot;CDMX&quot;, sex != &quot;Total&quot;, grade != &quot;other&quot;) %&gt;% mutate(municipio_name = reorder(municipio_name, percent, last)) ggplot(df_edu_cdmx, aes(x = grade, y = percent, group = sex, color = sex)) + geom_path() + facet_wrap(~municipio_name) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_x_discrete(limits = c(&quot;preschool&quot;, &quot;elementary&quot;, &quot;secondary&quot;, &quot;highschool&quot;, &quot;higher_edu&quot;)) Una columna asociada a más de una variable La siguiente base de datos proviene de la Organización Mundial de la Salud y contiene el número de casos confirmados de tuberculosis por país y año, la información esta por grupo demográfico de acuerdo a sexo (m, f), y edad (0-4, 5-14, etc). Los datos están disponibles en http://www.who.int/tb/country/data/download/en/. Utilizaremos un subconjunto de los datos de la prueba ENLACE a nivel primaria, la prueba ENLACE evaluaba a todos los alumnos de tercero a sexto de primaria y a los alumnos de secundaria del país en 3 áreas: español, matemáticas y formación cívica y ética. data(&quot;enlacep_2013&quot;) enlacep_sub_2013 &lt;- enlacep_2013 %&gt;% select(CVE_ENT:PUNT_FCE_6) %&gt;% sample_n(1000) glimpse(enlacep_sub_2013) #&gt; Observations: 1,000 #&gt; Variables: 22 #&gt; $ CVE_ENT &lt;chr&gt; &quot;24&quot;, &quot;16&quot;, &quot;13&quot;, &quot;24&quot;, &quot;05&quot;, &quot;31&quot;, &quot;16&quot;, &quot;02&quot;, &quot;12&quot;,… #&gt; $ NOM_ENT &lt;chr&gt; &quot;SAN LUIS POTOSI&quot;, &quot;MICHOACAN&quot;, &quot;HIDALGO&quot;, &quot;SAN LUIS … #&gt; $ CCT &lt;chr&gt; &quot;24DPR0841Z&quot;, &quot;16KPR3682W&quot;, &quot;13DPR1048A&quot;, &quot;24PPR0329N… #&gt; $ TURNO &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATU… #&gt; $ ESCUELA &lt;chr&gt; &quot;JULIAN DE LOS REYES&quot;, &quot;SOLEDAD, LA&quot;, &quot;ANTONIA RUBIO&quot;… #&gt; $ TIPO &lt;chr&gt; &quot;GENERAL&quot;, &quot;CONAFE&quot;, &quot;GENERAL&quot;, &quot;PARTICULAR&quot;, &quot;GENERA… #&gt; $ CVE_MUN &lt;chr&gt; &quot;050&quot;, &quot;111&quot;, &quot;047&quot;, &quot;028&quot;, &quot;035&quot;, &quot;052&quot;, &quot;052&quot;, &quot;004… #&gt; $ NOM_MUN &lt;chr&gt; &quot;VILLA DE REYES&quot;, &quot;ZIRACUARETIRO&quot;, &quot;PACULA&quot;, &quot;SAN LUI… #&gt; $ CVE_LOC &lt;chr&gt; &quot;0035&quot;, &quot;0025&quot;, &quot;0023&quot;, &quot;0001&quot;, &quot;0001&quot;, &quot;0001&quot;, &quot;0137… #&gt; $ NOM_LOC &lt;chr&gt; &quot;PARDO&quot;, &quot;SOLEDAD, LA&quot;, &quot;VICENTE GUERRERO (PRESIDIO)&quot;… #&gt; $ PUNT_ESP_3 &lt;dbl&gt; 523, 464, 489, 538, 536, 598, 760, 594, 536, 479, 620… #&gt; $ PUNT_MAT_3 &lt;dbl&gt; 540, 399, 494, 535, 538, 638, 770, 602, 568, 514, 658… #&gt; $ PUNT_FCE_3 &lt;dbl&gt; 440, 396, 456, 487, 463, 563, 640, 523, 477, 427, 557… #&gt; $ PUNT_ESP_4 &lt;dbl&gt; 467, 505, 549, 566, 489, 615, 670, 512, 628, 498, 581… #&gt; $ PUNT_MAT_4 &lt;dbl&gt; 480, 497, 585, 573, 510, 658, 679, 520, 691, 521, 564… #&gt; $ PUNT_FCE_4 &lt;dbl&gt; 436, 327, 520, 534, 457, 561, 568, 504, 543, 435, 540… #&gt; $ PUNT_ESP_5 &lt;dbl&gt; 511, 607, 513, 505, 482, 541, 545, 519, 558, 500, 580… #&gt; $ PUNT_MAT_5 &lt;dbl&gt; 542, 543, 542, 514, 478, 586, 625, 523, 617, 531, 610… #&gt; $ PUNT_FCE_5 &lt;dbl&gt; 466, 533, 482, 479, 457, 501, 494, 451, 511, 471, 554… #&gt; $ PUNT_ESP_6 &lt;dbl&gt; 448, 391, 487, 497, 481, 622, 691, 523, 633, 507, 565… #&gt; $ PUNT_MAT_6 &lt;dbl&gt; 502, 379, 469, 522, 549, 611, 737, 505, 676, 533, 562… #&gt; $ PUNT_FCE_6 &lt;dbl&gt; 443, 403, 488, 442, 453, 583, 623, 458, 557, 497, 558… De manera similar a los ejemplos anteriores, utiliza la función gather para apilar las columnas correspondientes a área-grado.             Piensa en como podemos separar la “variable” área-grado en dos columnas. Ahora separaremos las variables materia y grado de la columna materia_grado, para ello debemos pasar a la función separate(), esta recibe como parámetros: el nombre de la base de datos, el nombre de la variable que deseamos separar en más de una, la posición de donde deseamos “cortar” (hay más opciones para especificar como separar, ver ?separate). El default es separar valores en todos los lugares que encuentre un caracter que no es alfanumérico (espacio, guión,…). enlacep_tidy &lt;- separate(data = enlacep_long, col = AREA_GRADO, into = c(&quot;AREA&quot;, &quot;GRADO&quot;), sep = 9) enlacep_tidy #&gt; # A tibble: 11,267 x 13 #&gt; CVE_ENT NOM_ENT CCT TURNO ESCUELA TIPO CVE_MUN NOM_MUN CVE_LOC #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 24 SAN LU… 24DP… MATU… JULIAN… GENE… 050 VILLA … 0035 #&gt; 2 16 MICHOA… 16KP… MATU… SOLEDA… CONA… 111 ZIRACU… 0025 #&gt; 3 13 HIDALGO 13DP… MATU… ANTONI… GENE… 047 PACULA 0023 #&gt; 4 24 SAN LU… 24PP… MATU… FORMAC… PART… 028 SAN LU… 0001 #&gt; 5 05 COAHUI… 05EP… MATU… MEXICO GENE… 035 TORREON 0001 #&gt; 6 31 YUCATAN 31DP… VESP… VICENT… GENE… 052 MOTUL 0001 #&gt; 7 16 MICHOA… 16DP… MATU… 12 DE … GENE… 052 LAZARO… 0137 #&gt; 8 02 BAJA C… 02EP… MATU… &quot;LIC. … GENE… 004 TIJUANA 0001 #&gt; 9 12 GUERRE… 12DP… MATU… PROFA.… GENE… 057 TECPAN… 0079 #&gt; 10 14 JALISCO 14DP… MATU… ALVARO… GENE… 022 CIHUAT… 0001 #&gt; # … with 11,257 more rows, and 4 more variables: NOM_LOC &lt;chr&gt;, #&gt; # AREA &lt;chr&gt;, GRADO &lt;chr&gt;, PUNTAJE &lt;dbl&gt; # creamos un mejor código de área enlacep_tidy &lt;- enlacep_tidy %&gt;% mutate( AREA = substr(AREA, 6, 8), GRADO = as.numeric(GRADO) ) %&gt;% janitor::clean_names() glimpse(enlacep_tidy) #&gt; Observations: 11,267 #&gt; Variables: 13 #&gt; $ cve_ent &lt;chr&gt; &quot;24&quot;, &quot;16&quot;, &quot;13&quot;, &quot;24&quot;, &quot;05&quot;, &quot;31&quot;, &quot;16&quot;, &quot;02&quot;, &quot;12&quot;, &quot;1… #&gt; $ nom_ent &lt;chr&gt; &quot;SAN LUIS POTOSI&quot;, &quot;MICHOACAN&quot;, &quot;HIDALGO&quot;, &quot;SAN LUIS POT… #&gt; $ cct &lt;chr&gt; &quot;24DPR0841Z&quot;, &quot;16KPR3682W&quot;, &quot;13DPR1048A&quot;, &quot;24PPR0329N&quot;, … #&gt; $ turno &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTIN… #&gt; $ escuela &lt;chr&gt; &quot;JULIAN DE LOS REYES&quot;, &quot;SOLEDAD, LA&quot;, &quot;ANTONIA RUBIO&quot;, &quot;… #&gt; $ tipo &lt;chr&gt; &quot;GENERAL&quot;, &quot;CONAFE&quot;, &quot;GENERAL&quot;, &quot;PARTICULAR&quot;, &quot;GENERAL&quot;,… #&gt; $ cve_mun &lt;chr&gt; &quot;050&quot;, &quot;111&quot;, &quot;047&quot;, &quot;028&quot;, &quot;035&quot;, &quot;052&quot;, &quot;052&quot;, &quot;004&quot;, … #&gt; $ nom_mun &lt;chr&gt; &quot;VILLA DE REYES&quot;, &quot;ZIRACUARETIRO&quot;, &quot;PACULA&quot;, &quot;SAN LUIS P… #&gt; $ cve_loc &lt;chr&gt; &quot;0035&quot;, &quot;0025&quot;, &quot;0023&quot;, &quot;0001&quot;, &quot;0001&quot;, &quot;0001&quot;, &quot;0137&quot;, … #&gt; $ nom_loc &lt;chr&gt; &quot;PARDO&quot;, &quot;SOLEDAD, LA&quot;, &quot;VICENTE GUERRERO (PRESIDIO)&quot;, &quot;… #&gt; $ area &lt;chr&gt; &quot;ESP&quot;, &quot;ESP&quot;, &quot;ESP&quot;, &quot;ESP&quot;, &quot;ESP&quot;, &quot;ESP&quot;, &quot;ESP&quot;, &quot;ESP&quot;, … #&gt; $ grado &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,… #&gt; $ puntaje &lt;dbl&gt; 523, 464, 489, 538, 536, 598, 760, 594, 536, 479, 620, 6… Variables almacenadas en filas y columnas El problema más difícil es cuando las variables están tanto en filas como en columnas, veamos una base de datos de fertilidad. ¿Cuáles son las variables en estos datos? data(&quot;df_fertility&quot;) df_fertility #&gt; # A tibble: 306 x 11 #&gt; state size_localidad est age_15_19 age_20_24 age_25_29 age_30_34 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 A… Menos de 2 50… Valor 74.2 175. 175. 102. #&gt; 2 01 A… Menos de 2 50… Erro… 6.71 11.0 9.35 8.05 #&gt; 3 01 A… 2 500-14 999 … Valor 82.5 171. 140. 103. #&gt; 4 01 A… 2 500-14 999 … Erro… 9.79 12.5 10.4 8.76 #&gt; 5 01 A… 15 000-49 999… Valor 72.6 146. 147. 99.0 #&gt; 6 01 A… 15 000-49 999… Erro… 7.07 10.8 10.5 8.11 #&gt; 7 01 A… 100 000 y más… Valor 66.3 120. 102. 84.2 #&gt; 8 01 A… 100 000 y más… Erro… 7.57 8.66 8.98 8.59 #&gt; 9 02 B… Menos de 2 50… Valor 89.6 158. 117. 86.0 #&gt; 10 02 B… Menos de 2 50… Erro… 15.8 17.2 13.2 12.3 #&gt; # … with 296 more rows, and 4 more variables: age_35_39 &lt;dbl&gt;, #&gt; # age_40_44 &lt;dbl&gt;, age_45_49 &lt;dbl&gt;, global &lt;dbl&gt; Estos datos tienen variables en columnas individuales (state, size_localidad), en múltiples columnas (grupo de edad, age_15_19,..) y en filas (Valor y Error estándar). Comencemos por apilar las columnas. fertility_long &lt;- gather(df_fertility, age_bracket, value, age_15_19:global, na.rm = TRUE) fertility_long #&gt; # A tibble: 2,448 x 5 #&gt; state size_localidad est age_bracket value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 01 Aguascalientes Menos de 2 500 habitan… Valor age_15_19 74.2 #&gt; 2 01 Aguascalientes Menos de 2 500 habitan… Error están… age_15_19 6.71 #&gt; 3 01 Aguascalientes 2 500-14 999 habitantes Valor age_15_19 82.5 #&gt; 4 01 Aguascalientes 2 500-14 999 habitantes Error están… age_15_19 9.79 #&gt; 5 01 Aguascalientes 15 000-49 999 habitant… Valor age_15_19 72.6 #&gt; 6 01 Aguascalientes 15 000-49 999 habitant… Error están… age_15_19 7.07 #&gt; 7 01 Aguascalientes 100 000 y más habitant… Valor age_15_19 66.3 #&gt; 8 01 Aguascalientes 100 000 y más habitant… Error están… age_15_19 7.57 #&gt; 9 02 Baja Californ… Menos de 2 500 habitan… Valor age_15_19 89.6 #&gt; 10 02 Baja Californ… Menos de 2 500 habitan… Error están… age_15_19 15.8 #&gt; # … with 2,438 more rows Podemos crear algunas variables adicionales. fertility_vars &lt;- fertility_long %&gt;% mutate( state_code = str_sub(state, 1, 2), state_name = str_sub(state, 4) ) %&gt;% select(-state) fertility_vars #&gt; # A tibble: 2,448 x 6 #&gt; size_localidad est age_bracket value state_code state_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Menos de 2 500 habi… Valor age_15_19 74.2 01 Aguascalien… #&gt; 2 Menos de 2 500 habi… Error es… age_15_19 6.71 01 Aguascalien… #&gt; 3 2 500-14 999 habita… Valor age_15_19 82.5 01 Aguascalien… #&gt; 4 2 500-14 999 habita… Error es… age_15_19 9.79 01 Aguascalien… #&gt; 5 15 000-49 999 habit… Valor age_15_19 72.6 01 Aguascalien… #&gt; 6 15 000-49 999 habit… Error es… age_15_19 7.07 01 Aguascalien… #&gt; 7 100 000 y más habit… Valor age_15_19 66.3 01 Aguascalien… #&gt; 8 100 000 y más habit… Error es… age_15_19 7.57 01 Aguascalien… #&gt; 9 Menos de 2 500 habi… Valor age_15_19 89.6 02 Baja Califo… #&gt; 10 Menos de 2 500 habi… Error es… age_15_19 15.8 02 Baja Califo… #&gt; # … with 2,438 more rows Finalmente, la columna est no es una variable, sino que almacena el nombre de 5 variables, la operación que debemos aplicar (spread) es el inverso de apilar (gather): fertility_tidy &lt;- spread(data = fertility_vars, key = est, value = value) Y podemos mejorar los nombres de las columnas, una opción rápida es usar el paquete janitor. fertility_tidy %&gt;% janitor::clean_names() %&gt;% glimpse() #&gt; Observations: 1,224 #&gt; Variables: 6 #&gt; $ size_localidad &lt;chr&gt; &quot;100 000 y más habitantes&quot;, &quot;100 000 y más habita… #&gt; $ age_bracket &lt;chr&gt; &quot;age_15_19&quot;, &quot;age_15_19&quot;, &quot;age_15_19&quot;, &quot;age_15_19… #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;08&quot;, &quot;… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja Califo… #&gt; $ error_estandar &lt;dbl&gt; 7.572352, 3.204220, 13.055474, 9.145983, 4.819321… #&gt; $ valor &lt;dbl&gt; 66.33564, 43.03023, 58.97916, 61.79522, 80.08338,… o podemos hacerlo manualmente names(fertility_tidy)[5:6] &lt;- c(&quot;est&quot;, &quot;std_error&quot;) Ahora es inmediato no solo hacer gráficas sino también ajustar un modelo. # ajustamos un modelo lineal donde la variable respuesta es temperatura # máxima, y la variable explicativa es el mes fertility_sub &lt;- filter(fertility_tidy, age_bracket != &quot;global&quot;) fertility_lm &lt;- lm(est ~ age_bracket, data = fertility_sub) summary(fertility_lm) #&gt; #&gt; Call: #&gt; lm(formula = est ~ age_bracket, data = fertility_sub) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -7.3784 -2.3282 -0.5896 1.1359 31.5091 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 6.8723 0.3277 20.973 &lt; 2e-16 *** #&gt; age_bracketage_20_24 2.3948 0.4634 5.168 2.83e-07 *** #&gt; age_bracketage_25_29 2.3272 0.4634 5.022 5.99e-07 *** #&gt; age_bracketage_30_34 1.2363 0.4634 2.668 0.00775 ** #&gt; age_bracketage_35_39 -0.9413 0.4634 -2.031 0.04246 * #&gt; age_bracketage_40_44 -3.7525 0.4634 -8.098 1.52e-15 *** #&gt; age_bracketage_45_49 -6.0480 0.4634 -13.051 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 4.053 on 1064 degrees of freedom #&gt; Multiple R-squared: 0.3479, Adjusted R-squared: 0.3443 #&gt; F-statistic: 94.62 on 6 and 1064 DF, p-value: &lt; 2.2e-16 Mas de un tipo de observación en una misma tabla Esta es quizá el principio menos importante de datos limpios y su conveniencia dependerá de la situación, pero vale la pena conocerlo. En ocasiones las bases de datos involucran valores en diferentes niveles, en diferentes tipos de unidad observacional. En la limpieza de datos, cada unidad observacional debe estar almacenada en su propia tabla (esto esta ligado a normalización de una base de datos), es importante para evitar inconsistencias en los datos. ¿Cuáles son las unidades observacionales de los datos df_mxmunicipio? df_mxmunicipio #&gt; # A tibble: 2,457 x 18 #&gt; state_code municipio_code region state_name state_name_offi… state_abbr #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 01 001 01001 Aguascali… Aguascalientes AGS #&gt; 2 01 002 01002 Aguascali… Aguascalientes AGS #&gt; 3 01 003 01003 Aguascali… Aguascalientes AGS #&gt; 4 01 004 01004 Aguascali… Aguascalientes AGS #&gt; 5 01 005 01005 Aguascali… Aguascalientes AGS #&gt; 6 01 006 01006 Aguascali… Aguascalientes AGS #&gt; 7 01 007 01007 Aguascali… Aguascalientes AGS #&gt; 8 01 008 01008 Aguascali… Aguascalientes AGS #&gt; 9 01 009 01009 Aguascali… Aguascalientes AGS #&gt; 10 01 010 01010 Aguascali… Aguascalientes AGS #&gt; # … with 2,447 more rows, and 12 more variables: #&gt; # state_abbr_official &lt;chr&gt;, municipio_name &lt;chr&gt;, pop &lt;int&gt;, #&gt; # pop_male &lt;int&gt;, pop_female &lt;int&gt;, afromexican &lt;dbl&gt;, #&gt; # part_afromexican &lt;dbl&gt;, indigenous &lt;dbl&gt;, part_indigenous &lt;dbl&gt;, #&gt; # metro_area &lt;chr&gt;, long &lt;dbl&gt;, lat &lt;dbl&gt; Separemos esta base de datos en dos: la tabla a nivel municipio que almacena las variables de sexo (sex) a escolaridad media (schoolyrs). df_mxmunicipio_normal &lt;- df_mxmunicipio %&gt;% select(state_code, municipio_code, region, municipio_name:lat) glimpse(df_mxmunicipio_normal) #&gt; Observations: 2,457 #&gt; Variables: 14 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;,… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;… #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01002&quot;, &quot;01003&quot;, &quot;01004&quot;, &quot;01005&quot;, &quot;0… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Asientos&quot;, &quot;Calvillo&quot;, &quot;Cosí… #&gt; $ pop &lt;int&gt; 877190, 46464, 56048, 15577, 120405, 46473, 538… #&gt; $ pop_male &lt;int&gt; 425731, 22745, 27298, 7552, 60135, 22490, 26693… #&gt; $ pop_female &lt;int&gt; 451459, 23719, 28750, 8025, 60270, 23983, 27173… #&gt; $ afromexican &lt;dbl&gt; 532, 3, 10, 0, 32, 3, 13, 13, 4, 0, 43, 1139, 3… #&gt; $ part_afromexican &lt;dbl&gt; 2791, 130, 167, 67, 219, 74, 578, 37, 59, 60, 3… #&gt; $ indigenous &lt;dbl&gt; 104125, 1691, 7358, 2213, 8679, 6232, 6714, 173… #&gt; $ part_indigenous &lt;dbl&gt; 14209, 92, 2223, 191, 649, 251, 247, 84, 76, 24… #&gt; $ metro_area &lt;chr&gt; &quot;Aguascalientes&quot;, NA, NA, NA, &quot;Aguascalientes&quot;,… #&gt; $ long &lt;dbl&gt; -102.29605, -102.08928, -102.71875, -102.30004,… #&gt; $ lat &lt;dbl&gt; 21.87982, 22.23832, 21.84691, 22.36641, 21.9612… Y la tabla a nivel estado, que almacena las claves y variables a nivel estado. df_statecodes &lt;- df_mxmunicipio %&gt;% select(contains(&quot;state&quot;)) %&gt;% distinct() glimpse(df_statecodes) #&gt; Observations: 32 #&gt; Variables: 5 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;02&quot;, &quot;03&quot;, &quot;04&quot;, &quot;05&quot;, &quot;06&quot;, &quot;07&quot;, &quot;0… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja C… #&gt; $ state_name_official &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Baja California&quot;, &quot;Baja C… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;BC&quot;, &quot;BCS&quot;, &quot;CAMP&quot;, &quot;COAH&quot;, &quot;COL&quot;, &quot;… #&gt; $ state_abbr_official &lt;chr&gt; &quot;Ags.&quot;, &quot;BC&quot;, &quot;BCS&quot;, &quot;Camp.&quot;, &quot;Coah.&quot;, &quot;Col.… Una misma unidad observacional está almacenada en múltiples tablas También es común que los valores sobre una misma unidad observacional estén separados en muchas tablas o archivos, es común que estas tablas esten divididas de acuerdo a una variable, de tal manera que cada archivo representa a una persona, año o ubicación. Para juntar los archivos hacemos lo siguiente: Leemos los archivos en una lista de tablas. Para cada tabla agregamos una columna que registra el nombre del archivo original. Combinamos las tablas en un solo data frame. Veamos un ejemplo, descarga la carpeta con los datos de varios contaminantes de RAMA, usethis::use_zip(&quot;https://github.com/tereom/estcomp/raw/master/data-raw/19RAMA.zip&quot;, &quot;data&quot;) ésta contiene 9 archivos de excel que almacenan información de monitoreo de contaminantes. Cada archivo contiene información de un contaminante y el nombre del archivo indica el contaminante. Los pasos en R (usando el paquete purrr), primero creamos un vector con los nombres de los archivos en un directorio, eligiendo aquellos que contengan las letras “.csv”. library(here) dir_rama &lt;- here(&quot;data&quot;, &quot;19RAMA&quot;) paths &lt;- dir(dir_rama, pattern = &quot;\\\\.xls$&quot;, full.names = TRUE) paths #&gt; character(0) Después le asignamos el nombre del archivo al nombre de cada elemento del vector. Este paso se realiza para preservar los nombres de los archivos ya que estos los asignaremos a una variable mas adelante. paths &lt;- set_names(paths, basename(paths)) La función map_df itera sobre cada dirección, lee el archivo excel de dicha dirección y los combina en un data frame. library(readxl) rama &lt;- map_df(paths, read_excel, .id = &quot;FILENAME&quot;) # eliminamos la basura del id rama &lt;- rama %&gt;% mutate(PARAMETRO = str_remove(FILENAME, &quot;2019&quot;) %&gt;% str_remove(&quot;.xls&quot;)) %&gt;% select(PARAMETRO, FECHA:AJU) #&gt; Error in stri_replace_first_regex(string, pattern, fix_replacement(replacement), : object &#39;FILENAME&#39; not found # y apilamos para tener una columna por estación rama_tidy &lt;- rama %&gt;% gather(estacion, valor, ACO:AJU) %&gt;% mutate(valor = ifelse(-99, NA, valor)) #&gt; Error in is_string(x): object &#39;ACO&#39; not found rama_tidy #&gt; Error in eval(expr, envir, enclos): object &#39;rama_tidy&#39; not found Otras consideraciones En las buenas prácticas es importante tomar en cuenta los siguientes puntos: Incluir un encabezado con el nombre de las variables. Los nombres de las variables deben ser entendibles (e.g. AgeAtDiagnosis es mejor que AgeDx). En general los datos se deben guardar en un archivo por tabla. Escribir un script con las modificaciones que se hicieron a los datos crudos (reproducibilidad). Otros aspectos importantes en la limpieza de datos son: selección del tipo de variables (por ejemplo fechas), datos faltantes, typos y detección de valores atípicos. Recursos adicionales Data Transformation Cheat Sheet, RStudio. Referencias "],
["tareas.html", "Tareas", " Tareas Las tareas se envían por correo a teresa.ortiz.mancera@gmail.com con título: EstComp-TareaXX (donde XX corresponde al número de tarea, 01..). Las tareas deben incluir código y resultados (si conocen Rmarkdown es muy conveniente para este propósito). "],
["instalacion-y-visualizacion.html", "1. Instalación y visualización", " 1. Instalación y visualización 1. Instala los siguientes paquetes (o colecciones): tidyverse de CRAN (install.packages(&quot;tidyverse&quot;)) devtools de CRAN (install.packages(&quot;devtools&quot;)) gapminder de CRAN (install.packages(&quot;gapminder&quot;)) estcomp de GitHUB (debes haber instalado devtools y correr devtools::install_github(“tereom/estcomp”)`)) mxmaps instalarlo es opcional de GitHub 2. Visualización Elige un base de datos, recuerda que en la ayuda puedes encontrar más información de las variables (?gapminder): gapminder (paquete gapminder en CRAN). election_2012 ó election_sub_2012 (paquete estcomp). df_edu (paquete estcomp). enlacep_2013 o un subconjuto de este (paquete estcomp). Escribe algunas preguntas que consideres interesantes de los datos. Realiza \\(3\\) gráficas buscando explorar las preguntas de arriba y explica las relaciones que encuentres. Debes usar lo que revisamos en estas notas y al menos una de las gráficas debe ser de paneles (usando facet_wrap() o facet_grid). "],
["referencias.html", "Referencias", " Referencias "]
]
