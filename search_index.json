[
["index.html", "Estadística Computacional Información del curso", " Estadística Computacional María Teresa Ortiz Información del curso Notas del curso Estadística Computacional de los programas de maestría en Ciencia de Datos y en Computación del ITAM. Las notas fueron desarrolladas en 2014 por Teresa Ortiz quien las actualiza anualmente. En caso de encontrar errores o tener sugerencias del material se agradece la propuesta de correcciones mediante pull requests. Ligas Notas: https://tereom.github.io/est-computacional-2019/ Correo: teresa.ortiz.mancera@gmail.com GitHub: https://github.com/tereom/est-computacional-2019 Agradecimientos Se agradecen las contriubuciones a estas notas de @felipegonzalez y @mkokotchikova. Este trabajo está bajo una Licencia Creative Commons Atribución 4.0 Internacional. "],
["temario.html", "Temario", " Temario Manipulación y visualización de datos Principios de visualización. Reestructura y manipulación de datos. Temas selectos de programación en R: iteración, programación funcional, rendimiento. Referencias: Tufte (2006), Cleveland (1993), Wickham and Grolemund (2017), Wickham (2019), (???) (???), (???), (???). Inferencia y remuestreo Repaso de probabilidad. Muestreo y probabilidad. Inferencia. El principio del plug-in. Bootstrap Cálculo de errores estándar e intervalos de confianza. Estructuras de datos complejos. Referencias: Ross (1998), Efron and Tibshirani (1993), Chihara and Hesterberg (2018). Modelos de probabilidad y simulación Variables aleatorias y modelos probabilísticos. Familias importantes: discretas y continuas. Teoría básica de simulación El generador uniforme de números aleatorios. Pruebas de aleatoriedad. Simulación de variables aleatorias. Simulación para modelos gráficos Modelos probabilíticos gráficos. Simulación de modelos para: inferencia, evaluación de ajuste, cálculos de potencia/tamaño de muestra. Inferencia paramétrica y remuestreo Modelos paramétricos. Máxima verosimilitud y bootstrap paramétrico. Inferencia de gráficas Referencias: Gelman and Hill (2007), Hastie, Tibshirani, and Friedman (2001). Métodos computacionales e inferencia Bayesiana Inferencia bayesiana. Métodos diretos Familias conjugadas. Aproximación por cuadrícula. MCMC Cadenas de Markov. Metropolis. Muestreador de Gibbs. Monte Carlo Hamiltoniano. Diagnósticos de convergencia. Referencias: Kruschke (2015), Gelman et al. (2013), Gelman and Hill (2007). Calificación Tareas 20% (se envían por correo con título EstComp-TareaXX). Exámen parcial (proyecto y exámen en clase) 40%. Examen final 40%. Software R: https://www.r-project.org RStudio: https://www.rstudio.com Stan: http://mc-stan.org Otros recursos Socrative (Room ESTCOMP): Para encuestas y ejercicios en clase. Lista de correos: Suscribete si quieres recibir noticias del curso. Referencias "],
["noticias.html", "Noticias", " Noticias Los dos premios más importantes en estadística se entregaron en 2019 a Hadley Whickham y a Bradley Efron, gran parte de nuestro curso se desarrolla en torno a las contribuciones de estos dos estadísticos: Hadley Wickham cuyos paquetes, libros y artículos son los recursos esenciales para la primera parte del curso, ganó en 2019 el reconocido premio COPSS: “Por la importancia de su trabajo en el computo estadístico, visualización, gráficas y análisis de datos; por desarrollar e implementar una extensa ifraestructura computacional para el análisis de datos a través del software R; por hacer el pensamiento estadístico y el cómputo accesible a una gran audiencia; y por realzar el importante papel de la estadística entre los científicos de datos.” (2019 Presidents’ Award) Bradley Efron creador del bootstrap, que estudiaremos como segunda sección del curso, fue seleccionado en 2018 para recibir el premio internacional en estadística como reconocimiento al bootstrap, un método que desarrolló en 1977 para calcular incertidumbre en resultados científicos y que ha tenido un impacto extraordinario en muchos ámbitos. “A pesar de que la estadística no ofrece una píldora mágica para la investigación científica cuantitativa, el bootstrap es el mejor analgésico jamás producido” (Xiao-Li Meng, proff. at Harvard University.) "],
["principios-visualización.html", "Sección 1 Principios visualización", " Sección 1 Principios visualización “The simple graph has brought more information to the data analyst’s mind than any other device.” — John Tukey El cuarteto de Ascombe En 1971 un estadístico llamado Frank Anscombe (fundador del departamento de Estadística de la Universidad de Yale) publicó cuatro conjuntos de datos, cada uno consiste de 11 observaciones y tienen las mismas propiedades estadísticas. Sin embargo, cuando analizamos los datos de manera gráfica en un histograma encontramos rápidamente que los conjuntos de datos son muy distintos. #&gt; Error in library(bayesplot): there is no package called &#39;bayesplot&#39; Media de \\(x\\): 9 Varianza muestral de \\(x\\): 11 Media de \\(y\\): 7.50 Varianza muestral de \\(y\\): 4.12 Correlación entre \\(x\\) y \\(y\\): 0.816 Línea de regresión lineal: \\(y = 3.00 + 0.500x\\) En la gráfica del primer conjunto de datos, se ven datos como los que se tendrían en una relación lineal simple con un modelo que cumple los supuestos de normalidad. La segunda gráfica (arriba a la derecha) muestra unos datos que tienen una asociación pero definitivamente no es lineal. En la tercera gráfica (abajo a la izquierda) están puntos alineados perfectamente en una línea recta, excepto por uno de ellos. En la última gráfica podemos ver un ejemplo en el cual basta tener una observación atípica para que se produzca un coeficiente de correlación alto aún cuando en realidad no existe una asociación lineal entre las dos variables. El cuarteto de Ascombe inspiró una técnica para crear datos que comparten las propiedades estadísticas al igual que en el cuarteto, pero que producen gráficas muy distintas (Matejka, Fitzmaurice). "],
["introducción.html", "1.1 Introducción", " 1.1 Introducción La visualización de datos no trata de hacer gráficas “bonitas” o “divertidas”, ni de simplificar lo complejo o ayudar a una persona “que no entiende mucho” a entender ideas complejas. Más bien, trata de aprovechar nuestra gran capacidad de procesamiento visual para exhibir de manera clara aspectos importantes de los datos. El siguiente ejemplo de (Tufte 2006), ilustra claramente la diferencia entre estos dos enfoques. A la izquierda están gráficas (más o menos típicas de Powerpoint) basadas en la filosofía de simplificar, de intentar no “ahogar” al lector con datos. El resultado es una colección incoherente, de bajo contenido, que no tiene mucho qué decir y que es, “indeferente al contenido y la evidencia”. A la derecha está una variación del rediseño de Tufte en forma de tabla, que en este caso particular es una manera eficiente de mostrar claramente los patrones que hay en este conjunto simple de datos. ¿Qué principios son los que soportan la efectividad de esta tabla sobre la gráfica de la derecha? Veremos que hay dos conjuntos de principios importantes: unos relacionados con el diseño y otros con la naturaleza del análisis de datos, independientemente del método de visualización. Visualización de datos en la estadística El estándar científico para contestar preguntas o tomar decisiones es uno que se basa en el análisis de datos: para contestar preguntas o tomar decisiones es necesario, en primer lugar, reunir todos los datos disponibles que puedan contener o sugerir alguna guía para entender mejor la pregunta o la decisión a la que nos enfrentamos. Esta recopilación de datos -que pueden ser cualitativos, cuantitativos, o una mezcla de los dos, debe entonces ser analizada para extraer información relevante para nuestro problema. Tradicionalmente el análisis de datos se divide en dos distintos tipos de trabajo: El trabajo exploratorio o de detective: ¿cuáles son los aspectos importantes de estos datos? ¿qué indicaciones generales muestran los datos? ¿qué tareas de análisis debemos empezar haciendo? ¿cuáles son los caminos generales para formular con precisión y contestar algunas preguntas que nos interesen? El trabajo inferencial, confirmatorio, o de juez: ¿cómo evaluar el peso de la evidencia de los descubrimientos del paso anterior? ¿qué tan bien soportadas están las respuestas y conclusiones por nuestro conjunto de datos? Aunque en el proceso de inferencia las gráficas cada vez son más importantes, la visualización entra más claramente dentro del análisis exploratorio de datos. Y como en un principio no es claro como la visualización aporta al proceso de la inferencia, se le consideró por mucho tiempo como un área de poca importancia para la estadística: una herramienta que en todo caso sirve para comunicar ideas simples, de manera deficiente, y a personas poco sofisticadas. El peor lado de este punto de vista consiste en restringirse a el análisis estadístico rutinario Cleveland (1993): aplicar las recetas y negarse a ver los datos de distinta manera (¡incluso pensar que esto puede sesgar los resultados, o que nos podría engañar!). El siguiente ejemplo muestra un caso grave y real (no simulado) de este análisis estadístico rutinario (tomado de Cleveland (1994)). A la derecha mostramos los resultados de un experimento de agricultura. Se cultivaron diez variedades de cebada en seis sitios de Minnesota, en \\(1921\\) y \\(1932\\). Este es uno de los primeros ejemplos en el que se aplicaron las ideas de Fisher en cuanto a diseño de experimentos. En primer lugar, observamos: Los niveles generales de rendimiento varían mucho dependiendo del sitio: hay mejores y peores sitios. Los rendimientos son típicamente más altos en 1931 que en 1932. Sin embargo, Morris es anómalo en cuanto a que el patrón no es consistente con el resto de los sitios. Hay variación considerable de las variedades dentro de cada sitio. ¿Existe alguna variedad que sea mejor que otras? Notamos claramente la anomalía en las diferencias: en el sitio Morris, el año 1932 fue mejor que el de 1931. Estos datos fueron reanalizados desde la época en la que se recolectaron por muchos agrónomos. Hasta muy recientemente se detectó la anomalía en el comportamiento de los años en el sitio Morris, el cual es evidente en la gráfica. Investigación posterior ha mostrado que es muy plausible que en algún momento alguien volteó las etiquetas de los años en este sitio. Este ejemplo muestra, en primer lugar, que la visualización es crucial en el proceso de análisis de datos: sin ella estamos expuestos a no encontrar aspectos importantes de los datos (errores) que deben ser discutidos - aún cuando nuestra receta de análisis no considere estos aspectos. Ninguna receta puede aproximarse a describir todas las complejidades y detalles en un conjunto de datos de tamaño razonable (este ejemplo, en realidad, es chico). Sin embargo, la visualización de datos, por su enfoque menos estructurado, y el hecho de que se apoya en un medio con un “ancho de banda” mayor al que puede producir un cierto número de cantidades resumen, es ideal para investigar estos aspectos y detalles. Visualización popular de datos Publicaciones populares (periódicos, revistas, sitios internet) muchas veces incluyen visualización de datos como parte de sus artículos o reportajes. En general siguen el mismo patrón que en la visión tradicionalista de la estadística: sirven más para divertir que para explicar, tienden a explicar ideas simples y conjuntos chicos de datos, y se consideran como una “ayuda” para los “lectores menos sofisticados”. Casi siempre se trata de gráficas triviales (muchas veces con errores graves) que no aportan mucho a artículos que tienen un nivel de complejidad mucho mayor (es la filosofía: lo escrito para el adulto, lo graficado para el niño). Referencias "],
["teoría-de-visualización-de-datos.html", "1.2 Teoría de visualización de datos", " 1.2 Teoría de visualización de datos Existe teoría fundamentada acerca de la visualización. Después del trabajo pionero de Tukey, los principios e indicadores de Tufte se basan en un estudio de la historia de la graficación y ejercicios de muestreo de la práctica gráfica a lo largo de varias disciplinas (¿cuáles son las mejores gráficas? ¿por qué? El trabajo de Cleveland es orientado a la práctica del análisis de datos (¿cuáles gráficas nos han ayudado a mostrar claramente los resultados del análisis?), por una parte, y a algunos estudios de percepción visual. En resumen, hablaremos de las siguientes guías: Principios generales del diseño analítico Aplicables a una presentación o análisis completos, y como guía para construir nuevas visualizaciones (Tufte 2006). Principio 1. Muestra comparaciones, contrastes, diferencias. Principio 2. Muestra causalidad, mecanismo, explicación, estructura sistemática. Principio 3. Muestra datos multivariados, es decir, más de una o dos variables. Principio 4. Integra palabras, números, imágenes y diagramas. Principio 5. Describe la totalidad de la evidencia. Muestra fuentes usadas y problemas relevantes. Principio 6. Las presentaciones analíticas, a fin de cuentas, se sostienen o caen dependiendo de la calidad, relevancia e integridad de su contenido. Técnicas de visualización Esta categoría incluye técnicas específicas que dependen de la forma de nuestros datos y el tipo de pregunta que queremos investigar (Tukey (1977), Cleveland (1993), Cleveland (1994), Tufte (2006)). Tipos de gráficas: cuantiles, histogramas, caja y brazos, gráficas de dispersión, puntos/barras/ líneas, series de tiempo. Técnicas para mejorar gráficas: Transformación de datos, transparencia, vibración, banking 45, suavizamiento y bandas de confianza. Pequeños múltiplos Indicadores de calidad gráfica Aplicables a cualquier gráfica en particular. Estas son guías concretas y relativamente objetivas para evaluar la calidad de una gráfica (Tufte 1986). Integridad Gráfica. El factor de engaño, es decir, la distorsión gráfica de las cantidades representadas, debe ser mínimo. Chartjunk. Minimizar el uso de decoración gráfica que interfiera con la interpretación de los datos: 3D, rejillas, rellenos con patrones. Tinta de datos. Maximizar la proporción de tinta de datos vs. tinta total de la gráfica. For non-data- ink, less is more. For data-ink, less is a bore. Densidad de datos. Las mejores gráficas tienen mayor densidad de datos, que es la razón entre el tamaño del conjunto de datos y el área de la gráfica. Las gráficas se pueden encoger mucho. Percepción visual. Algunas tareas son más fáciles para el ojo humano que otras (Cleveland 1994). Referencias "],
["factor-de-engaño-y-chartjunk.html", "1.3 Factor de engaño y Chartjunk", " 1.3 Factor de engaño y Chartjunk El factor de engaño es el cociente entre el efecto mostrado en una gráfica y el efecto correspondiente en los datos. Idealmente, el factor de engaño debe ser 1 (ninguna distorsión). El chartjunk son aquellos elementos gráficos que no corresponden a variación de datos, o que entorpecen la interpretación de una gráfica. Estos son los indicadores de calidad más fáciles de entender y aplicar, y afortunadamente cada vez son menos comunes. Un diseño popular que califica como chartjunk y además introduce factores de engaño es el pie de 3D. En la gráfica de la derecha, podemos ver como la rebanada C se ve más grande que la rebanada A, aunque claramente ese no es el caso (factor de engaño). La razón es la variación en la perspectiva que no corresponde a variación en los datos (chartjunk). Crítica gráfica: Gráfica de Pie Todavía elementos que pueden mejorar la comprensión de nuestra gráfica de pie: se trata de la decodificiación que hay que hacer categoría - color - cuantificación. Podemos agregar las etiquetas como se muestra en la serie de la derecha, pero entonces: ¿por qué no mostrar simplemente la tabla de datos? ¿qué agrega el pie a la interpretación? La deficiencias en el pie se pueden ver claramente al intentar graficar más categorías (13). En el primer pie no podemos distinguir realmente cuáles son las categorías grandes y cuáles las chicas, y es muy difícil tener una imagen mental clara de estos datos. Agregar los porcentajes ayuda, pero entonces, otra vez, preguntamos cuál es el propósito del pie. La tabla de la izquierda hace todo el trabajo (una vez que ordenamos las categrías de la más grande a la más chica). Es posible hacer una gráfica de barras como la de abajo a la izquierda. Hay otros tipos de chartjunk comunes: uno es la textura de barras, por ejemplo. El efecto es la producción de un efecto moiré que es desagradable y quita la atención de los datos, como en la gráfica de barras de abajo. Otro común son las rejillas, como mostramos en las gráficas de la izquierda. Nótese como en estos casos hay efectos ópticos no planeados que degradan la percepción de los patrones en los datos. "],
["pequeños-múltiplos-y-densidad-gráfica.html", "1.4 Pequeños múltiplos y densidad gráfica", " 1.4 Pequeños múltiplos y densidad gráfica La densidad de una gráfica es el tamaño del conjunto de datos que se grafica comparado con el área total de la gráfica. En el siguiente ejemplo, graficamos en logaritmo-10 de cabezas de ganado en Francia (cerdos, res, ovejas y caballos). La gráfica de la izquierda es pobre en densidad pues sólo representa 4 datos. La manera más fácil de mejorar la densidad es hacer más chica la gráfica: La razón de este encogimiento es una que tiene qué ver con las oportunidades perdidas de una gráfica grande. Si repetimos este mismo patrón (misma escala, mismos tipos de ganado) para distintos países obtenemos la siguiente gráfica: Esta es una gráfica de puntos. Es útil como sustituto de una gráfica de barras, y es superior en el sentido de que una mayor proporción de la tinta que se usa es tinta de datos. Otra vez, mayor proporción de tinta de datos representa más oportunidades que se pueden capitalizar, como muestra la gráfica de punto y líneas que mostramos al principio (rendimiento en campos de cebada). Más pequeños múltiplos Los pequeños múltiplos presentan oportunidades para mostrar más acerca de nuestro problema de interés. Consideramos por ejemplo la relación de radiación solar y niveles de ozono. Podemos ver que si incluimos una variable adicional (velocidad del viento) podemos entender más acerca de la relación de radiación solar y niveles de ozono: "],
["tinta-de-datos.html", "1.5 Tinta de datos", " 1.5 Tinta de datos Maximizar la proporción de tinta de datos en nuestras gráficas tiene beneficios inmediatos. La regla es: si hay tinta que no representa variación en los datos, o la eliminación de esa tinta no representa pérdidas de significado, esa tinta debe ser eliminada. El ejemplo más claro es el de las rejillas en gráficas y tablas: ¿Por qué usar grises en lugar de negros? La respuesta tiene qué ver con el principio de tinta de datos: si marcamos las diferencias sutil pero claramente, tenemos más oportunidades abiertas para hacer énfasis en lo que nos interesa: a una gráfica o tabla saturada no se le puede hacer más - es difícil agregar elementos adicionales que ayuden a la comprensión. Si comenzamos marcando con sutileza, entonces se puede hacer más. Los mapas geográficos son un buen ejemplo de este principio. El espacio en blanco es suficientemente bueno para indicar las fronteras en una tabla, y facilita la lectura: Para un ejemplo del proceso de rediseño de una tabla, ver aquí. Finalmente, podemos ver un ejemplo que intenta incorporar los elementos del diseño analítico, incluyendo pequeños múltiplos: "],
["decoración.html", "1.6 Decoración", " 1.6 Decoración "],
["percepción-de-escala.html", "1.7 Percepción de escala", " 1.7 Percepción de escala Entre la percepción visual y la interpretación de una gráfica están implícitas tareas visuales específicas que las personas debemos realizar para ver correctamente la gráfica. En la década de los ochenta, William S. Cleveland y Robert McGill realizaron algunos experimentos identificando y clasificando estas tareas para diferentes tipos de gráficos (Cleveland and McGill 1984). En estos, se le pregunta a la persona que compare dos valores dentro de una gráfica, por ejemplo, en dos barras en una gráfica de barras, o dos rebanadas de una gráfica de pie. Los resultados de Cleveland y McGill fueron replicados por Heer y Bostock en 2010 y los resultados se muestran en las gráficas de la abajo: Imagen de Heer y Bostock, 2010 Referencias "],
["ejemplos-gráfica-de-minard.html", "1.8 Ejemplos: gráfica de Minard", " 1.8 Ejemplos: gráfica de Minard Concluimos esta sección con una gráfica que, aunque poco común, ejemplifica los principios de una buena gráfica, y es reconocida como una de las mejores visualizaciones de la historia. Una gráfica excelente, presenta datos interesantes de forma bien diseñada: es una cuestión de fondo, de diseño, y estadística… [Se] compone de ideas complejas comunicadas con claridad, precisión y eficiencia. … [Es] lo que da al espectador la mayor cantidad de ideas, en el menor tiempo, con la menor cantidad de tinta, y en el espacio más pequeño. … Es casi siempre multivariado. … Una excelente gráfica debe decir la verdad acerca de los datos. (Tufte, 1983) La famosa visualización de Charles Joseph Minard de la marcha de Napoleón sobre Moscú, ilustra los principios de una buena gráfica. Tufte señala que esta imagen “bien podría ser el mejor gráfico estadístico jamás dibujado”, y sostiene que “cuenta una historia rica y coherente con sus datos multivariados, mucho más esclarecedora que un solo número que rebota en el tiempo”. Se representan seis variables: el tamaño del ejército, su ubicación en una superficie bidimensional, la dirección del movimiento del ejército y la temperatura en varias fechas durante la retirada de Moscú“. Hoy en día Minard es reconocido como uno de los principales contribuyentes a la teoría de análisis de datos y creación de infografías con un fundamento estadístico. Se grafican 6 variables: el número de tropas de Napoleón, la distancia, la temperatura, la ubicación (latitud y longitud), la dirección en que viajaban las tropas y la localización relativa a fechas específicas. La gráfica de Minard, como la describe E.J. Marey, parece “desafiar la pluma del historiador con su brutal elocuencia”, la combinación de datos del mapa, y la serie de tiempo, dibujados en 1869, “retratan una secuencia de pérdidas devastadoras que sufrieron las tropas de Napoleón en 1812”. Comienza en la izquierda, en la frontera de Polonia y Rusia, cerca del río Niemen. La línea gruesa dorada muestra el tamaño de la Gran Armada (422,000) en el momento en que invadía Rusia en junio de 1812. El ancho de esta banda indica el tamaño de la armada en cada punto del mapa. En septiembre, la armada llegó a Moscú, que ya había sido saqueada y dejada desértica, con sólo 100,000 hombres. El camino del retiro de Napoleón desde Moscú está representado por la línea oscuara (gris) que está en la parte inferior, que está relacionada a su vez con la temperatura y las fechas en el diagrama de abajo. Fue un invierno muy frío, y muchos se congelaron en su salida de Rusia. Como se muestra en el mapa, cruzar el río Berezina fue un desastre, y el ejército de Napoleón logró regresar a Polonia con tan sólo 10,000 hombres. También se muestran los movimientos de las tropas auxiliaries, que buscaban proteger por atrás y por la delantera mientras la armada avanzaba hacia Moscú. La gráfica de Minard cuenta una historia rica y cohesiva, coherente con datos multivariados y con los hechos históricos, y que puede ser más ilustrativa que tan sólo representar un número rebotando a lo largo del tiempo. "],
["introducción-a-r-y-al-paquete-ggplot2.html", "Sección 2 Introducción a R y al paquete ggplot2", " Sección 2 Introducción a R y al paquete ggplot2 ¿Qué es R? R es un lenguaje de programación y un ambiente de cómputo estadístico R es software libre (no dice qué puedes o no hacer con el software), de código abierto (todo el código de R se puede inspeccionar - y se inspecciona). Cuando instalamos R, instala la base de R. Mucha de la funcionalidad adicional está en paquetes (conjunto de funciones y datos documentados) que la comunidad contribuye. ¿Cómo entender R? Hay una sesión de R corriendo. La consola de R es la interfaz entre R y nosotros. En la sesión hay objetos. Todo en R es un objeto: vectores, tablas, funciones, etc. Operamos aplicando funciones a los objetos y creando nuevos objetos. ¿Por qué R? R funciona en casi todas las plataformas (Mac, Windows, Linux e incluso en Playstation 3). R es un lenguaje de programación completo, permite desarrollo de DSLs. R promueve la investigación reproducible. R está actualizado gracias a que tiene una activa comunidad. Solo en CRAN hay cerca de \\(10,000\\) paquetes (funcionalidad adicional de R creadas creada por la comunidad). R se puede combinar con otras herramientas. R tiene capacidades gráficas muy sofisticadas. R es popular (Revolutions blog). "],
["r-primeros-pasos.html", "2.1 R: primeros pasos", " 2.1 R: primeros pasos Para comenzar se debe descargar R, esta descarga incluye R básico y un editor de textos para escribir código. Después de descargar R se recomienda descargar RStudio (gratis y libre). Rstudio es un ambiente de desarrollo integrado para R: incluye una consola, un editor de texto y un conjunto de herramientas para administrar el espacio de trabajo cuando se utiliza R. Algunos shortcuts útiles en RStudio son: En el editor command/ctrl + enter: enviar código a la consola ctrl + 2: mover el cursor a la consola En la consola flecha hacia arriba: recuperar comandos pasados ctrl + flecha hacia arriba: búsqueda en los comandos ctrl + 1: mover el cursor al editor R en análisis de datos El estándar científico para contestar preguntas o tomar decisiones es uno que se basa en el análisis de datos. Aquí consideramos técnicas cuantitativas: recolectar, organizar, entender, interpretar y extraer información de colecciones de datos predominantemente numéricos. Todas estas tareas son partes del análisis de datos, cuyo proceso podría resumirse con el siguiente diagrama: Es importante la forma en que nos movemos dentro de estos procesos en el análisis de datos y en este curso buscamos dar herramientas para facilitar cumplir los siguientes principios: Reproducibilidad. Debe ser posible reproducir el análisis en todos sus pasos, en cualquier momento. Claridad. Los pasos del análisis deben estar documentados apropiadamente, de manera que las decisiones importantes puedan ser entendidas y explicadas claramente. Dedicaremos las primeras sesiones a aprender herramientas básicas para poder movernos agilmente a lo largo de las etapas de análisis utilizando R y nos enfocaremos en los paquetes que forman parte del tidyverse. Paquetes y el Tidyverse La mejor manera de usar R para análisis de datos es aprovechando la gran cantidad de paquetes que aportan funcionalidad adicional. Desde Rstudio podemos instalar paquetes (Tools - &gt; Install packages o usar la función install.packages(&quot;nombre_paquete&quot;)). Las siguientes lineas instalan los paquetes devtools y readr. install.packages(&quot;devtools&quot;) install.packages(&quot;readr&quot;) Una vez instalados, podemos cargarlos a nuestra sesión de R mediante library. Por ejemplo, para cargar el paquete readr hacemos: print(read_csv) #&gt; function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), #&gt; na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, #&gt; trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, #&gt; n_max), progress = show_progress(), skip_empty_rows = TRUE) #&gt; { #&gt; tokenizer &lt;- tokenizer_csv(na = na, quoted_na = quoted_na, #&gt; quote = quote, comment = comment, trim_ws = trim_ws, #&gt; skip_empty_rows = skip_empty_rows) #&gt; read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, #&gt; locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, #&gt; comment = comment, n_max = n_max, guess_max = guess_max, #&gt; progress = progress) #&gt; } #&gt; &lt;bytecode: 0x9aa8060&gt; #&gt; &lt;environment: namespace:readr&gt; library(readr) print(read_csv) #&gt; function (file, col_names = TRUE, col_types = NULL, locale = default_locale(), #&gt; na = c(&quot;&quot;, &quot;NA&quot;), quoted_na = TRUE, quote = &quot;\\&quot;&quot;, comment = &quot;&quot;, #&gt; trim_ws = TRUE, skip = 0, n_max = Inf, guess_max = min(1000, #&gt; n_max), progress = show_progress(), skip_empty_rows = TRUE) #&gt; { #&gt; tokenizer &lt;- tokenizer_csv(na = na, quoted_na = quoted_na, #&gt; quote = quote, comment = comment, trim_ws = trim_ws, #&gt; skip_empty_rows = skip_empty_rows) #&gt; read_delimited(file, tokenizer, col_names = col_names, col_types = col_types, #&gt; locale = locale, skip = skip, skip_empty_rows = skip_empty_rows, #&gt; comment = comment, n_max = n_max, guess_max = guess_max, #&gt; progress = progress) #&gt; } #&gt; &lt;bytecode: 0x9aa8060&gt; #&gt; &lt;environment: namespace:readr&gt; read_csv es una función que aporta el paquete readr, que a su vez está incluido en el tidyverse. El paquete de arriba se instaló de CRAN, pero podemos instalar paquetes que están en otros repositorios (por ejemplo BioConductor) o paquetes que están en GitHub. library(devtools) install_github(&quot;tereom/estcomp&quot;) Los paquetes se instalan una sola vez, sin embargo, se deben cargar (ejecutar library(readr)) en cada sesión de R que los ocupemos. En estas notas utilizaremos la colección de paquetes incluídos en el tidyverse. Estos paquetes de R están diseñados para ciencia de datos, y para funcionar juntos como parte de un flujo de trabajo. La siguiente imagen tomada de Why the tidyverse (Joseph Rickert) indica que paquetes del tidyverse se utilizan para cada etapa del análisis de datos. knitr::include_graphics(&quot;img/tidyverse.png&quot;) Recursos Existen muchos recursos gratuitos para aprender R, y resolver nuestras dudas: Buscar ayuda: Google, StackOverflow o RStudio Community. Para aprender más sobre un paquete o una función pueden visitar Rdocumentation.org. La referencia principal de estas notas es el libro R for Data Science de Hadley Wickham. RStudio tiene una Lista de recursos en línea. Para aprender programación avanzada en R, el libro gratuito Advanced R de Hadley Wickham es una buena referencia. En particular es conveniente leer la guía de estilo (para todos: principiantes, intermedios y avanzados). Para mantenerse al tanto de las noticias de la comunidad de R pueden seguir #rstats en Twitter. Para aprovechar la funcionalidad de RStudio. "],
["visualización-con-ggplot2.html", "2.2 Visualización con ggplot2", " 2.2 Visualización con ggplot2 Utilizaremos el paquete ggplot2, fue desarrollado por Hadley Wickham y es una implementación de la gramática de las gráficas (Wilkinson et al. 2005). Si no lo tienes instalado comienza instalando el paquete ggplot2 o el tidyverse que lo incluye. Gráficas de dispersión library(tidyverse) # Cargamos el paquete en nuestra sesión Usaremos el conjunto de datos election_sub_2012 que se incluye en el paquete estcomp, puedes encontrar información de esta base de datos tecleando ?election_sub_2012. library(estcomp) data(election_sub_2012) ?election_sub_2012 glimpse(election_sub_2012) #&gt; Observations: 1,500 #&gt; Variables: 23 #&gt; $ state_code &lt;chr&gt; &quot;19&quot;, &quot;30&quot;, &quot;09&quot;, &quot;07&quot;, &quot;09&quot;, &quot;27&quot;, &quot;20&quot;, &quot;15&quot;, &quot;25&quot;,… #&gt; $ state_name &lt;chr&gt; &quot;Nuevo León&quot;, &quot;Veracruz&quot;, &quot;Ciudad de México&quot;, &quot;Chiapa… #&gt; $ state_abbr &lt;chr&gt; &quot;NL&quot;, &quot;VER&quot;, &quot;CDMX&quot;, &quot;CHPS&quot;, &quot;CDMX&quot;, &quot;TAB&quot;, &quot;OAX&quot;, &quot;M… #&gt; $ district_loc_17 &lt;int&gt; 20, 30, 27, 5, 26, 21, 15, 43, 4, 19, 17, 9, 9, 6, 7,… #&gt; $ district_fed_17 &lt;int&gt; 7, 11, 22, 5, 15, 6, 3, 7, 4, 5, 6, 4, 1, 1, 3, 7, 4,… #&gt; $ polling_id &lt;int&gt; 90532, 134417, 32160, 15456, 31925, 122541, 94510, 63… #&gt; $ section &lt;int&gt; 347, 1775, 2705, 1121, 4358, 502, 37, 826, 2207, 1028… #&gt; $ region &lt;chr&gt; &quot;noreste&quot;, &quot;este&quot;, &quot;centrosur&quot;, &quot;suroeste&quot;, &quot;centrosu… #&gt; $ polling_type &lt;chr&gt; &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C&quot;, &quot;B-C… #&gt; $ section_type &lt;chr&gt; &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;U&quot;, &quot;M&quot;, NA, &quot;U&quot;, &quot;M&quot;, &quot;R&quot;,… #&gt; $ pri_pvem &lt;int&gt; 150, 146, 103, 135, 108, 102, 121, 157, 134, 187, 44,… #&gt; $ pan &lt;int&gt; 111, 52, 43, 33, 95, 27, 60, 94, 40, 128, 53, 56, 49,… #&gt; $ panal &lt;int&gt; 12, 4, 10, 10, 4, 4, 8, 16, 4, 10, 0, 12, 8, 1, 106, … #&gt; $ prd_pt_mc &lt;int&gt; 78, 226, 240, 237, 181, 290, 141, 158, 90, 63, 6, 179… #&gt; $ otros &lt;int&gt; 1, 4, 4, 7, 6, 14, 1, 8, 8, 11, 2, 14, 7, 12, 48, 19,… #&gt; $ total &lt;int&gt; 352, 432, 400, 422, 394, 437, 331, 433, 276, 399, 105… #&gt; $ nominal_list &lt;int&gt; 675, 636, 688, 672, 522, 698, 596, 716, 506, 584, 188… #&gt; $ pri_pvem_pct &lt;dbl&gt; 43, 34, 26, 32, 27, 23, 37, 36, 49, 47, 42, 42, 25, 4… #&gt; $ pan_pct &lt;dbl&gt; 32, 12, 11, 8, 24, 6, 18, 22, 14, 32, 50, 12, 16, 37,… #&gt; $ panal_pct &lt;dbl&gt; 3, 1, 2, 2, 1, 1, 2, 4, 1, 3, 0, 3, 3, 0, 28, 0, 5, 1… #&gt; $ prd_pt_mc_pct &lt;dbl&gt; 22, 52, 60, 56, 46, 66, 43, 36, 33, 16, 6, 40, 54, 18… #&gt; $ otros_pct &lt;dbl&gt; 0, 1, 1, 2, 2, 3, 0, 2, 3, 3, 2, 3, 2, 3, 13, 8, 2, 2… #&gt; $ winner &lt;chr&gt; &quot;pri_pvem&quot;, &quot;prd_pt_mc&quot;, &quot;prd_pt_mc&quot;, &quot;prd_pt_mc&quot;, &quot;p… Comencemos con nuestra primera gráfica: ggplot(data = election_sub_2012) + geom_point(mapping = aes(x = total, y = prd_pt_mc)) En ggplot2 se inicia una gráfica con la instrucción ggplot(), debemos especificar explicitamente que base de datos usamos, este es el primer argumento en la función ggplot(). Una vez que creamos la base añadimos capas, y dentro de aes() escribimos las variables que queremos graficar y el atributo de la gráfica al que queremos mapearlas. La función geom_point() añade una capa de puntos, hay muchas funciones geometrías incluídas en ggplot2: geom_line(), geom_boxplot(), geom_histogram,… Cada una acepta distintos argumentos para mapear las variables en los datos a características estéticas de la gráfica. En el ejemplo de arriba mapeamos displ al eje x, prd_pt_mc al eje y, pero geom_point() nos permite representar más variables usando la forma, color y/o tamaño del punto. Esta flexibilidad nos permite entender o descubrir patrones más interesantes en los datos. ggplot(election_sub_2012) + geom_point(aes(x = total, y = prd_pt_mc, color = polling_type)) Experimenta con los aesthetics color (color), tamaño (size) y forma (shape). ¿Qué diferencia hay entre las variables categóricas y las continuas? ¿Qué ocurre cuando combinas varios aesthetics? El mapeo de las propiedades estéticas se denomina escalamiento y depende del tipo de variable, las variables discretas (por ejemplo, tipo de casilla, región, estado) se mapean a distintas escalas que las variables continuas (variables numéricas como voto por un partido, lista nominal, etc.), los defaults de escalamiento para algunos atributos son (los escalamientos se pueden modificar): aes Discreta Continua Color (color) Arcoiris de colores Gradiente de colores Tamaño (size) Escala discreta de tamaños Mapeo lineal entre el área y el valor Forma (shape) Distintas formas No aplica Transparencia (alpha) No aplica Mapeo lineal a la transparencia Los geoms controlan el tipo de gráfica p &lt;- ggplot(election_sub_2012, aes(x = total, y = prd_pt_mc)) p + geom_line() ¿Qué problema tiene la siguiente gráfica? p &lt;- ggplot(election_sub_2012, aes(x = pan_pct, y = prd_pt_mc_pct)) p + geom_point(size = 0.8) p + geom_jitter(size = 0.8) ¿Cómo podemos mejorar la siguiente gráfica? ggplot(election_sub_2012, aes(x = state_abbr, y = prd_pt_mc_pct)) + geom_point(size = 0.8) Intentemos reodenar los niveles de la variable clase ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_point(size = 0.8) Podemos probar otros geoms. ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_jitter(size = 0.8) ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_boxplot() También podemos usar más de un geom! ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_jitter(size = 0.8) + geom_boxplot() Y mejorar presentación: ggplot(election_sub_2012, aes(x = reorder(state_abbr, prd_pt_mc), y = prd_pt_mc)) + geom_jitter(alpha = 0.6, size = 0.8) + geom_boxplot(outlier.color = NA) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) + labs(title = &quot;Votos por casilla y estado&quot;, subtitle = &quot;PRD-PT-MC&quot;, x = &quot;estado&quot;, y = &quot;total de votos&quot;) Lee la ayuda de reorder y repite las gráficas anteriores ordenando por la mediana de prd_pt_mc. ¿Cómo harías para graficar los puntos encima de las cajas de boxplot? Paneles Ahora veremos como hacer gráficas de paneles, la idea es hacer varios múltiplos de una gráfica donde cada múltiplo representa un subconjunto de los datos, es una práctica muy útil para explorar relaciones condicionales. En ggplot podemos usar facet_wrap() para hacer paneles dividiendo los datos de acuerdo a las categorías de una sola variable ggplot(election_sub_2012, aes(x = reorder(state_abbr, pri_pvem_pct, median), y = pri_pvem_pct)) + geom_boxplot() + facet_wrap(~ section_type) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Podemos eliminar los NA. Veremos la función filter() en la próxima sesión. ggplot(filter(election_sub_2012, !is.na(section_type)), aes(x = reorder(state_abbr, pri_pvem_pct, median), y = pri_pvem_pct)) + geom_boxplot() + facet_wrap(~ section_type) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) También podemos hacer una cuadrícula de \\(2\\) dimensiones usando facet\\_grid(filas~columnas) # Veremos como manipular datos en las próximas clases election_region_2012 &lt;- election_2012 %&gt;% group_by(region, section_type) %&gt;% summarise_at(vars(pri_pvem:total), sum) %&gt;% mutate_at(vars(pri_pvem:otros), .funs = ~ 100 * ./total) %&gt;% ungroup() %&gt;% mutate(region = reorder(region, pri_pvem)) %&gt;% gather(party, prop_votes, pri_pvem:otros) %&gt;% filter(!is.na(section_type)) ggplot(election_region_2012, aes(x = reorder(party, prop_votes), y = prop_votes, fill = reorder(party, -prop_votes))) + geom_col(show.legend = FALSE) + facet_grid(region ~ section_type) + theme(axis.text.x = element_text(angle = 45, hjust = 1)) Los páneles pueden ser muy útiles para entender relaciones en nuestros datos. En la siguiente gráfica es difícil entender si existe una relación entre radiación solar y ozono. data(airquality) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() #&gt; Warning: Removed 42 rows containing missing values (geom_point). Veamos que ocurre si realizamos páneles separando por velocidad del viento. library(Hmisc) airquality$Wind.cat &lt;- cut2(airquality$Wind, g = 3) ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() + facet_wrap(~ Wind.cat) Podemos agregar un suavizador (loess) para ver mejor la relación de las variables en cada panel. ggplot(airquality, aes(x = Solar.R, y = Ozone)) + geom_point() + facet_wrap(~ Wind.cat) + geom_smooth(method = &quot;lm&quot;) Como vimos en el caso de los resultados electorales por región, en ocasiones es necesario realizar transformaciones u obtener subconjuntos de los datos para poder responder preguntas de nuestro interés. library(dplyr) library(babynames) glimpse(babynames) #&gt; Observations: 1,924,665 #&gt; Variables: 5 #&gt; $ year &lt;dbl&gt; 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880, 1880… #&gt; $ sex &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;,… #&gt; $ name &lt;chr&gt; &quot;Mary&quot;, &quot;Anna&quot;, &quot;Emma&quot;, &quot;Elizabeth&quot;, &quot;Minnie&quot;, &quot;Margaret&quot;, &quot;Ida&quot;… #&gt; $ n &lt;int&gt; 7065, 2604, 2003, 1939, 1746, 1578, 1472, 1414, 1320, 1288, 1258… #&gt; $ prop &lt;dbl&gt; 0.07238359, 0.02667896, 0.02052149, 0.01986579, 0.01788843, 0.01… Supongamos que queremos ver la tendencia del nombre “John”, para ello debemos generar un subconjunto de la base de datos. ¿Qué ocurre en la siguiente gráfica? babynames_John &lt;- filter(babynames, name == &quot;Teresa&quot;) ggplot(babynames_John, aes(x = year, y = prop)) + geom_line() ggplot(babynames_John, aes(x = year, y = prop, color = sex)) + geom_line() La preparación de los datos es un aspecto muy importante del análisis y suele ser la fase que lleva más tiempo. Es por ello que el siguiente tema se enfocará en herramientas para hacer transformaciones de manera eficiente. Recursos El libro R for Data Science (Wickham and Grolemund 2017) tiene un capítulo de visualización. Documentación con ejemplos en la página de ggplot2. Otro recurso muy útil es el acordeón de ggplot. La teoría detrás de ggplot2 se explica en el libro de ggplot2 (Wickham 2009), Google, stackoverflow y RStudio Community tienen un tag para preguntas relacionadas con ggplot2. Referencias "],
["manipulación-y-agrupación-de-datos.html", "Sección 3 Manipulación y agrupación de datos", " Sección 3 Manipulación y agrupación de datos En esta sección continuamos con la introducción a R para análisis de datos, en particular mostraremos herramientas de manipulación y transformación de datos. Trataremos los siguientes puntos: Transformación de datos. Estrategia separa-aplica-combina. Reestructura de datos y el principio de los datos limpios. Es sabido que limpieza y preparación de datos ocupan gran parte del tiempo del análisis de datos (Dasu y Johnson, 2003 y NYT’s ‘Janitor Work’ Is Key Hurdle to Insights), es por ello que vale la pena dedicar un tiempo a aprender técnicas que faciliten estas tareas, y entender que estructura en los datos es más conveniente para trabajar. "],
["transformación-de-datos.html", "3.1 Transformación de datos", " 3.1 Transformación de datos Es sorprendente que una gran variedad de necesidades de transformación de datos se pueden resolver con pocas funciones, en esta sección veremos 5 verbos que fueron diseñados para la tarea de transformación de datos y que comparten una filosofía en cuanto a su estructura. Estudiaremos las siguientes funciones: filter: obten un subconjunto de las filas de acuerdo a un criterio. select: selecciona columnas de acuerdo al nombre arrange: reordena las filas mutate: agrega nuevas variables summarise: reduce variables a valores (crear nuevas bases de datos con resúmenes de variables de la base original) Estas funciones trabajan de manera similar, el primer argumento que reciben es un data.frame, los argumentos que siguen indican que operación se va a efectuar y el resultado es un nuevo data.frame. Adicionalmente, se pueden usar con group_by() que veremos más adelante y que cambia el dominio de cada función, pasando de operar en el conjunto de datos completos a operar en grupos. Datos Usaremos datos de población municipal incluidos en el paquete mxmaps y datos de educación, situación conyugal y hogar incluídos en el estcomp, para tener acceso a ellos cargamos los paquetes correspondientes. library(tidyverse, warn.conflicts = FALSE, quietly = TRUE) library(mxmaps) library(estcomp) Una alternatica a instalar mxmaps es leer únicamente los datos, se descargan del repositorio de GitHub y se cargan con la función load(). download.file(&quot;https://github.com/diegovalle/mxmaps/blob/master/data/df_mxmunicipio.RData?raw=true&quot;, &quot;df_mxmunicipio.RData&quot;) load(&quot;df_mxmunicipio.RData&quot;) Observemos la estructura de los datos: df_mxmunicipio &lt;- as_tibble(df_mxmunicipio) glimpse(df_mxmunicipio) #&gt; Observations: 2,457 #&gt; Variables: 18 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, … #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01002&quot;, &quot;01003&quot;, &quot;01004&quot;, &quot;01005&quot;, &quot;010… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_name_official &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, … #&gt; $ state_abbr_official &lt;chr&gt; &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Asientos&quot;, &quot;Calvillo&quot;, &quot;Cosío&quot;… #&gt; $ pop &lt;int&gt; 877190, 46464, 56048, 15577, 120405, 46473, 53866… #&gt; $ pop_male &lt;int&gt; 425731, 22745, 27298, 7552, 60135, 22490, 26693, … #&gt; $ pop_female &lt;int&gt; 451459, 23719, 28750, 8025, 60270, 23983, 27173, … #&gt; $ afromexican &lt;dbl&gt; 532, 3, 10, 0, 32, 3, 13, 13, 4, 0, 43, 1139, 351… #&gt; $ part_afromexican &lt;dbl&gt; 2791, 130, 167, 67, 219, 74, 578, 37, 59, 60, 377… #&gt; $ indigenous &lt;dbl&gt; 104125, 1691, 7358, 2213, 8679, 6232, 6714, 1733,… #&gt; $ part_indigenous &lt;dbl&gt; 14209, 92, 2223, 191, 649, 251, 247, 84, 76, 244,… #&gt; $ metro_area &lt;chr&gt; &quot;Aguascalientes&quot;, NA, NA, NA, &quot;Aguascalientes&quot;, N… #&gt; $ long &lt;dbl&gt; -102.29605, -102.08928, -102.71875, -102.30004, -… #&gt; $ lat &lt;dbl&gt; 21.87982, 22.23832, 21.84691, 22.36641, 21.96127,… glimpse(df_edu) #&gt; Observations: 7,371 #&gt; Variables: 16 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, … #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;002&quot;, &quot;002&quot;, &quot;002&quot;, &quot;003&quot;, &quot;003&quot;… #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01002&quot;, &quot;01002&quot;, &quot;01002&quot;, … #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ sex &lt;chr&gt; &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mujeres&quot;, &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mu… #&gt; $ pop_15 &lt;dbl&gt; 631064, 301714, 329350, 31013, 14991, 16022, 38678, 18… #&gt; $ no_school &lt;dbl&gt; 2.662329, 2.355211, 2.943677, 4.011221, 4.389300, 3.65… #&gt; $ preschool &lt;dbl&gt; 0.17335801, 0.17466873, 0.17215728, 0.25795634, 0.2935… #&gt; $ elementary &lt;dbl&gt; 20.15247, 18.60073, 21.57401, 33.77938, 35.48129, 32.1… #&gt; $ secondary &lt;dbl&gt; 29.31145, 30.37976, 28.33278, 39.21259, 37.45581, 40.8… #&gt; $ highschool &lt;dbl&gt; 23.31824, 22.84912, 23.74799, 16.07068, 15.67607, 16.4… #&gt; $ higher_edu &lt;dbl&gt; 24.291989, 25.560299, 23.130105, 6.355399, 6.357148, 6… #&gt; $ other &lt;dbl&gt; 0.09016518, 0.08020841, 0.09928647, 0.31277206, 0.3468… #&gt; $ schoolyrs &lt;dbl&gt; 10.211152, 10.380144, 10.056383, 7.854005, 7.692086, 8… Filtrar Creamos una tabla de datos de juguete para mostrar el funcionamiento de cada instrucción: df_ej &lt;- tibble(sexo = c(&quot;mujer&quot;, &quot;hombre&quot;, &quot;mujer&quot;, &quot;mujer&quot;, &quot;hombre&quot;), estatura = c(1.65, 1.80, 1.70, 1.60, 1.67)) df_ej #&gt; # A tibble: 5 x 2 #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 El primer argumento de filter() es el nombre del data frame, los subsecuentes son las expresiones que indican que filas filtrar. filter(df_ej, sexo == &quot;mujer&quot;) #&gt; # A tibble: 3 x 2 #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 mujer 1.7 #&gt; 3 mujer 1.6 filter(df_ej, estatura &gt; 1.65 &amp; estatura &lt; 1.75) #&gt; # A tibble: 2 x 2 #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.7 #&gt; 2 hombre 1.67 Algunos operadores importantes para filtrar son: x &gt; 1 x &gt;= 1 x &lt; 1 x &lt;= 1 x != 1 x == 1 x %in% c(&quot;a&quot;, &quot;b&quot;) Crea un subconjunto de los datos df_mxmunicipio que contenga únicamente los municipios de la CDMX (state_abbr es CDMX) Los municipios de Nuevo León con más de 200,000 habitantes. Los municipios donde más de la mitad la población se autoidentifica como afromexicana o parte afromexicana. Observación == y operadores booleanos Debemos tener cuidado al usar ==, ¿qué devuelven las siguientes expresiones? sqrt(2) ^ 2 == 2 1/49 * 49 == 1 Los resultados de arriba se deben a que las computadoras usan aritmética de precisión finita: print(1/49 * 49, digits = 22) #&gt; [1] 0.9999999999999998889777 Para estos casos es útil usar la función near() near(sqrt(2) ^ 2, 2) #&gt; [1] TRUE near(1 / 49 * 49, 1) #&gt; [1] TRUE Los operadores booleanos también son convenientes para filtrar: # Conjuntos a | b # a o b a &amp; b # a y b a &amp; !b # a y no-b xor(a, b) El siguiente esquema nos ayuda a entender que hace cada operación, x está representada por el círculo del lado izquierdo y y por el círculo del lado derecho, la parte sombreada muestra las regiones que selecciona el operador: Figure 3.1: Operaciones booleanas, imagen del libro r4ds. Observación: faltantes NA Un caso común es cuando se desea eliminar o localizar los registros con faltantes en una o más columnas de las tablas de datos, en R los datos faltantes se expresan como NA, para seleccionar los registros con faltante en la variable schoolyrs de los datos df_edu resulta natural escribir: filter(df_edu, schoolyrs == NA) #&gt; # A tibble: 0 x 16 #&gt; # … with 16 variables: state_code &lt;chr&gt;, municipio_code &lt;chr&gt;, region &lt;chr&gt;, #&gt; # state_name &lt;chr&gt;, state_abbr &lt;chr&gt;, municipio_name &lt;chr&gt;, sex &lt;chr&gt;, #&gt; # pop_15 &lt;dbl&gt;, no_school &lt;dbl&gt;, preschool &lt;dbl&gt;, elementary &lt;dbl&gt;, #&gt; # secondary &lt;dbl&gt;, highschool &lt;dbl&gt;, higher_edu &lt;dbl&gt;, other &lt;dbl&gt;, #&gt; # schoolyrs &lt;dbl&gt; Y para eliminarlos filter(df_edu, schoolyrs != NA) #&gt; # A tibble: 0 x 16 #&gt; # … with 16 variables: state_code &lt;chr&gt;, municipio_code &lt;chr&gt;, region &lt;chr&gt;, #&gt; # state_name &lt;chr&gt;, state_abbr &lt;chr&gt;, municipio_name &lt;chr&gt;, sex &lt;chr&gt;, #&gt; # pop_15 &lt;dbl&gt;, no_school &lt;dbl&gt;, preschool &lt;dbl&gt;, elementary &lt;dbl&gt;, #&gt; # secondary &lt;dbl&gt;, highschool &lt;dbl&gt;, higher_edu &lt;dbl&gt;, other &lt;dbl&gt;, #&gt; # schoolyrs &lt;dbl&gt; en ambos casos nos devuelve una tabla vacía! El problema resulta de usar los operadores == y !=, pensemos ¿qué regresan las siguientes expresiones? 5 + NA NA / 2 sum(c(5, 4, NA)) mean(c(5, 4, NA)) NA &lt; 3 NA == 3 NA == NA Las expresiones anteriores regresan NA, el hecho que la media de un vector que incluye NAs o su suma regrese NAs se debe a que por defecto en R se propagan los valores faltantes, esto es, si deconozco el valor de una de las componentes de un vector, también desconozco la suma del mismo; sin embargo, muchas funciones tienen un argumento na.rm para eliminarlos, sum(c(5, 4, NA), na.rm = TRUE) #&gt; [1] 9 mean(c(5, 4, NA), na.rm = TRUE) #&gt; [1] 4.5 Aún queda pendiente como filtrarlos en una tabla, para esto veamos que el manejo de datos faltantes en R utiliza una lógica ternaria (como SQL): NA == NA #&gt; [1] NA La expresión anterior puede resultar confusa, una manera de pensar en esto es considerar los NA como no sé, por ejemplo si no se la edad de Juan y no se la edad de Esteban, la respuesta a ¿Juan tiene la misma edad que Esteban? es no sé (NA). edad_Juan &lt;- NA edad_Esteban &lt;- NA edad_Juan == edad_Esteban #&gt; [1] NA edad_Jose &lt;- 32 # Juan es menor que José? edad_Juan &lt; edad_Jose #&gt; [1] NA Por tanto para determinar si un valor es faltante usamos la instrucción is.na(). is.na(NA) #&gt; [1] TRUE Y finalmente podemos filtrar, filter(df_edu, is.na(schoolyrs)) Seleccionar Elegir columnas de un conjunto de datos. df_ej #&gt; # A tibble: 5 x 2 #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 select(df_ej, sexo) #&gt; # A tibble: 5 x 1 #&gt; sexo #&gt; &lt;chr&gt; #&gt; 1 mujer #&gt; 2 hombre #&gt; 3 mujer #&gt; 4 mujer #&gt; 5 hombre select(df_ej, -sexo) #&gt; # A tibble: 5 x 1 #&gt; estatura #&gt; &lt;dbl&gt; #&gt; 1 1.65 #&gt; 2 1.8 #&gt; 3 1.7 #&gt; 4 1.6 #&gt; 5 1.67 select(df_ej, starts_with(&quot;s&quot;)) select(df_ej, contains(&quot;x&quot;)) Ve la ayuda de select (?select) y escribe tres maneras de seleccionar las variables del estado en los datos df_mxmunicipio. Ordenar Ordenar de acuerdo al valor de una o más variables: arrange(df_ej, sexo) #&gt; # A tibble: 5 x 2 #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.8 #&gt; 2 hombre 1.67 #&gt; 3 mujer 1.65 #&gt; 4 mujer 1.7 #&gt; 5 mujer 1.6 arrange(df_ej, desc(estatura)) #&gt; # A tibble: 5 x 2 #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 hombre 1.8 #&gt; 2 mujer 1.7 #&gt; 3 hombre 1.67 #&gt; 4 mujer 1.65 #&gt; 5 mujer 1.6 Ordena los municipios por población, de mayor a menor. ¿Cuáles son los municipios con mayor disparidad de sexo (a total)? ¿Cuáles son los municipios con mayor disparidad de sexo (proporcional)?, elimina los municipios con menos de 5000 habitantes y repite. Mutar Mutar consiste en crear nuevas variables aplicando una función a columnas existentes: mutate(df_ej, estatura_cm = estatura * 100) #&gt; # A tibble: 5 x 3 #&gt; sexo estatura estatura_cm #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 165 #&gt; 2 hombre 1.8 180 #&gt; 3 mujer 1.7 170 #&gt; 4 mujer 1.6 160 #&gt; 5 hombre 1.67 167 mutate(df_ej, estatura_cm = estatura * 100, estatura_in = estatura_cm * 0.3937) #&gt; # A tibble: 5 x 4 #&gt; sexo estatura estatura_cm estatura_in #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 165 65.0 #&gt; 2 hombre 1.8 180 70.9 #&gt; 3 mujer 1.7 170 66.9 #&gt; 4 mujer 1.6 160 63.0 #&gt; 5 hombre 1.67 167 65.7 Calcula el porcentaje de población indígena de cada municipio y almacenalo en una nueva variable. Crea una nueva variable que muestre el cociente entre la población femenina y masculina. Hay muchas funciones que podemos usar para crear nuevas variables con mutate(), éstas deben cumplir ser funciones vectorizadas, es decir, reciben un vector de valores y devuelven un vector de la misma dimensión, por ejemplo multiplicar columnas o por un escalar. ¿Cuáles de las siguientes funciones son adecuadas para mutate()? Notar que hay escenarios en los que nos puede interesar usar funciones no vectorizadas con mutate() pero vale la pena entender que es lo que regresan. * mean, pmin, max, *, ^, quantile df_ej_2 &lt;- add_column(df_ej, peso_actual = c(60, 80, 70, 50, 65), peso_anterior = c(66, 78, 73, 54, 61)) mutate(df_ej_2, peso_medio = mean(c(peso_actual, peso_anterior))) mutate(df_ej_2, peso_menor = pmin(peso_actual, peso_anterior)) mutate(df_ej_2, peso_mayor = max(peso_actual, peso_anterior)) mutate(df_ej_2, estatura_sq = estatura ^ 2, bmi = peso_actual / estatura_sq) Summarise y resúmenes por grupo Summarise sirve para crear nuevas bases de datos con resúmenes o agregaciones de los datos originales. summarise(df_ej, promedio = mean(estatura)) #&gt; # A tibble: 1 x 1 #&gt; promedio #&gt; &lt;dbl&gt; #&gt; 1 1.68 Calcula la población total, indígena y afromexicana a total. summarise(df_mxmunicipio, indigeonous = sum(indigenous), afromexican = sum(afromexican)) #&gt; # A tibble: 1 x 2 #&gt; indigeonous afromexican #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 25694928 1381853 La mayor utlidad de summarise es cuando la combinamos con una variable de agrupación y esta combinación es la estrategia separa-aplica combina. Separa-aplica-combina (split-apply-combine) Muchos problemas de análisis de datos involucran la aplicación de la estrategia separa-aplica-combina (Wickham 2011), esta consiste en romper un problema en pedazos (de acuerdo a una variable de interés), operar sobre cada subconjunto de manera independiente (ej. calcular la media de cada grupo, ordenar observaciones por grupo, estandarizar por grupo) y después unir los pedazos nuevamente. El siguiente diagrama ejemplifiaca el paradigma de divide-aplica-combina: Separa la base de datos original. Aplica funciones a cada subconjunto. Combina los resultados en una nueva base de datos. Figure 3.2: Imagen de Software Carpentry con licencia CC-BY 4.0. Ahora, cuando pensamos como implementar la estrategia divide-aplica-combina es natural pensar en iteraciones, por ejemplo utilizar un ciclo for para recorrer cada grupo de interés y aplicar las funciones resumen, sin embargo la aplicación de ciclos for desemboca en código difícil de entender por lo que preferimos trabajar con funciones creadas para estas tareas, usaremos el paquete dplyr que además de ser más claro suele ser más veloz. Podemos hacer resúmenes por grupo, primero creamos una base de datos agrupada: by_sexo &lt;- group_by(df_ej, sexo) by_sexo #&gt; # A tibble: 5 x 2 #&gt; # Groups: sexo [2] #&gt; sexo estatura #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 mujer 1.65 #&gt; 2 hombre 1.8 #&gt; 3 mujer 1.7 #&gt; 4 mujer 1.6 #&gt; 5 hombre 1.67 y después operamos sobre cada grupo, creando un resumen a nivel grupo y uniendo los subconjuntos en una base nueva: Calcula la población total por estado. Calcula la población indígena y afromexicana por estado. ¿Qué otros resúmenes puedes hacer para explorar los datos? Algunas funciones útiles con summarise son min(x), median(x), max(x), quantile(x, p), n(), sum(x), sum(x &gt; 1), mean(x &gt; 1), sd(x). Por ejemplo, para cada área metropolitana: cuántos municipios engloba (n()), la población total (sum()) y al estado al que pertenece (first()). by_metro_area &lt;- group_by(df_mxmunicipio, metro_area) no_miss &lt;- filter(by_metro_area, !is.na(metro_area)) pop_metro_area &lt;- summarise(no_miss, state = first(state_abbr), n_municipios = n(), pop_total = sum(pop)) head(pop_metro_area) #&gt; # A tibble: 6 x 4 #&gt; metro_area state n_municipios pop_total #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 Acapulco GRO 2 886975 #&gt; 2 Acayucan VER 3 120340 #&gt; 3 Aguascalientes AGS 3 1044049 #&gt; 4 Cancún QROO 2 763121 #&gt; 5 Celaya GTO 3 635706 #&gt; 6 Chihuahua CHIH 3 918339 Operador pipeline En R, cuando uno hace varias operaciones es difícil leer y entender el código: library(estcomp) summarise(group_by(filter(election_2012, !is.na(section_type)), region, section_type), n = n(), pri_pvem = sum(pri_pvem), prd_pt_mc = sum(prd_pt_mc), pan = sum(pan)) #&gt; # A tibble: 24 x 6 #&gt; # Groups: region [8] #&gt; region section_type n pri_pvem prd_pt_mc pan #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 centronorte M 2071 331221 143225 228112 #&gt; 2 centronorte R 5049 651507 211524 447886 #&gt; 3 centronorte U 8940 1229241 653540 1171415 #&gt; 4 centrosur M 1839 324327 277470 126264 #&gt; 5 centrosur R 2541 495288 223978 181755 #&gt; 6 centrosur U 27515 3698793 4765575 1936586 #&gt; 7 este M 3158 462510 370352 306124 #&gt; 8 este R 6768 905078 521793 654839 #&gt; 9 este U 11403 1373876 1602217 1179497 #&gt; 10 noreste M 1259 176191 77062 169285 #&gt; # … with 14 more rows La dificultad radica en que usualmente los parámetros se asignan después del nombre de la función usando (). Una alternativa es ir almacenando las salidas en tablas de datos intermedias pero esto resulta poco práctico porque: 1) almacenamos en el mismo objeto sobreescribiendo ó 2) terminamos con muchos objetos con nombres poco significativos. El operador Forward Pipe (%&gt;%) cambia el orden en que se asignan los parámetros, de manera que un parámetro que precede a la función es enviado (“piped”) a la función: x %&gt;% f(y) se vuelve f(x, y), x %&gt;% f(y) %&gt;% g(z) se vuelve g(f(x, y), z). Es así que podemos reescribir el código para poder leer las operaciones que vamos aplicando de izquierda a derecha y de arriba hacia abajo. Veamos como cambia el código del ejemplo: election_2012 %&gt;% filter(!is.na(section_type)) %&gt;% group_by(region, section_type) %&gt;% summarise( n = n(), pri_pvem = sum(pri_pvem), prd_pt_mc = sum(prd_pt_mc), pan = sum(pan) ) #&gt; # A tibble: 24 x 6 #&gt; # Groups: region [8] #&gt; region section_type n pri_pvem prd_pt_mc pan #&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 centronorte M 2071 331221 143225 228112 #&gt; 2 centronorte R 5049 651507 211524 447886 #&gt; 3 centronorte U 8940 1229241 653540 1171415 #&gt; 4 centrosur M 1839 324327 277470 126264 #&gt; 5 centrosur R 2541 495288 223978 181755 #&gt; 6 centrosur U 27515 3698793 4765575 1936586 #&gt; 7 este M 3158 462510 370352 306124 #&gt; 8 este R 6768 905078 521793 654839 #&gt; 9 este U 11403 1373876 1602217 1179497 #&gt; 10 noreste M 1259 176191 77062 169285 #&gt; # … with 14 more rows podemos leer %&gt;% como “después”. Tip: Un atajo para producir el operador pipeline %&gt;% es shift + ctrl/cmd + M Siguiendo con los datos election_2012, ¿Qué estados tienen la mayor participación (esto es del total de votantes en la lista nominal que porcentaje asistió a votar)? Tip: debes eliminar las casillas especiales pues la lista nominal (ln) no está definida. Variables por grupo En ocasiones es conveniente crear variables por grupo, por ejemplo estandarizar dentro de cada grupo z = (x - mean(x)) / sd(x). Para esto usamos group_by() y mutate(). Veamos un ejemplo: z_prd_pt_mc_state &lt;- election_2012 %&gt;% filter(total &gt; 50, !is.na(section_type)) %&gt;% mutate(prd_pt_mc_pct = prd_pt_mc / total) %&gt;% group_by(state_abbr) %&gt;% mutate( n = n(), sd_prd_pt_mc = sd(prd_pt_mc_pct), mean_prd_pt_mc = mean(prd_pt_mc_pct), z_prd_pt_mc = (prd_pt_mc_pct - mean_prd_pt_mc) / sd_prd_pt_mc ) Verbos de dos tablas Muchas veces debemos reunir información que está almacenada a lo largo de muchas tablas, por ejemplo, si nos interesa conocer como se relaciona el año de escolaridad promedio (schoolyrs en el df_edu) con el porcentaje de población indígena (indigenous en df_mxmunicipios), debemos poder pegar las dos tablas. Hay varias maneras de unir dos bases de datos y debemos pensar en el obejtivo: x &lt;- tibble(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Stuart&quot;, &quot;Pete&quot;), instrument = c(&quot;guitar&quot;, &quot;bass&quot;, &quot;guitar&quot;, &quot;drums&quot;, &quot;bass&quot;, &quot;drums&quot;)) y &lt;- tibble(name = c(&quot;John&quot;, &quot;Paul&quot;, &quot;George&quot;, &quot;Ringo&quot;, &quot;Brian&quot;), band = c(&quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;TRUE&quot;, &quot;FALSE&quot;)) x #&gt; # A tibble: 6 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar #&gt; 2 Paul bass #&gt; 3 George guitar #&gt; 4 Ringo drums #&gt; 5 Stuart bass #&gt; 6 Pete drums y #&gt; # A tibble: 5 x 2 #&gt; name band #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John TRUE #&gt; 2 Paul TRUE #&gt; 3 George TRUE #&gt; 4 Ringo TRUE #&gt; 5 Brian FALSE inner_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 4 x 3 #&gt; name instrument band #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar TRUE #&gt; 2 Paul bass TRUE #&gt; 3 George guitar TRUE #&gt; 4 Ringo drums TRUE left_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 6 x 3 #&gt; name instrument band #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar TRUE #&gt; 2 Paul bass TRUE #&gt; 3 George guitar TRUE #&gt; 4 Ringo drums TRUE #&gt; 5 Stuart bass &lt;NA&gt; #&gt; 6 Pete drums &lt;NA&gt; semi_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 4 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 John guitar #&gt; 2 Paul bass #&gt; 3 George guitar #&gt; 4 Ringo drums anti_join(x, y) #&gt; Joining, by = &quot;name&quot; #&gt; # A tibble: 2 x 2 #&gt; name instrument #&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Stuart bass #&gt; 2 Pete drums Resumamos lo que observamos arriba: Tipo Acción inner Incluye únicamente las filas que aparecen tanto en x como en y left Incluye todas las filas en x y las filas de y que coincidan semi Incluye las filas de x que coincidan con y anti Incluye las filas de x que no coinciden con y Ahora tu turno, ¿cómo se relacionan los años de escolaridad con el porcentaje de población indígena. Utiliza los datos df_mxmunicipio y df_edu para explorar la relación. ¿cuál es el join adecuado? ¿de qué tamaño serán los datos finales? glimpse(df_edu) #&gt; Observations: 7,371 #&gt; Variables: 16 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, … #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;002&quot;, &quot;002&quot;, &quot;002&quot;, &quot;003&quot;, &quot;003&quot;… #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01002&quot;, &quot;01002&quot;, &quot;01002&quot;, … #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ sex &lt;chr&gt; &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mujeres&quot;, &quot;Total&quot;, &quot;Hombres&quot;, &quot;Mu… #&gt; $ pop_15 &lt;dbl&gt; 631064, 301714, 329350, 31013, 14991, 16022, 38678, 18… #&gt; $ no_school &lt;dbl&gt; 2.662329, 2.355211, 2.943677, 4.011221, 4.389300, 3.65… #&gt; $ preschool &lt;dbl&gt; 0.17335801, 0.17466873, 0.17215728, 0.25795634, 0.2935… #&gt; $ elementary &lt;dbl&gt; 20.15247, 18.60073, 21.57401, 33.77938, 35.48129, 32.1… #&gt; $ secondary &lt;dbl&gt; 29.31145, 30.37976, 28.33278, 39.21259, 37.45581, 40.8… #&gt; $ highschool &lt;dbl&gt; 23.31824, 22.84912, 23.74799, 16.07068, 15.67607, 16.4… #&gt; $ higher_edu &lt;dbl&gt; 24.291989, 25.560299, 23.130105, 6.355399, 6.357148, 6… #&gt; $ other &lt;dbl&gt; 0.09016518, 0.08020841, 0.09928647, 0.31277206, 0.3468… #&gt; $ schoolyrs &lt;dbl&gt; 10.211152, 10.380144, 10.056383, 7.854005, 7.692086, 8… glimpse(df_mxmunicipio) #&gt; Observations: 2,457 #&gt; Variables: 18 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;… #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;002&quot;, &quot;003&quot;, &quot;004&quot;, &quot;005&quot;, &quot;006&quot;, &quot;007&quot;, … #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01002&quot;, &quot;01003&quot;, &quot;01004&quot;, &quot;01005&quot;, &quot;010… #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_name_official &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalient… #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, … #&gt; $ state_abbr_official &lt;chr&gt; &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;Ags.&quot;, &quot;… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Asientos&quot;, &quot;Calvillo&quot;, &quot;Cosío&quot;… #&gt; $ pop &lt;int&gt; 877190, 46464, 56048, 15577, 120405, 46473, 53866… #&gt; $ pop_male &lt;int&gt; 425731, 22745, 27298, 7552, 60135, 22490, 26693, … #&gt; $ pop_female &lt;int&gt; 451459, 23719, 28750, 8025, 60270, 23983, 27173, … #&gt; $ afromexican &lt;dbl&gt; 532, 3, 10, 0, 32, 3, 13, 13, 4, 0, 43, 1139, 351… #&gt; $ part_afromexican &lt;dbl&gt; 2791, 130, 167, 67, 219, 74, 578, 37, 59, 60, 377… #&gt; $ indigenous &lt;dbl&gt; 104125, 1691, 7358, 2213, 8679, 6232, 6714, 1733,… #&gt; $ part_indigenous &lt;dbl&gt; 14209, 92, 2223, 191, 649, 251, 247, 84, 76, 244,… #&gt; $ metro_area &lt;chr&gt; &quot;Aguascalientes&quot;, NA, NA, NA, &quot;Aguascalientes&quot;, N… #&gt; $ long &lt;dbl&gt; -102.29605, -102.08928, -102.71875, -102.30004, -… #&gt; $ lat &lt;dbl&gt; 21.87982, 22.23832, 21.84691, 22.36641, 21.96127,… Si queremos un mapa del ganador de las elecciones por estado debemos unir los datos de elecciones con datos geográficos, estos estan incluídos en mxmaps, son mxstate.map. election_2012_state &lt;- election_2012 %&gt;% group_by(state_code) %&gt;% summarise( pri_pvem = 100 * sum(pri_pvem) / sum(total), pan = 100 * sum(pan) / sum(total), prd_pt_mc = 100 * sum(prd_pt_mc) / sum(total) ) %&gt;% mutate(winner = case_when( pri_pvem &gt; pan &amp; pri_pvem &gt; prd_pt_mc ~ &quot;pri_pvem&quot;, pan &gt; pri_pvem &amp; pan &gt; prd_pt_mc ~ &quot;pan&quot;, TRUE ~ &quot;prd_pt_mc&quot;), winner_pct = pmax(pri_pvem, pan, prd_pt_mc)) election_map &lt;- mxstate.map %&gt;% left_join(election_2012_state, by = c(&quot;region&quot; = &quot;state_code&quot;)) ggplot(election_map, aes(long, lat, group = group)) + geom_polygon(aes(fill = winner)) + coord_map() Podemos especificar el color de cada categoría y la intensidad puede variar de acuerdo al porcentaje de votos que se llevó el partido/alianza ganador. library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine map_edo &lt;- ggplot(election_map, aes(long, lat, group = group)) + geom_polygon(aes(fill = winner, alpha = winner_pct), color = &quot;#666666&quot;, size = .05, show.legend = FALSE) + coord_map() + scale_fill_manual(values = c(&quot;prd_pt_mc&quot; = &quot;#FFCC00&quot;, &quot;pan&quot; = &quot;#3399FF&quot;, &quot;pri_pvem&quot; = &quot;#00CD66&quot;)) + theme_void() election_hexbinmap &lt;- mxhexbin.map %&gt;% left_join(election_2012_state, by = c(&quot;region&quot; = &quot;state_code&quot;)) state_labels_map &lt;- mxhexbin.map %&gt;% group_by(state_abbr) %&gt;% summarise(long = mean(long), lat = mean(lat), group = first(group)) hexbinmap_edo &lt;- ggplot(election_hexbinmap, aes(long, lat, group = group)) + geom_polygon(aes(fill = winner, alpha = winner_pct), color = &quot;#666666&quot;, size = .05, show.legend = FALSE) + coord_map() + scale_fill_manual(values = c(&quot;prd_pt_mc&quot; = &quot;#FFCC00&quot;, &quot;pan&quot; = &quot;#3399FF&quot;, &quot;pri_pvem&quot; = &quot;#00CD66&quot;)) + geom_text(data = state_labels_map, aes(long, lat, label = state_abbr)) + theme_void() grid.arrange(map_edo, hexbinmap_edo, nrow = 1) Genera un mapa a nivel municipo que muestre el porcentaje de la población casada a total (mayores de 12 años). Referencias "],
["datos-limpios.html", "3.2 Datos limpios", " 3.2 Datos limpios Una vez que importamos datos a R es conveniente limpiarlos, esto implica almacenarlos de una manera consisistente que nos permita enfocarnos en responder preguntas de los datos en lugar de estar luchando con los datos. Datos limpios son datos que facilitan las tareas del análisis de datos: Visualización: Resúmenes de datos usando gráficas, análisis exploratorio, o presentación de resultados. Manipulación: Manipulación de variables como agregar, filtrar, reordenar, transformar. Modelación: Ajustar modelos es sencillo si los datos están en la forma correcta. Los principios de los datos limpios (Wickham 2014) proveen una manera estándar de organizar la información: Cada columna es una variable. Cada renglón es una observación . Cada celda es un único valor. Vale la pena notar que los principios de los datos limpios se pueden ver como teoría de algebra relacional para estadísticos, estós principios junto con cada tipo de unidad observacional forma una tabla equivalen a la tercera forma normal de Codd con enfoque en una sola tabla de datos en lugar de muchas conectadas en bases de datos relacionales. Veamos un ejemplo: La mayor parte de las bases de datos en estadística tienen forma rectangular, ¿cuántas variables tiene la siguiente tabla? tratamientoA tratamientoB Juan Aguirre - 2 Ana Bernal 16 11 José López 3 1 La tabla anterior también se puede estructurar de la siguiente manera: Juan Aguirre Ana Bernal José López tratamientoA - 16 3 tratamientoB 2 11 1 Si vemos los principios (cada variable forma una columna, cada observación forma un renglón, cada tipo de unidad observacional forma una tabla), ¿las tablas anteriores cumplen los principios? Para responder la pregunta identifiquemos primero cuáles son las variables y cuáles las observaciones de esta pequeña base. Las variables son: persona/nombre, tratamiento y resultado. Entonces, siguiendo los principios de datos limpios obtenemos la siguiente estructura: nombre tratamiento resultado Juan Aguirre a - Ana Bernal a 16 José López a 3 Juan Aguirre b 2 Ana Bernal b 11 José López b 1 Limpieza bases de datos Los principios de los datos limpios parecen obvios pero la mayor parte de los datos no los cumplen debido a: La mayor parte de la gente no está familiarizada con los principios y es difícil derivarlos por uno mismo. Los datos suelen estar organizados para facilitar otros aspectos que no son análisis, por ejemplo, la captura. Algunos de los problemas más comunes en las bases de datos que no están limpias son: Los encabezados de las columnas son valores y no nombres de variables. Más de una variable por columna. Las variables están organizadas tanto en filas como en columnas. Más de un tipo de observación en una tabla. Una misma unidad observacional está almacenada en múltiples tablas. La mayor parte de estos problemas se pueden arreglar con pocas herramientas, a continuación veremos como limpiar datos usando 2 funciones del paquete tidyr: pivot_longer(): recibe múltiples columnas y las convierte en pares de valores y nombres de tal manera que alarga los datos. pivot_wider(): el opuesto a pivot_longer() recibe columnas que separa haciendo los datos más anchos. Repasaremos los problemas más comunes que se encuentran en conjuntos de datos sucios y mostraremos como se puede manipular la tabla de datos (usando las funciones de pivoteo) con el fin de estructurarla para que cumpla los principios de datos limpios. Nota: Quizá has visto código de tidyr usando las funciones gather() y spread(), estas son versiones anteriores a las funciones de pivoteo, sin embargo, se les seguirá dando mantenimiento puesto que son muy populares, aquí puedes encontrar una versión de las notas usando que utilizan gather() y spread(). Los encabezados de las columanas son valores Usaremos ejemplos para entender los conceptos más facilmente. Comenzaremos con una tabla de datos que contiene las mediciones de partículas suspendidas PM2.5 de la red automática de monitoreo atmosférico (RAMA) para los primeros meses del 2019. library(tidyverse) library(estcomp) pm25_2019 #&gt; # A tibble: 5,088 x 26 #&gt; date hour AJM AJU BJU CAM CCA COY FAR GAM HGM INN #&gt; &lt;date&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;lgl&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 2019-01-01 1 19 35 62 90 66 NA NA NA 56 22 #&gt; 2 2019-01-01 2 17 24 88 104 84 NA NA NA 61 14 #&gt; 3 2019-01-01 3 14 20 107 140 95 NA NA NA 74 8 #&gt; 4 2019-01-01 4 6 15 101 162 97 NA NA NA 90 9 #&gt; 5 2019-01-01 5 4 8 121 133 88 NA NA NA 90 3 #&gt; 6 2019-01-01 6 7 7 93 106 77 NA NA NA 106 3 #&gt; 7 2019-01-01 7 12 8 84 98 51 NA NA NA 115 NA #&gt; 8 2019-01-01 8 15 7 101 82 39 NA NA NA 90 NA #&gt; 9 2019-01-01 9 24 3 89 54 26 NA NA NA 90 NA #&gt; 10 2019-01-01 10 24 NA 88 76 26 NA NA NA 99 2 #&gt; # … with 5,078 more rows, and 14 more variables: MER &lt;dbl&gt;, MGH &lt;dbl&gt;, #&gt; # MON &lt;dbl&gt;, MPA &lt;lgl&gt;, NEZ &lt;dbl&gt;, PED &lt;dbl&gt;, SAC &lt;lgl&gt;, SAG &lt;dbl&gt;, #&gt; # SFE &lt;dbl&gt;, SJA &lt;lgl&gt;, TLA &lt;dbl&gt;, UAX &lt;dbl&gt;, UIZ &lt;dbl&gt;, XAL &lt;dbl&gt; ¿Cuáles son las variables en estos datos? Esta base de datos tiene 4 variables: fecha, hora, estación y medición (en microgramos por metro cúbico \\(\\mu g/m^3\\)). Al alargar los datos desaparecerán las columnas que se agrupan y darán lugar a dos nuevas columnas: la correspondiente a estación y la correspondiente a medición. Entonces, usamos la función pivot_longer() que recibe los argumentos: data: data.frame que vamos a pivotear, alargar. cols: columnas que vamos a pivotear (apilar), la notación para seleccionarlas es tidyselect, la misma que usamos con select() en dplyr. names_to: nombre (string: en comillas “”) de la nueva columna que almacenará los nombres de las columnas en los datos. values_to: nombre (string: en comillas “”) de la nueva columna que almacenará los valores en los datos. pm25_2019_tidy &lt;- pivot_longer(pm25_2019, cols = AJM:XAL, names_to = &quot;station&quot;, values_to = &quot;measurement&quot;) pm25_2019_tidy #&gt; # A tibble: 122,112 x 4 #&gt; date hour station measurement #&gt; &lt;date&gt; &lt;dbl&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 2019-01-01 1 AJM 19 #&gt; 2 2019-01-01 1 AJU 35 #&gt; 3 2019-01-01 1 BJU 62 #&gt; 4 2019-01-01 1 CAM 90 #&gt; 5 2019-01-01 1 CCA 66 #&gt; 6 2019-01-01 1 COY NA #&gt; 7 2019-01-01 1 FAR NA #&gt; 8 2019-01-01 1 GAM NA #&gt; 9 2019-01-01 1 HGM 56 #&gt; 10 2019-01-01 1 INN 22 #&gt; # … with 122,102 more rows Observemos que en la tabla original teníamos bajo la columna AJM, en el renglón correspondiente a 2019-01-01 hora 1 un valor de 19, y podemos ver que este valor en la tabla larga se almacena bajo la columna measurement y corresponde a la estación AJM. La nueva estructura de la base de datos nos permite, por ejemplo, hacer fácilmente una gráfica donde podemos comparar las diferencias en las frecuencias. pm25_2019_tidy %&gt;% mutate( missing = is.na(measurement), station = reorder(station, missing, sum) ) %&gt;% ggplot(aes(x = date, y = hour, fill = is.na(measurement))) + geom_raster(alpha = 0.8) + facet_wrap(~ station) + scale_fill_manual(&quot;faltante&quot;, values = c(&quot;TRUE&quot; = &quot;salmon&quot;, &quot;FALSE&quot; = &quot;gray&quot;)) Otro ejemplo, veamos los datos df_edu, ¿cuántas variables tenemos? df_edu #&gt; # A tibble: 7,371 x 16 #&gt; state_code municipio_code region state_name state_abbr municipio_name sex #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 01 001 01001 Aguascali… AGS Aguascalientes Total #&gt; 2 01 001 01001 Aguascali… AGS Aguascalientes Homb… #&gt; 3 01 001 01001 Aguascali… AGS Aguascalientes Muje… #&gt; 4 01 002 01002 Aguascali… AGS Asientos Total #&gt; 5 01 002 01002 Aguascali… AGS Asientos Homb… #&gt; 6 01 002 01002 Aguascali… AGS Asientos Muje… #&gt; 7 01 003 01003 Aguascali… AGS Calvillo Total #&gt; 8 01 003 01003 Aguascali… AGS Calvillo Homb… #&gt; 9 01 003 01003 Aguascali… AGS Calvillo Muje… #&gt; 10 01 004 01004 Aguascali… AGS Cosío Total #&gt; # … with 7,361 more rows, and 9 more variables: pop_15 &lt;dbl&gt;, no_school &lt;dbl&gt;, #&gt; # preschool &lt;dbl&gt;, elementary &lt;dbl&gt;, secondary &lt;dbl&gt;, highschool &lt;dbl&gt;, #&gt; # higher_edu &lt;dbl&gt;, other &lt;dbl&gt;, schoolyrs &lt;dbl&gt; Notemos que el nivel de escolaridad esta guardado en 6 columnas (preschool, elementary, …, other), este tipo de almacenamiento no es limpio aunque puede ser útil al momento de ingresar la información o para presentarla. Para tener datos limpios apilamos los niveles de escolaridad de manera que sea una sola columna (nuevamente alargamos los datos): df_edu_tidy &lt;- pivot_longer(data = df_edu, cols = preschool:other, names_to = &quot;grade&quot;, values_to = &quot;percent&quot;, values_drop_na = TRUE) glimpse(df_edu_tidy) #&gt; Observations: 44,226 #&gt; Variables: 12 #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, … #&gt; $ municipio_code &lt;chr&gt; &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;, &quot;001&quot;… #&gt; $ region &lt;chr&gt; &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, &quot;01001&quot;, … #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ state_abbr &lt;chr&gt; &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;, &quot;AGS&quot;… #&gt; $ municipio_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ sex &lt;chr&gt; &quot;Total&quot;, &quot;Total&quot;, &quot;Total&quot;, &quot;Total&quot;, &quot;Total&quot;, &quot;Total&quot;, … #&gt; $ pop_15 &lt;dbl&gt; 631064, 631064, 631064, 631064, 631064, 631064, 301714… #&gt; $ no_school &lt;dbl&gt; 2.662329, 2.662329, 2.662329, 2.662329, 2.662329, 2.66… #&gt; $ schoolyrs &lt;dbl&gt; 10.211152, 10.211152, 10.211152, 10.211152, 10.211152,… #&gt; $ grade &lt;chr&gt; &quot;preschool&quot;, &quot;elementary&quot;, &quot;secondary&quot;, &quot;highschool&quot;, … #&gt; $ percent &lt;dbl&gt; 0.17335801, 20.15247265, 29.31144860, 23.31823714, 24.… El parámetro values_drop_na = TRUE se utiliza para eliminar los renglones con valores faltantes en la columna de porcentaje, esto es, eliminamos aquellas observaciones que tenían NA en la columnas de nivel de escolaridad de la tabla ancha. En este caso optamos por que los faltantes sean implícitos, la conveniencia de tenerlos implícitos/explícitos dependerá de la aplicación. Con los datos limpios es facil hacer manipulaciones y grfiacs, ¿cómo habrían hecho la siguiente gráfica antes de la limpieza? df_edu_cdmx &lt;- df_edu_tidy %&gt;% filter(state_abbr == &quot;CDMX&quot;, sex != &quot;Total&quot;, grade != &quot;other&quot;) %&gt;% mutate(municipio_name = reorder(municipio_name, percent, last)) ggplot(df_edu_cdmx, aes(x = grade, y = percent, group = sex, color = sex)) + geom_path() + facet_wrap(~municipio_name) + theme(axis.text.x = element_text(angle = 60, hjust = 1)) + scale_x_discrete(limits = c(&quot;preschool&quot;, &quot;elementary&quot;, &quot;secondary&quot;, &quot;highschool&quot;, &quot;higher_edu&quot;)) Una columna asociada a más de una variable Utilizaremos un subconjunto de los datos de la prueba ENLACE a nivel primaria, la prueba ENLACE evaluaba a todos los alumnos de tercero a sexto de primaria y a los alumnos de secundaria del país en 3 áreas: español, matemáticas y formación cívica y ética. data(&quot;enlacep_2013&quot;) enlacep_sub_2013 &lt;- enlacep_2013 %&gt;% select(CVE_ENT:PUNT_FCE_6) %&gt;% sample_n(1000) glimpse(enlacep_sub_2013) #&gt; Observations: 1,000 #&gt; Variables: 22 #&gt; $ CVE_ENT &lt;chr&gt; &quot;07&quot;, &quot;15&quot;, &quot;30&quot;, &quot;32&quot;, &quot;07&quot;, &quot;28&quot;, &quot;07&quot;, &quot;30&quot;, &quot;09&quot;, &quot;13&quot;… #&gt; $ NOM_ENT &lt;chr&gt; &quot;CHIAPAS&quot;, &quot;MEXICO&quot;, &quot;VERACRUZ&quot;, &quot;ZACATECAS&quot;, &quot;CHIAPAS&quot;, &quot;… #&gt; $ CCT &lt;chr&gt; &quot;07DPB0173G&quot;, &quot;15EPR4155A&quot;, &quot;30DPR2979I&quot;, &quot;32DPR1118U&quot;, &quot;0… #&gt; $ TURNO &lt;chr&gt; &quot;MATUTINO&quot;, &quot;VESPERTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTIN… #&gt; $ ESCUELA &lt;chr&gt; &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;MIGUEL HIDALGO&quot;, &quot;BENITO JUAR… #&gt; $ TIPO &lt;chr&gt; &quot;INDêGENA&quot;, &quot;GENERAL&quot;, &quot;GENERAL&quot;, &quot;GENERAL&quot;, &quot;CONAFE&quot;, &quot;GE… #&gt; $ CVE_MUN &lt;chr&gt; &quot;083&quot;, &quot;109&quot;, &quot;008&quot;, &quot;009&quot;, &quot;111&quot;, &quot;032&quot;, &quot;031&quot;, &quot;151&quot;, &quot;0… #&gt; $ NOM_MUN &lt;chr&gt; &quot;SOCOLTENANGO&quot;, &quot;TULTITLAN&quot;, &quot;ALPATLAHUAC&quot;, &quot;CHALCHIHUITES… #&gt; $ CVE_LOC &lt;chr&gt; &quot;0051&quot;, &quot;0001&quot;, &quot;0015&quot;, &quot;0012&quot;, &quot;0041&quot;, &quot;0001&quot;, &quot;0452&quot;, &quot;0… #&gt; $ NOM_LOC &lt;chr&gt; &quot;ESTRELLA ROJA&quot;, &quot;TULTITLAN DE MARIANO ESCOBEDO&quot;, &quot;TEANQUI… #&gt; $ PUNT_ESP_3 &lt;dbl&gt; 617, 533, 424, 419, 395, 525, 419, 432, 536, 553, 603, 382… #&gt; $ PUNT_MAT_3 &lt;dbl&gt; 731, 555, 416, 459, 385, 572, 408, 428, 569, 619, 646, 353… #&gt; $ PUNT_FCE_3 &lt;dbl&gt; 618, 474, 404, 368, 399, 470, 380, 392, 479, 496, 560, 342… #&gt; $ PUNT_ESP_4 &lt;dbl&gt; 595, 552, 403, 426, 354, 478, 372, 393, 499, 557, 545, 403… #&gt; $ PUNT_MAT_4 &lt;dbl&gt; 730, 619, 417, 418, 395, 502, 436, 421, 523, 624, 633, 431… #&gt; $ PUNT_FCE_4 &lt;dbl&gt; 616, 488, 405, 440, 373, 418, 378, 385, 443, 497, 509, 406… #&gt; $ PUNT_ESP_5 &lt;dbl&gt; 431, 519, 560, 412, 357, 399, 379, 429, 504, 591, 489, 438… #&gt; $ PUNT_MAT_5 &lt;dbl&gt; 435, 539, 549, 479, 336, 397, 341, 346, 515, 623, 535, 407… #&gt; $ PUNT_FCE_5 &lt;dbl&gt; 440, 489, 478, 342, 355, 386, 329, 364, 455, 530, 473, 399… #&gt; $ PUNT_ESP_6 &lt;dbl&gt; 414, 552, 520, 431, 348, 443, 364, 386, 469, 567, 484, 432… #&gt; $ PUNT_MAT_6 &lt;dbl&gt; 433, 586, 492, 400, 351, 486, 382, 363, 501, 655, 530, 381… #&gt; $ PUNT_FCE_6 &lt;dbl&gt; 440, 507, 485, 393, 359, 396, 332, 368, 414, 540, 473, 360… ¿Cuántas variables tiene este subconjunto de los datos? De manera similar a los ejemplos anteriores, utiliza la función pivot_longer para apilar las columnas correspondientes a área-grado. Piensa en como podemos separar la “variable” área-grado en dos columnas. Ahora separaremos las variables área y grado de la columna AREA_GRADO, para ello debemos pasar a la función separate(), esta recibe como parámetros: el nombre de la base de datos, el nombre de la variable que deseamos separar en más de una, la posición de donde deseamos “cortar” (hay más opciones para especificar como separar, ver ?separate). El default es separar valores en todos los lugares que encuentre un caracter que no es alfanumérico (espacio, guión,…). enlacep_tidy &lt;- separate(data = enlacep_long, col = AREA_GRADO, into = c(&quot;AREA&quot;, &quot;GRADO&quot;), sep = 9) enlacep_tidy #&gt; # A tibble: 12,000 x 13 #&gt; CVE_ENT NOM_ENT CCT TURNO ESCUELA TIPO CVE_MUN NOM_MUN CVE_LOC NOM_LOC #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 2 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 3 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 4 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 5 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 6 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 7 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 8 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 9 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; 10 07 CHIAPAS 07DP… MATU… LIC. A… INDê… 083 SOCOLT… 0051 ESTREL… #&gt; # … with 11,990 more rows, and 3 more variables: AREA &lt;chr&gt;, GRADO &lt;chr&gt;, #&gt; # PUNTAJE &lt;dbl&gt; # creamos un mejor código de área enlacep_tidy &lt;- enlacep_tidy %&gt;% mutate( AREA = substr(AREA, 6, 8), GRADO = as.numeric(GRADO) ) glimpse(enlacep_tidy) #&gt; Observations: 12,000 #&gt; Variables: 13 #&gt; $ CVE_ENT &lt;chr&gt; &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;… #&gt; $ NOM_ENT &lt;chr&gt; &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAP… #&gt; $ CCT &lt;chr&gt; &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DP… #&gt; $ TURNO &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;… #&gt; $ ESCUELA &lt;chr&gt; &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC.… #&gt; $ TIPO &lt;chr&gt; &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;… #&gt; $ CVE_MUN &lt;chr&gt; &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;… #&gt; $ NOM_MUN &lt;chr&gt; &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO… #&gt; $ CVE_LOC &lt;chr&gt; &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051… #&gt; $ NOM_LOC &lt;chr&gt; &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA … #&gt; $ AREA &lt;chr&gt; &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;, &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;, &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;… #&gt; $ GRADO &lt;dbl&gt; 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 3, 3, 3, 4, 4, 4, 5, 5, 5… #&gt; $ PUNTAJE &lt;dbl&gt; 617, 731, 618, 595, 730, 616, 431, 435, 440, 414, 433, 440, 5… Conforme nos habituemos a las funciones podemos sacar provecho de sus argumentos adicionales: names_prefix: recibe una expresión regular para eliminar el texto que coincida del inicio de una variable. pivot_longer(enlacep_sub_2013, cols = contains(&quot;PUNT&quot;), names_to = c(&quot;AREA_GRADO&quot;), values_to = &quot;PUNTAJE&quot;, names_prefix = &quot;PUNT_&quot;) %&gt;% glimpse() #&gt; Observations: 12,000 #&gt; Variables: 12 #&gt; $ CVE_ENT &lt;chr&gt; &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;… #&gt; $ NOM_ENT &lt;chr&gt; &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CH… #&gt; $ CCT &lt;chr&gt; &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;0… #&gt; $ TURNO &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;… #&gt; $ ESCUELA &lt;chr&gt; &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;L… #&gt; $ TIPO &lt;chr&gt; &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;… #&gt; $ CVE_MUN &lt;chr&gt; &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;0… #&gt; $ NOM_MUN &lt;chr&gt; &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENA… #&gt; $ CVE_LOC &lt;chr&gt; &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0… #&gt; $ NOM_LOC &lt;chr&gt; &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTREL… #&gt; $ AREA_GRADO &lt;chr&gt; &quot;ESP_3&quot;, &quot;MAT_3&quot;, &quot;FCE_3&quot;, &quot;ESP_4&quot;, &quot;MAT_4&quot;, &quot;FCE_4&quot;, &quot;ESP… #&gt; $ PUNTAJE &lt;dbl&gt; 617, 731, 618, 595, 730, 616, 431, 435, 440, 414, 433, 440… names_sep: nos permite hacer el pivoteo y separar en una misma operación, en este caso names_to consiste en un vector con más de una entrada y names_sep indica como separar el nombre de las columnas. pivot_longer(enlacep_sub_2013, cols = contains(&quot;PUNT&quot;), names_to = c(&quot;AREA&quot;, &quot;GRADO&quot;), values_to = &quot;PUNTAJE&quot;, names_prefix = &quot;PUNT_&quot;, names_sep = &quot;_&quot;) %&gt;% glimpse() #&gt; Observations: 12,000 #&gt; Variables: 13 #&gt; $ CVE_ENT &lt;chr&gt; &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;… #&gt; $ NOM_ENT &lt;chr&gt; &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAP… #&gt; $ CCT &lt;chr&gt; &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DP… #&gt; $ TURNO &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;… #&gt; $ ESCUELA &lt;chr&gt; &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC.… #&gt; $ TIPO &lt;chr&gt; &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;… #&gt; $ CVE_MUN &lt;chr&gt; &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;… #&gt; $ NOM_MUN &lt;chr&gt; &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO… #&gt; $ CVE_LOC &lt;chr&gt; &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051… #&gt; $ NOM_LOC &lt;chr&gt; &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA … #&gt; $ AREA &lt;chr&gt; &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;, &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;, &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;… #&gt; $ GRADO &lt;chr&gt; &quot;3&quot;, &quot;3&quot;, &quot;3&quot;, &quot;4&quot;, &quot;4&quot;, &quot;4&quot;, &quot;5&quot;, &quot;5&quot;, &quot;5&quot;, &quot;6&quot;, &quot;6&quot;, &quot;6&quot;, &quot;… #&gt; $ PUNTAJE &lt;dbl&gt; 617, 731, 618, 595, 730, 616, 431, 435, 440, 414, 433, 440, 5… names_pattern: similar a names_sep pero recibe una expresión regular. pivot_longer(enlacep_sub_2013, cols = contains(&quot;PUNT&quot;), names_to = c(&quot;AREA&quot;, &quot;GRADO&quot;), names_pattern = &quot;PUNT_?(.*)_(.*)&quot;, values_to = &quot;PUNTAJE&quot;) names_ptypes, values_ptypes: permiten especificar el tipo de las nuevas columnas. pivot_longer(enlacep_sub_2013, cols = contains(&quot;PUNT&quot;), names_to = c(&quot;AREA&quot;, &quot;GRADO&quot;), values_to = &quot;PUNTAJE&quot;, names_prefix = &quot;PUNT_&quot;, names_sep = &quot;_&quot;, names_ptypes = list(GRADO = integer())) %&gt;% glimpse() #&gt; Observations: 12,000 #&gt; Variables: 13 #&gt; $ CVE_ENT &lt;chr&gt; &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;07&quot;, &quot;… #&gt; $ NOM_ENT &lt;chr&gt; &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAPAS&quot;, &quot;CHIAP… #&gt; $ CCT &lt;chr&gt; &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DPB0173G&quot;, &quot;07DP… #&gt; $ TURNO &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;… #&gt; $ ESCUELA &lt;chr&gt; &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC. ADOLFO LOPEZ MATEOS&quot;, &quot;LIC.… #&gt; $ TIPO &lt;chr&gt; &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;… #&gt; $ CVE_MUN &lt;chr&gt; &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;, &quot;083&quot;… #&gt; $ NOM_MUN &lt;chr&gt; &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO&quot;, &quot;SOCOLTENANGO… #&gt; $ CVE_LOC &lt;chr&gt; &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051&quot;, &quot;0051… #&gt; $ NOM_LOC &lt;chr&gt; &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA ROJA&quot;, &quot;ESTRELLA … #&gt; $ AREA &lt;chr&gt; &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;, &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;, &quot;ESP&quot;, &quot;MAT&quot;, &quot;FCE&quot;… #&gt; $ GRADO &lt;int&gt; 3, 3, 3, 4, 4, 4, 5, 5, 5, 6, 6, 6, 3, 3, 3, 4, 4, 4, 5, 5, 5… #&gt; $ PUNTAJE &lt;dbl&gt; 617, 731, 618, 595, 730, 616, 431, 435, 440, 414, 433, 440, 5… Variables almacenadas en filas y columnas El problema más difícil es cuando las variables están tanto en filas como en columnas, veamos una base de datos de fertilidad. ¿Cuáles son las variables en estos datos? data(&quot;df_fertility&quot;) df_fertility #&gt; # A tibble: 306 x 11 #&gt; state size_localidad est age_15_19 age_20_24 age_25_29 age_30_34 age_35_39 #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 A… Menos de 2 50… Valor 74.2 175. 175. 102. 59.0 #&gt; 2 01 A… Menos de 2 50… Erro… 6.71 11.0 9.35 8.05 7.29 #&gt; 3 01 A… 2 500-14 999 … Valor 82.5 171. 140. 103. 72.0 #&gt; 4 01 A… 2 500-14 999 … Erro… 9.79 12.5 10.4 8.76 9.08 #&gt; 5 01 A… 15 000-49 999… Valor 72.6 146. 147. 99.0 58.7 #&gt; 6 01 A… 15 000-49 999… Erro… 7.07 10.8 10.5 8.11 7.37 #&gt; 7 01 A… 100 000 y más… Valor 66.3 120. 102. 84.2 53.9 #&gt; 8 01 A… 100 000 y más… Erro… 7.57 8.66 8.98 8.59 6.61 #&gt; 9 02 B… Menos de 2 50… Valor 89.6 158. 117. 86.0 42.0 #&gt; 10 02 B… Menos de 2 50… Erro… 15.8 17.2 13.2 12.3 9.64 #&gt; # … with 296 more rows, and 3 more variables: age_40_44 &lt;dbl&gt;, age_45_49 &lt;dbl&gt;, #&gt; # global &lt;dbl&gt; Estos datos tienen variables en columnas individuales (state, size_localidad), en múltiples columnas (grupo de edad, age_15_19,..) y en filas (Valor y Error estándar). Comencemos por apilar las columnas. fertility_long &lt;- pivot_longer(df_fertility, cols = age_15_19:global, names_to = &quot;age_bracket&quot;, values_to = &quot;value&quot;, names_prefix = &quot;age_&quot;) fertility_long #&gt; # A tibble: 2,448 x 5 #&gt; state size_localidad est age_bracket value #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 01 Aguascalientes Menos de 2 500 habitantes Valor 15_19 74.2 #&gt; 2 01 Aguascalientes Menos de 2 500 habitantes Valor 20_24 175. #&gt; 3 01 Aguascalientes Menos de 2 500 habitantes Valor 25_29 175. #&gt; 4 01 Aguascalientes Menos de 2 500 habitantes Valor 30_34 102. #&gt; 5 01 Aguascalientes Menos de 2 500 habitantes Valor 35_39 59.0 #&gt; 6 01 Aguascalientes Menos de 2 500 habitantes Valor 40_44 23.0 #&gt; 7 01 Aguascalientes Menos de 2 500 habitantes Valor 45_49 4.49 #&gt; 8 01 Aguascalientes Menos de 2 500 habitantes Valor global 3.06 #&gt; 9 01 Aguascalientes Menos de 2 500 habitantes Error estándar 15_19 6.71 #&gt; 10 01 Aguascalientes Menos de 2 500 habitantes Error estándar 20_24 11.0 #&gt; # … with 2,438 more rows Podemos crear algunas variables adicionales. fertility_vars &lt;- fertility_long %&gt;% mutate( state_code = str_sub(state, 1, 2), state_name = str_sub(state, 4) ) %&gt;% select(-state) fertility_vars #&gt; # A tibble: 2,448 x 6 #&gt; size_localidad est age_bracket value state_code state_name #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 Menos de 2 500 habita… Valor 15_19 74.2 01 Aguascalien… #&gt; 2 Menos de 2 500 habita… Valor 20_24 175. 01 Aguascalien… #&gt; 3 Menos de 2 500 habita… Valor 25_29 175. 01 Aguascalien… #&gt; 4 Menos de 2 500 habita… Valor 30_34 102. 01 Aguascalien… #&gt; 5 Menos de 2 500 habita… Valor 35_39 59.0 01 Aguascalien… #&gt; 6 Menos de 2 500 habita… Valor 40_44 23.0 01 Aguascalien… #&gt; 7 Menos de 2 500 habita… Valor 45_49 4.49 01 Aguascalien… #&gt; 8 Menos de 2 500 habita… Valor global 3.06 01 Aguascalien… #&gt; 9 Menos de 2 500 habita… Error está… 15_19 6.71 01 Aguascalien… #&gt; 10 Menos de 2 500 habita… Error está… 20_24 11.0 01 Aguascalien… #&gt; # … with 2,438 more rows Finalmente, la columna est no es una variable, sino que almacena el nombre de 2 variables: Valor y Error Estándar la operación que debemos aplicar (pivot_wider()) es el inverso de apilar (pivot_longer), sus argumentos son: data: data.frame que vamos a pivotear. names_from: nombre o nombres de las columnas (sin comillas) de los cuáles obtendremos los nombres de las nuevas columnas. values_from: nombre o nombres de las columnas (sin comillas) de los cuáles obtendremos los valores que llenarán las nuevas columnas. fertility_tidy &lt;- pivot_wider(fertility_vars, names_from = est, values_from = value) Y podemos mejorar los nombres de las columnas, una opción rápida es usar el paquete janitor. fertility_tidy %&gt;% janitor::clean_names() %&gt;% glimpse() #&gt; Observations: 1,224 #&gt; Variables: 6 #&gt; $ size_localidad &lt;chr&gt; &quot;Menos de 2 500 habitantes&quot;, &quot;Menos de 2 500 habitante… #&gt; $ age_bracket &lt;chr&gt; &quot;15_19&quot;, &quot;20_24&quot;, &quot;25_29&quot;, &quot;30_34&quot;, &quot;35_39&quot;, &quot;40_44&quot;, … #&gt; $ state_code &lt;chr&gt; &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, &quot;01&quot;, … #&gt; $ state_name &lt;chr&gt; &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, &quot;Aguascalientes&quot;, … #&gt; $ valor &lt;dbl&gt; 74.2032276, 175.0281396, 174.5274362, 101.5836230, 58.… #&gt; $ error_estandar &lt;dbl&gt; 6.70671255, 11.00329648, 9.34594033, 8.04764573, 7.286… o podemos hacerlo manualmente names(fertility_tidy)[5:6] &lt;- c(&quot;est&quot;, &quot;std_error&quot;) Ahora es inmediato no solo hacer gráficas sino también ajustar un modelo. # ajustamos un modelo lineal donde la variable respuesta es temperatura # máxima, y la variable explicativa es el mes fertility_sub &lt;- filter(fertility_tidy, age_bracket != &quot;global&quot;) fertility_lm &lt;- lm(est ~ age_bracket, data = fertility_sub) summary(fertility_lm) #&gt; #&gt; Call: #&gt; lm(formula = est ~ age_bracket, data = fertility_sub) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -55.060 -5.778 -0.383 6.874 55.133 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 66.505 1.143 58.21 &lt;2e-16 *** #&gt; age_bracket20_24 70.675 1.616 43.74 &lt;2e-16 *** #&gt; age_bracket25_29 58.881 1.616 36.44 &lt;2e-16 *** #&gt; age_bracket30_34 22.910 1.616 14.18 &lt;2e-16 *** #&gt; age_bracket35_39 -20.312 1.616 -12.57 &lt;2e-16 *** #&gt; age_bracket40_44 -53.346 1.616 -33.01 &lt;2e-16 *** #&gt; age_bracket45_49 -64.797 1.616 -40.10 &lt;2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 14.13 on 1064 degrees of freedom #&gt; Multiple R-squared: 0.922, Adjusted R-squared: 0.9215 #&gt; F-statistic: 2096 on 6 and 1064 DF, p-value: &lt; 2.2e-16 Vale la pena notar que aunque los datos limpios facilitan las tareas de análisis, distintas funciones o tareas requieren los datos en distintos formas y saber pivotear las tablas es muy útil. Grafica el valor estimado de fertilidad del grupo de edad 20-24 contra 25-29. ¿Qué transformación debes hacer? Tip: elimina la columna que corresponde al error estándar antes de ensanchar los datos. Una misma unidad observacional está almacenada en múltiples tablas También es común que los valores sobre una misma unidad observacional estén separados en muchas tablas o archivos, es común que estas tablas esten divididas de acuerdo a una variable, de tal manera que cada archivo representa a una persona, año o ubicación. Para juntar los archivos hacemos lo siguiente: Enlistamos las rutas de los archivos. Leemos cada archivo y agregamos una columna con el nombre del archivo. Combinamos las tablas en un solo data frame. Veamos un ejemplo, descargamos la carpeta con los datos de varios contaminantes de RAMA, usethis::use_zip(&quot;https://github.com/tereom/estcomp/raw/master/data-raw/19RAMA.zip&quot;, &quot;data&quot;) ésta contiene 9 archivos de excel que almacenan información de monitoreo de contaminantes. Cada archivo contiene información de un contaminante y el nombre del archivo indica el contaminante. Los pasos en R (usando el paquete purrr), primero creamos un vector con los nombres de los archivos en un directorio, eligiendo aquellos que contengan las letras “.csv”. paths &lt;- dir(&quot;data/19RAMA&quot;, pattern = &quot;\\\\.xls$&quot;, full.names = TRUE) Después le asignamos el nombre del archivo al nombre de cada elemento del vector. Este paso se realiza para preservar los nombres de los archivos ya que estos los asignaremos a una variable mas adelante. paths &lt;- set_names(paths, basename(paths)) La función map_df itera sobre cada dirección, lee el archivo excel de dicha dirección y los combina en un data frame. library(readxl) rama &lt;- map_df(paths, read_excel, .id = &quot;FILENAME&quot;) # eliminamos la basura del id rama &lt;- rama %&gt;% mutate(PARAMETRO = str_remove(FILENAME, &quot;2019&quot;) %&gt;% str_remove(&quot;.xls&quot;)) %&gt;% select(PARAMETRO, FECHA:AJU) # y apilamos para tener una columna por estación rama_tidy &lt;- rama %&gt;% gather(estacion, valor, ACO:AJU) %&gt;% mutate(valor = ifelse(-99, NA, valor)) rama_tidy #&gt; # A tibble: 1,648,512 x 5 #&gt; PARAMETRO FECHA HORA estacion valor #&gt; &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 CO 2019-01-01 00:00:00 1 ACO NA #&gt; 2 CO 2019-01-01 00:00:00 2 ACO NA #&gt; 3 CO 2019-01-01 00:00:00 3 ACO NA #&gt; 4 CO 2019-01-01 00:00:00 4 ACO NA #&gt; 5 CO 2019-01-01 00:00:00 5 ACO NA #&gt; 6 CO 2019-01-01 00:00:00 6 ACO NA #&gt; 7 CO 2019-01-01 00:00:00 7 ACO NA #&gt; 8 CO 2019-01-01 00:00:00 8 ACO NA #&gt; 9 CO 2019-01-01 00:00:00 9 ACO NA #&gt; 10 CO 2019-01-01 00:00:00 10 ACO NA #&gt; # … with 1,648,502 more rows Otras consideraciones En las buenas prácticas es importante tomar en cuenta los siguientes puntos: Incluir un encabezado con el nombre de las variables. Los nombres de las variables deben ser entendibles (e.g. AgeAtDiagnosis es mejor que AgeDx). En general los datos se deben guardar en un archivo por tabla. Escribir un script con las modificaciones que se hicieron a los datos crudos (reproducibilidad). Otros aspectos importantes en la limpieza de datos son: selección del tipo de variables (por ejemplo fechas), datos faltantes, typos y detección de valores atípicos. Recursos adicionales Data Transformation Cheat Sheet de RStudio. Data Wrangling Cheat Sheet de RStudio. Limpiar nombres de columnas, eliminar filas vacías y más, paquete janitor. Lectura de datos tabulares con distintas estructuras, paquete tidycells. Referencias "],
["temas-selectos-de-r.html", "Sección 4 Temas selectos de R", " Sección 4 Temas selectos de R Esta sección describe algunos aspectos de R como lenguaje de programación (en contraste a introducir funciones para análisis de datos). Es importante tener en cuenta como funciona R para escribir código más claro, minimizando errores y más eficiente. Las referencias para esta sección son Wickham (2019) y Wickham and Grolemund (2017). Referencias "],
["funciones-e-iteración.html", "4.1 Funciones e iteración", " 4.1 Funciones e iteración “To understand computations in R, two slogans are helpful: * Everything that exists is an object. * Everything that happens is a function call.” — John Chambers Funciones En R todas las operaciones son producto de la llamada a una función, esto incluye operaciones como +, operadores que controlan flujo como for, if y while, e incluso operadores para obtener subconjuntos como [ ] y $. a &lt;- 3 b &lt;- 4 a + b #&gt; [1] 7 `+`(a, b) #&gt; [1] 7 for (i in 1:2) print(i) #&gt; [1] 1 #&gt; [1] 2 `for`(i, 1:2, print(i)) #&gt; [1] 1 #&gt; [1] 2 Para escribir código eficiente y fácil de leer es importante saber escribir funciones, se dice que si hiciste copy-paste de una sección de tu código 3 o más veces es momento de escribir una función. Escribimos una función para calcular un promedio ponderado: wtd_mean &lt;- function(x, wt = rep(1, length(x))) { sum(x * wt) / sum(wt) } Notemos que esta función recibe hasta dos argumentos: x: el vector a partir del cual calcularemos el promedio y wt: un vector de ponderadores para cada componente del vector x. Notemos además que al segundo argumento le asignamos un valor predeterminado, esto implica que si no especificamos los ponderadores la función usará el valor predeterminado y promediara con mismo peso a todas las componentes. wtd_mean(c(1:10)) #&gt; [1] 5.5 wtd_mean(1:10, 10:1) #&gt; [1] 4 Veamos como escribir una función que reciba un vector y devuelva el mismo vector centrado en cero. Comenzamos escribiendo el código para un caso particular, por ejemplo, reescalando el vector \\((0, 5, 10)\\). vec &lt;- c(0, 5, 10) vec - mean(vec) #&gt; [1] -5 0 5 Una vez que lo probamos lo convertimos en función: center_vector &lt;- function(vec) { vec - mean(vec) } center_vector(c(0, 5, 10)) #&gt; [1] -5 0 5 Ejercicio Escribe una función que reciba un vector y devuelva el mismo vector reescalado al rango 0 a 1. Comienza escribiendo el código para un caso particular, por ejemplo, empieza reescalando el vector . Tip: la función range() devuelve el rango de un vector. Estructura de una función Las funciones de R tienen tres partes: El cuerpo: el código dentro de la función body(wtd_mean) #&gt; { #&gt; sum(x * wt)/sum(wt) #&gt; } Los formales: la lista de argumentos que controlan como puedes llamar a la función, formals(wtd_mean) #&gt; $x #&gt; #&gt; #&gt; $wt #&gt; rep(1, length(x)) El ambiente: el mapeo de la ubicación de las variables de la función, cómo busca la función cada función el valor de las variables que usa. environment(wtd_mean) #&gt; &lt;environment: R_GlobalEnv&gt; Veamos mas ejemplos, ¿qué regresan las siguientes funciones? # 1 x &lt;- 5 f &lt;- function(){ y &lt;- 10 c(x = x, y = y) } rm(x, f) # 2 x &lt;- 5 g &lt;- function(){ x &lt;- 20 y &lt;- 10 c(x = x, y = y) } rm(x, g) # 3 x &lt;- 5 h &lt;- function(){ y &lt;- 10 i &lt;- function(){ z &lt;- 20 c(x = x, y = y, z = z) } i() } # 4 ¿qué ocurre si la corremos por segunda vez? j &lt;- function(){ if (!exists(&quot;a&quot;)){ a &lt;- 5 } else{ a &lt;- a + 1 } print(a) } x &lt;- 0 y &lt;- 10 # 5 ¿qué regresa k()? ¿y k()()? k &lt;- function(){ x &lt;- 1 function(){ y &lt;- 2 x + y } } Las reglas de búsqueda determinan como se busca el valor de una variable libre en una función. A nivel lenguaje R usa lexical scoping, esto implica que en R los valores de los símbolos se basan en como se anidan las funciones cuando fueron creadas y no en como son llamadas. Las reglas de bússqueda de R, lexical scoping, son: Enmascaramiento de nombres: los nombres definidos dentro de una función enmascaran aquellos definidos fuera. x &lt;- 5 g &lt;- function(){ x &lt;- 20 y &lt;- 10 c(x = x, y = y) } g() #&gt; x y #&gt; 20 10 Si un nombre no está definido R busca un nivel arriba, x &lt;- 5 f &lt;- function(){ y &lt;- 10 c(x = x, y = y) } f() #&gt; x y #&gt; 5 10 Y lo mismo ocurre cuando una función está definida dentro de una función. x &lt;- 5 h &lt;- function(){ y &lt;- 10 i &lt;- function(){ z &lt;- 20 c(x = x, y = y, z = z) } i() } h() #&gt; x y z #&gt; 5 10 20 Y cuando una función crea otra función: x &lt;- 10 k &lt;- function(){ x &lt;- 1 function(){ y &lt;- 2 x + y } } k()() #&gt; [1] 3 Funciones o variables: en R las funciones son objetos, sin embargo una función y un objeto no-función pueden llamarse igual. En estos casos usamos un nombre en el llamado de una función se buscará únicamente entre los objetos de tipo función. p &lt;- function(x) { 5 * x } m &lt;- function(){ p &lt;- 2 p(p) } m() #&gt; [1] 10 Cada vez que llamamos una función es un ambiente limpio, es decir, los objetos que se crean durante la llamada de la función no se pasan a las llamadas posteriores. # 4 ¿qué ocurre si la corremos por segunda vez? j &lt;- function(){ if (!exists(&quot;a&quot;)) { a &lt;- 5 } else{ a &lt;- a + 1 } print(a) } j() #&gt; [1] 4 j() #&gt; [1] 4 Búsqueda dinámica: la búsqueda lexica determina donde se busca un valor más no determina cuando. En el caso de R los valores se buscan cuando la función se llama, y no cuando la función se crea. q &lt;- function() x + 1 x &lt;- 15 q() #&gt; [1] 16 x &lt;- 20 q() #&gt; [1] 21 Las reglas de búsqueda de R lo hacen muy flexible pero también propenso a cometer errores. Una función que suele resultar útil para revisar las dependencias de nuestras funciones es findGlobals() en el paquete codetools, esta función enlista las dependencias dentro de una función: codetools::findGlobals(q) #&gt; [1] &quot;+&quot; &quot;x&quot; Observaciones del uso de funciones Cuando llamamos a una función podemos especificar los argumentos en base a posición, nombre completo o nombre parcial: f &lt;- function(abcdef, bcde1, bcde2) { c(a = abcdef, b1 = bcde1, b2 = bcde2) } # Posición f(1, 2, 3) #&gt; a b1 b2 #&gt; 1 2 3 f(2, 3, abcdef = 1) #&gt; a b1 b2 #&gt; 1 2 3 # Podemos abreviar el nombre de los argumentos f(2, 3, a = 1) #&gt; a b1 b2 #&gt; 1 2 3 # Siempre y cuando la abreviación no sea ambigua f(1, 3, b = 1) #&gt; Error in f(1, 3, b = 1): argument 3 matches multiple formal arguments Los argumentos de las funciones en R se evalúan conforme se necesitan (lazy evaluation), f &lt;- function(a, b){ a ^ 2 } f(2) #&gt; [1] 4 La función anterior nunca utiliza el argumento b, de tal manera que f(2) no produce ningún error. Funciones con el mismo nombre en distintos paquetes: La función filter() (incluida en R base) aplica un filtro lineal a una serie de tiempo de una variable. x &lt;- 1:100 filter(x, rep(1, 3)) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] NA 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 #&gt; [19] 57 60 63 66 69 72 75 78 81 84 87 90 93 96 99 102 105 108 #&gt; [37] 111 114 117 120 123 126 129 132 135 138 141 144 147 150 153 156 159 162 #&gt; [55] 165 168 171 174 177 180 183 186 189 192 195 198 201 204 207 210 213 216 #&gt; [73] 219 222 225 228 231 234 237 240 243 246 249 252 255 258 261 264 267 270 #&gt; [91] 273 276 279 282 285 288 291 294 297 NA Ahora cargamos dplyr. library(dplyr) #&gt; #&gt; Attaching package: &#39;dplyr&#39; #&gt; The following objects are masked from &#39;package:stats&#39;: #&gt; #&gt; filter, lag #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; intersect, setdiff, setequal, union filter(x, rep(1, 3)) #&gt; Error in UseMethod(&quot;filter_&quot;): no applicable method for &#39;filter_&#39; applied to an object of class &quot;c(&#39;integer&#39;, &#39;numeric&#39;)&quot; R tiene un conflicto en la función a llamar, nosotros requerimos usar filter de stats y no la función filter de dplyr. R utiliza por default la función que pertenece al último paquete que se cargó. La función search() nos enlista los paquetes cargados y el orden. search() #&gt; [1] &quot;.GlobalEnv&quot; &quot;package:dplyr&quot; &quot;package:forcats&quot; #&gt; [4] &quot;package:stringr&quot; &quot;package:purrr&quot; &quot;package:readr&quot; #&gt; [7] &quot;package:tidyr&quot; &quot;package:tibble&quot; &quot;package:ggplot2&quot; #&gt; [10] &quot;package:tidyverse&quot; &quot;package:stats&quot; &quot;package:graphics&quot; #&gt; [13] &quot;package:grDevices&quot; &quot;package:utils&quot; &quot;package:datasets&quot; #&gt; [16] &quot;package:methods&quot; &quot;Autoloads&quot; &quot;package:base&quot; Una opción es especificar el paquete en la llamada de la función: stats::filter(x, rep(1, 3)) #&gt; Time Series: #&gt; Start = 1 #&gt; End = 100 #&gt; Frequency = 1 #&gt; [1] NA 6 9 12 15 18 21 24 27 30 33 36 39 42 45 48 51 54 #&gt; [19] 57 60 63 66 69 72 75 78 81 84 87 90 93 96 99 102 105 108 #&gt; [37] 111 114 117 120 123 126 129 132 135 138 141 144 147 150 153 156 159 162 #&gt; [55] 165 168 171 174 177 180 183 186 189 192 195 198 201 204 207 210 213 216 #&gt; [73] 219 222 225 228 231 234 237 240 243 246 249 252 255 258 261 264 267 270 #&gt; [91] 273 276 279 282 285 288 291 294 297 NA Como alternativa surge el paquete conflicted que alerta cuando hay conflictos y tiene funciones para especificar a que paquete se desea dar preferencia en una sesión de R. "],
["vectores.html", "4.2 Vectores", " 4.2 Vectores En R se puede trabajar con distintas estructuras de datos, algunas son de una sola dimensión y otras permiten más, como indica el diagrama de abajo: Hasta ahora nos hemos centrado en trabajar con data.frames, y hemos usado vectores atómicos sin profundizar, en esta sección se explican características de los vectores, y veremos que son la base de los data.frames. En R hay dos tipos de vectores, esto es, estructuras de datos de una sola dimensión: los vectores atómicos y las listas. Los vectores atómicos pueden ser de 6 tipos: lógico, entero, double, caracter, complejo y raw. Los dos últimos son poco comunes. Vector atómico de tipo lógico: a &lt;- c(TRUE, FALSE, FALSE) a #&gt; [1] TRUE FALSE FALSE Numérico (double): b &lt;- c(5, 2, 4.1, 7, 9.2) b #&gt; [1] 5.0 2.0 4.1 7.0 9.2 b[1] #&gt; [1] 5 b[2] #&gt; [1] 2 b[2:4] #&gt; [1] 2.0 4.1 7.0 Las operaciones básicas con vectores atómicos son componente a componente: c &lt;- b + 10 c #&gt; [1] 15.0 12.0 14.1 17.0 19.2 d &lt;- sqrt(b) d #&gt; [1] 2.236068 1.414214 2.024846 2.645751 3.033150 b + d #&gt; [1] 7.236068 3.414214 6.124846 9.645751 12.233150 10 * b #&gt; [1] 50 20 41 70 92 b * d #&gt; [1] 11.180340 2.828427 8.301867 18.520259 27.904982 Y podemos crear secuencias como sigue: e &lt;- 1:10 e #&gt; [1] 1 2 3 4 5 6 7 8 9 10 f &lt;- seq(0, 1, 0.25) f #&gt; [1] 0.00 0.25 0.50 0.75 1.00 Para calcular características de vectores atómicos usamos funciones: # media del vector mean(b) #&gt; [1] 5.46 # suma de sus componentes sum(b) #&gt; [1] 27.3 # longitud del vector length(b) #&gt; [1] 5 Y ejemplo de vector atómico de tipo caracter y funciones: frutas &lt;- c(&#39;manzana&#39;, &#39;manzana&#39;, &#39;pera&#39;, &#39;plátano&#39;, &#39;fresa&#39;, &quot;kiwi&quot;) frutas #&gt; [1] &quot;manzana&quot; &quot;manzana&quot; &quot;pera&quot; &quot;plátano&quot; &quot;fresa&quot; &quot;kiwi&quot; grep(&quot;a&quot;, frutas) #&gt; [1] 1 2 3 4 5 gsub(&quot;a&quot;, &quot;x&quot;, frutas) #&gt; [1] &quot;mxnzxnx&quot; &quot;mxnzxnx&quot; &quot;perx&quot; &quot;plátxno&quot; &quot;fresx&quot; &quot;kiwi&quot; Las listas, a diferencia de los vectores atómicos, pueden contener otras listas. Las listas son muy flexibles pues pueden almacenar objetos de cualquier tipo. x &lt;- list(1:3, &quot;Mila&quot;, c(TRUE, FALSE, FALSE), c(2, 5, 3.2)) str(x) #&gt; List of 4 #&gt; $ : int [1:3] 1 2 3 #&gt; $ : chr &quot;Mila&quot; #&gt; $ : logi [1:3] TRUE FALSE FALSE #&gt; $ : num [1:3] 2 5 3.2 Las listas son vectores recursivos debido a que pueden almacenar otras listas. y &lt;- list(list(list(list()))) str(y) #&gt; List of 1 #&gt; $ :List of 1 #&gt; ..$ :List of 1 #&gt; .. ..$ : list() Para construir subconjuntos a partir de listas usamos [] y [[]]. En el primer caso siempre obtenemos como resultado una lista: x_1 &lt;- x[1] x_1 #&gt; [[1]] #&gt; [1] 1 2 3 str(x_1) #&gt; List of 1 #&gt; $ : int [1:3] 1 2 3 Y en el caso de [[]] extraemos un componente de la lista, eliminando un nivel de la jerarquía de la lista. x_2 &lt;- x[[1]] x_2 #&gt; [1] 1 2 3 str(x_2) #&gt; int [1:3] 1 2 3 ¿Cómo se comparan y, y[1] y y[[1]]? Propiedades Todos los vectores (atómicos y listas) tienen las propiedades tipo y longitud, la función typeof() se usa para determinar el tipo, typeof(a) #&gt; [1] &quot;logical&quot; typeof(b) #&gt; [1] &quot;double&quot; typeof(frutas) #&gt; [1] &quot;character&quot; typeof(x) #&gt; [1] &quot;list&quot; y length() la longitud: length(a) #&gt; [1] 3 length(frutas) #&gt; [1] 6 length(x) #&gt; [1] 4 length(y) #&gt; [1] 1 La flexibilidad de las listas las convierte en estructuras muy útiles y muy comunes, muchas funciones regresan resultados en forma de lista. Incluso podemos ver que un data.frame es una lista de vectores, donde todos los vectores son de la misma longitud. Adicionalmente, los vectores pueden tener atributo de nombres, que puede usarse para indexar. names(b) &lt;- c(&quot;momo&quot;, &quot;mila&quot;, &quot;duna&quot;, &quot;milu&quot;, &quot;moka&quot;) b #&gt; momo mila duna milu moka #&gt; 5.0 2.0 4.1 7.0 9.2 b[&quot;moka&quot;] #&gt; moka #&gt; 9.2 names(x) &lt;- c(&quot;a&quot;, &quot;b&quot;, &quot;c&quot;, &quot;d&quot;) x #&gt; $a #&gt; [1] 1 2 3 #&gt; #&gt; $b #&gt; [1] &quot;Mila&quot; #&gt; #&gt; $c #&gt; [1] TRUE FALSE FALSE #&gt; #&gt; $d #&gt; [1] 2.0 5.0 3.2 x$a #&gt; [1] 1 2 3 x[[&quot;c&quot;]] #&gt; [1] TRUE FALSE FALSE "],
["iteración.html", "4.3 Iteración", " 4.3 Iteración En análisis de datos es común implementar rutinas iteraivas, esto es, cuando debemos aplicar los mismos pasos a distintas entradas. Veremos que hay dos paradigmas de iteración: Programación imperativa: ciclos for y ciclos while. Programación funcional: los ciclos se implmentan mediante funciones, La ventaja de la programación imperativa es que hacen la iteración de manera clara, sin embargo, veremos que una vez que nos familiarizamos con el paradigma de programación funcional, resulta en código más fácil de mantener y menos propenso a errores. Ciclos for Supongamos que tenemos una base de datos y queremos calcular la media de sus columnas numéricas. df &lt;- data.frame(id = 1:10, a = rnorm(10), b = rnorm(10, 2), c = rnorm(10, 3), d = rnorm(10, 4)) df #&gt; id a b c d #&gt; 1 1 0.3612787 2.4293341 2.507944 3.909600 #&gt; 2 2 -0.4806637 3.3721098 4.353338 5.014960 #&gt; 3 3 -0.8846985 3.0396986 3.135359 3.266996 #&gt; 4 4 -1.5948402 2.0896639 2.532486 3.175891 #&gt; 5 5 0.2546178 2.0533773 2.840186 3.189521 #&gt; 6 6 -0.5459508 0.9785084 4.328133 3.483898 #&gt; 7 7 0.3781570 2.1315852 2.578436 4.582246 #&gt; 8 8 0.6694799 2.3508348 3.015562 2.259758 #&gt; 9 9 -0.2358184 1.7352233 2.569199 4.681486 #&gt; 10 10 -1.5209278 1.1932796 4.005951 4.933696 Podemos crear el código para cada columna pero esto involucra copy-paste y no será muy práctico si aumenta el número de columnas: mean(df$a) #&gt; [1] -0.3599366 mean(df$b) #&gt; [1] 2.137361 mean(df$c) #&gt; [1] 3.186659 mean(df$d) #&gt; [1] 3.849805 Con un ciclo for sería: salida &lt;- vector(&quot;double&quot;, 4) for (i in 1:4) { salida[[i]] &lt;- mean(df[[i + 1]]) } salida #&gt; [1] -0.3599366 2.1373615 3.1866594 3.8498052 Los ciclos for tienen 3 componentes: La salida: salida &lt;- vector(&quot;double&quot;, 4). Es importante especificar el tamaño de la salida antes de iniciar el ciclo for, de lo contrario el código puede ser muy lento. La secuencia: determina sobre que será la iteración, la función seq_along puede resultar útil. salida &lt;- vector(&quot;double&quot;, 5) for (i in seq_along(df)) { salida[[i]] &lt;- mean(df[[i]]) } seq_along(df) #&gt; [1] 1 2 3 4 5 El cuerpo: salida[[i]] &lt;- mean(df[[i]]), el código que calcula lo que nos interesa sobre cada objeto en la iteración. Calcula el valor máximo de cada columna numérica de los datos de ENLACE 3o de primaria enlacep_2013_3. library(estcomp) head(enlacep_2013_3) #&gt; # A tibble: 6 x 6 #&gt; CVE_ENT PUNT_ESP_3 PUNT_MAT_3 PUNT_FCE_3 ALUM_NOCONFIABLE_3 ALUM_EVAL_3 #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 01 513 536 459 0 40 #&gt; 2 01 472 472 404 2 36 #&gt; 3 01 496 526 426 0 96 #&gt; 4 01 543 586 495 5 74 #&gt; 5 01 554 560 506 0 30 #&gt; 6 01 505 546 460 0 67 Recordando la limpieza de datos de la sección anterior en uno de los últimos ejercicios leíamos archivos de manera iteativa. En este ejercicio descargaremos un archivo zip con archivos csv que contienen información de monitoreo de contaminantes en ciudad de México (RAMA), en particular PM2.5. Y juntaremos la información en una sola tabla, la siguiente instrucción descarga los datos en una carpeta data. library(usethis) use_directory(&quot;data&quot;) # crea carpeta en caso de que no exista ya usethis::use_zip(&quot;https://github.com/tereom/estcomp/raw/master/data-raw/19RAMA.zip&quot;, &quot;data&quot;) # descargar y descomprimir zip Enlistamos los archivos xls en la carpeta. paths &lt;- dir(&quot;data/19RAMA&quot;, pattern = &quot;\\\\.xls$&quot;, full.names = TRUE) Tu turno, implementa un ciclo for para leer los archivos y crear una única tabla de datos. Si pegas los data.frames de manera iterativa sugerimos usar la función bind_rows(). Programación funcional Ahora veremos como abordar iteración usando programación funcional. Regresando al ejemplo inicial de calcular la media de las columnas de una tabla de datos: salida &lt;- vector(&quot;double&quot;, 4) for (i in 1:4) { salida[[i]] &lt;- mean(df[[i + 1]]) } salida #&gt; [1] -0.3599366 2.1373615 3.1866594 3.8498052 Podemos crear una función que calcula la media de las columnas de un data.frame: col_media &lt;- function(df) { salida &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { salida[i] &lt;- mean(df[[i]]) } salida } col_media(df) #&gt; [1] 5.5000000 -0.3599366 2.1373615 3.1866594 3.8498052 col_media(select(iris, -Species)) #&gt; [1] 5.843333 3.057333 3.758000 1.199333 Y podemos extender a crear más funciones que describan los datos: col_mediana &lt;- function(df) { salida &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { salida[i] &lt;- median(df[[i]]) } salida } col_sd &lt;- function(df) { salida &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { salida[i] &lt;- sd(df[[i]]) } salida } Podemos hacer nuestro código más general y compacto escribiendo una función que reciba los datos sobre los que queremos iterar y la función que queremos aplicar: col_describe &lt;- function(df, fun) { salida &lt;- vector(&quot;double&quot;, length(df)) for (i in seq_along(df)) { salida[i] &lt;- fun(df[[i]]) } salida } col_describe(df, median) #&gt; [1] 5.500000 -0.358241 2.110625 2.927874 3.696749 col_describe(df, mean) #&gt; [1] 5.5000000 -0.3599366 2.1373615 3.1866594 3.8498052 Ahora utilizaremos esta idea de pasar funciones a funciones para eliminar los ciclos for. La iteración a través de funciones es muy común en R, hay funciones para hacer esto en R base (lapply(), apply(), sapply()). Nosotros utilizaremos las funciones del paquete purrr, La familia de funciones del paquete iteran siempre sobre un vector (vector atómico o lista), aplican una función a cada parte y regresan un nuevo vector de la misma longitud que el vector entrada. Cada función especifica en su nombre el tipo de salida: map() devuelve una lista. map_lgl() devuelve un vector lógico. map_int() devuelve un vector entero. map_dbl() devuelve un vector double. map_chr() devuelve un vector caracter. map_df() devuelve un data.frame. En el ejemplo de las medias, map puede recibir un data.frame (lista de vectores) y aplicará las funciones a las columnas del mismo. library(purrr) map_dbl(df, mean) #&gt; id a b c d #&gt; 5.5000000 -0.3599366 2.1373615 3.1866594 3.8498052 map_dbl(select(iris, -Species), median) #&gt; Sepal.Length Sepal.Width Petal.Length Petal.Width #&gt; 5.80 3.00 4.35 1.30 Usaremos map para ajustar un modelo lineal a subconjuntos de los datos mtcars determinados por el cilindraje del motor. models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(function(df) lm(mpg ~ wt, data = df)) Podemos usar la notación . para hacer código más corto: models &lt;- mtcars %&gt;% split(.$cyl) %&gt;% map(~lm(mpg ~ wt, data = .)) Usemos map_** para unir tablas de datos que están almacenadas en múltiples archivos csv. En este caso es más apropiado usar map_df library(readxl) rama &lt;- map_df(paths, read_excel, .id = &quot;FILENAME&quot;) Ejercicio Usa la función map_** para calcular el número de valores únicos en las columnas de iris. Usa la función map_** para extraer el coeficiete de la variable wt para cada modelo: models[[1]]$coefficients[2] #&gt; wt #&gt; -5.647025 "],
["rendimiento-en-r.html", "4.4 Rendimiento en R", " 4.4 Rendimiento en R “We should forget about small efficiencies, say about 97% of the time: premature optimization is the root of all evil. Yet we should not pass up our opportunities in that critical 3%. A good programmer will not be lulled into complacency by such reasoning, he will be wise to look carefully at the critical code; but only after that code has been identified.” -Donald Knuth Diseña primero, luego optimiza. La optimización del código es un proceso iterativo: Encuentra el cuello de botella más importante. Intenta eliminarlo (no siempre se puede). Repite hasta que tu código sea lo suficientemente rápido. Diagnosticar Una vez que tienes código que se puede leer y funciona, el perfilamiento (profiling) del código es un método sistemático que nos permite conocer cuanto tiempo se esta usando en diferentes partes del programa. Comenzaremos con la función system.time (no es perfilamiento aún), esta calcula el tiempo en segundos que toma ejecutar una expresión (si hay un error, regresa el tiempo hasta que ocurre el error): data(&quot;Batting&quot;, package = &quot;Lahman&quot;) system.time(lm(R ~ AB + teamID, Batting)) #&gt; user system elapsed #&gt; 3.781 0.128 3.909 user time: Tiempo usado por el CPU(s) para evaluar esta expresión, tiempo que experimenta la computadora. elapsed time: tiempo en el reloj, tiempo que experimenta la persona. Notemos que el tiempo de usuario (user) puede ser menor al tiempo transcurrido (elapsed), system.time(readLines(&quot;http://www.jhsph.edu&quot;)) #&gt; user system elapsed #&gt; 0.013 0.012 8.742 o al revés: library(parallel) system.time(mclapply(2000:2007, function(x){ sub &lt;- subset(Batting, yearID == x) lm(R ~ AB + playerID, sub) }, mc.cores = 7)) #&gt; user system elapsed #&gt; 12.779 0.676 9.673 Comparemos la velocidad de dplyr con funciones que se encuentran en R estándar y plyr. # dplyr dplyr_st &lt;- system.time({ Batting %&gt;% group_by(playerID) %&gt;% summarise(total = sum(R, na.rm = TRUE), n = n()) %&gt;% dplyr::arrange(desc(total)) }) # plyr plyr_st &lt;- system.time({ Batting %&gt;% plyr::ddply(&quot;playerID&quot;, plyr::summarise, total = sum(R, na.rm = TRUE), n = length(R)) %&gt;% plyr::arrange(-total) }) # estándar lento est_l_st &lt;- system.time({ players &lt;- unique(Batting$playerID) n_players &lt;- length(players) total &lt;- rep(NA, n_players) n &lt;- rep(NA, n_players) for (i in 1:n_players) { sub_Batting &lt;- Batting[Batting$playerID == players[i], ] total[i] &lt;- sum(sub_Batting$R, na.rm = TRUE) n[i] &lt;- nrow(sub_Batting) } Batting_2 &lt;- data.frame(playerID = players, total = total, n = n) Batting_2[order(Batting_2$total, decreasing = TRUE), ] }) # estándar rápido est_r_st &lt;- system.time({ Batting_2 &lt;- aggregate(. ~ playerID, data = Batting[, c(&quot;playerID&quot;, &quot;R&quot;)], sum) Batting_ord &lt;- Batting_2[order(Batting_2$R, decreasing = TRUE), ] }) dplyr_st #&gt; user system elapsed #&gt; 0.109 0.000 0.110 plyr_st #&gt; user system elapsed #&gt; 3.965 0.008 3.973 est_l_st #&gt; user system elapsed #&gt; 59.928 1.708 61.639 est_r_st #&gt; user system elapsed #&gt; 0.382 0.008 0.390 La función system.time supone que sabes donde buscar, es decir, que sabes que expresiones debes evaluar, una función que puede ser más útil cuando uno desconoce cuál es la función que alenta un programa es profvis() del paquete con el mismo nombre. library(profvis) Batting_recent &lt;- filter(Batting, yearID &gt; 2006) profvis({ players &lt;- unique(Batting_recent$playerID) n_players &lt;- length(players) total &lt;- rep(NA, n_players) n &lt;- rep(NA, n_players) for (i in 1:n_players) { sub_Batting &lt;- Batting_recent[Batting_recent$playerID == players[i], ] total[i] &lt;- sum(sub_Batting$R, na.rm = TRUE) n[i] &lt;- nrow(sub_Batting) } Batting_2 &lt;- data.frame(playerID = players, total = total, n = n) Batting_2[order(Batting_2$total, decreasing = TRUE), ] }) profvis() utiliza a su vez la función Rprof() de R base, este es un perfilador de muestreo que registra cambios en la pila de funciones, funciona tomando muestras a intervalos regulares y tabula cuánto tiempo se lleva en cada función. Estrategias para mejorar desempeño Algunas estrategias para mejorar desempeño: Utilizar apropiadamente funciones de R, o funciones de paquetes que muchas veces están mejor escritas de lo que nosotros podríamos hacer. Hacer lo menos posible. Usar funciones vectorizadas en R (casi siempre). No hacer crecer objetos (es preferible definir su tamaño antes de operar en ellos). Paralelizar. La más simple y muchas veces la más barata: conseguir una máquina más grande (por ejemplo Amazon web services). A continuación revisamos y ejemplificamos los puntos anteriores, los ejemplos de código se tomaron del taller EfficientR, impartido por Martin Morgan. Utilizar apropiadamente funciones de R Si el cuello de botella es la función de un paquete vale la pena buscar alternativas, CRAN task views es un buen lugar para buscar. Hacer lo menos posible Utiliza funciones más específicas, por ejemplo: rowSums(), colSums(), rowMeans() y colMeans() son más rápidas que las invocaciones equivalentes de apply(). Si quieres checar si un vector contiene un valor any(x == 10) es más veloz que 10 %in% x, esto es porque examinar igualdad es más sencillo que examinar inclusión en un conjunto. Este conocimiento requiere que conozcas alternativas, para ello debes construir tu vocabulario, puedes comenzar por lo básico e ir incrementando conforme lees código. Otro caso es cuando las funciones son más rápidas cunado les das más información del problema, por ejemplo: read.csv(), especificar las clases de las columnas con colClasses. factor() especifica los niveles con el argumento levels. Usar funciones vectorizadas en R Es común escuchar que en R vectorizar es conveniente, el enfoque vectorizado va más allá que evitar ciclos for: Pensar en objetos, en lugar de enfocarse en las componentes de un vector, se piensa únicamente en el vector completo. Los ciclos en las funciones vectorizadas de R están escritos en C, lo que los hace más veloces. Las funciones vectorizadas programadas en R pueden mejorar la interfaz de una función pero no necesariamente mejorar el desempeño. Usar vectorización para desempeño implica encontrar funciones de R implementadas en C. Al igual que en el punto anterior, vectorizar requiere encontrar las funciones apropiadas, algunos ejemplos incluyen: _rowSums(), colSums(), rowMeans() y colMeans(). Ejemplo: iteración (for, lapply(), sapply(), vapply(), mapply(), apply(), …) en un vector de n elementos llama a R base n veces compute_pi0 &lt;- function(m) { s = 0 sign = 1 for (n in 0:m) { s = s + sign / (2 * n + 1) sign = -sign } 4 * s } compute_pi1 &lt;- function(m) { even &lt;- seq(0, m, by = 2) odd &lt;- seq(1, m, by = 2) s &lt;- sum(1 / (2 * even + 1)) - sum(1 / (2 * odd + 1)) 4 * s } m &lt;- 1e6 Utilizamos el paquete microbenchmark para medir tiempos varias veces. library(microbenchmark) m &lt;- 1e4 result &lt;- microbenchmark( compute_pi0(m), compute_pi0(m * 10), compute_pi0(m * 100), compute_pi1(m), compute_pi1(m * 10), compute_pi1(m * 100), compute_pi1(m * 1000), times = 20 ) result #&gt; Unit: microseconds #&gt; expr min lq mean median #&gt; compute_pi0(m) 783.172 792.7335 806.1465 802.1365 #&gt; compute_pi0(m * 10) 7856.850 7907.9525 7975.5348 7965.0105 #&gt; compute_pi0(m * 100) 78957.173 79222.5280 79777.0224 79448.0805 #&gt; compute_pi1(m) 152.825 189.9740 240.8507 260.3435 #&gt; compute_pi1(m * 10) 1273.590 1337.6375 1787.0261 1392.3180 #&gt; compute_pi1(m * 100) 12426.282 12755.1545 23192.9735 18997.3795 #&gt; compute_pi1(m * 1000) 220932.045 357147.3230 351538.6745 359446.6780 #&gt; uq max neval #&gt; 812.1105 869.290 20 #&gt; 8013.3715 8289.812 20 #&gt; 79803.6695 85650.761 20 #&gt; 288.9500 306.516 20 #&gt; 1430.1280 9563.743 20 #&gt; 23498.2080 121950.894 20 #&gt; 365329.4820 488520.759 20 Evitar copias Otro aspecto importante es que generalmente conviene asignar objetos en lugar de hacerlos crecer (es más eficiente asignar toda la memoria necesaria antes del cálculo que asignarla sucesivamente). Esto es porque cuando se usan instrucciones para crear un objeto más grande (e.g. append(), cbind(), c(), rbind()) R debe primero asignar espacio a un nuevo objeto y luego copiar al nuevo lugar. Para leer más sobre esto Burns (2015) es una buena referencia. Ejemplo: crecer un vector puede causar que R copie de manera repetida el vector chico en el nuevo vector, aumentando el tiempo de ejecución. Solución: crear vector de tamaño final y llenarlo con valores. Las funciones como lapply() y map hacen esto de manera automática y son más sencillas que los ciclos for. memory_copy1 &lt;- function(n) { result &lt;- numeric() for (i in seq_len(n)) result &lt;- c(result, 1/i) result } memory_copy2 &lt;- function(n) { result &lt;- numeric() for (i in seq_len(n)) result[i] &lt;- 1 / i result } pre_allocate1 &lt;- function(n) { result &lt;- numeric(n) for (i in seq_len(n)) result[i] &lt;- 1 / i result } pre_allocate2 &lt;- function(n) { vapply(seq_len(n), function(i) 1 / i, numeric(1)) } vectorized &lt;- function(n) { 1 / seq_len(n) } n &lt;- 10000 microbenchmark( memory_copy1(n), memory_copy2(n), pre_allocate1(n), pre_allocate2(n), vectorized(n), times = 10, unit = &quot;relative&quot; ) #&gt; Unit: relative #&gt; expr min lq mean median uq #&gt; memory_copy1(n) 5796.10927 5368.12443 609.400534 4182.21951 3084.91090 #&gt; memory_copy2(n) 99.77897 94.69146 12.069795 74.99731 55.93357 #&gt; pre_allocate1(n) 22.10778 20.82364 4.094677 16.48949 11.99598 #&gt; pre_allocate2(n) 214.99549 200.51464 24.036098 159.15682 120.67895 #&gt; vectorized(n) 1.00000 1.00000 1.000000 1.00000 1.00000 #&gt; max neval #&gt; 90.651773 10 #&gt; 3.000340 10 #&gt; 2.333233 10 #&gt; 4.170577 10 #&gt; 1.000000 10 Un caso común donde se hacen copias sin necesidad es al trabajar con data.frames. Ejemplo: actualizar un data.frame copia el data.frame completo. Solución: operar en vectores y actualiza el data.frame al final. n &lt;- 1e4 df &lt;- data.frame(Index = 1:n, A = seq(10, by = 1, length.out = n)) f1 &lt;- function(df) { ## constants cost1 &lt;- 3 cost2 &lt;- 0.05 cost3 &lt;- 50 ## update data.frame -- copies entire data frame each time! df$S[1] &lt;- cost1 for (j in 2:(n)) df$S[j] &lt;- df$S[j - 1] - cost3 + df$S[j - 1] * cost2 / 12 ## return result df } .f2helper &lt;- function(cost1, cost2, cost3, n) { ## create the result vector separately cost2 &lt;- cost2 / 12 # &#39;hoist&#39; common operations result &lt;- numeric(n) result[1] &lt;- cost1 for (j in 2:(n)) result[j] &lt;- (1 + cost2) * result[j - 1] - cost3 result } f2 &lt;- function(df) { cost1 &lt;- 3 cost2 &lt;- 0.05 cost3 &lt;- 50 ## update the data.frame once df$S &lt;- .f2helper(cost1, cost2, cost3, n) df } microbenchmark( f1(df), f2(df), times = 5, unit = &quot;relative&quot; ) #&gt; Unit: relative #&gt; expr min lq mean median uq max neval #&gt; f1(df) 252.27 248.4018 82.39169 231.2569 69.34723 29.61848 5 #&gt; f2(df) 1.00 1.0000 1.00000 1.0000 1.00000 1.00000 5 Paralelizar Paralelizar usa varios cores para trabajar de manera simultánea en varias secciones de un problema, no reduce el tiempo computacional pero incrementa el tiempo del usuario pues aprovecha los recursos. Como referencia está [Parallel Computing for Data Science] de Norm Matloff. Referencias "],
["lecturas-y-recursos-recomendados-de-r.html", "4.5 Lecturas y recursos recomendados de R", " 4.5 Lecturas y recursos recomendados de R Algunas recomendaciones para mejorar el flujo de trabajo y aprender nuevas herramientas de R son: What they forgot to teach you about R de Jenny Bryan y Jim Hester. Flujos de trabajo basados en proyectos que facilitan el trabajo del analista. Good enough practices in scientific computing, Greg Wilson, Jennifer Bryan, et al. Happy Git with R, Jenny Bryan y Jim Hester. R Markdown: The Definitive Guide, Yihui Xie, J. J. Allaire, Garrett Grolemund. "],
["introducción-a-cálculo-de-probabilidades.html", "Sección 5 Introducción a Cálculo de Probabilidades", " Sección 5 Introducción a Cálculo de Probabilidades “Probabilidad es el lenguaje matemático para cuantificar incertidumbre.” -Wasserman En esta parte hacemos un repaso de conceptos de probabilidad con un enfoque computacional: Modelos probabilísticos: espacio de resultados, eventos, funciones de probabilidad. Interpretación frecuentista de probabilidad. Variables aleatorias. Las referencias para esta sección son Pitman (1992), Ross (1998) y Wasserman (2010). Referencias "],
["probabilidad-como-extensión-de-proporción.html", "5.1 Probabilidad como extensión de proporción", " 5.1 Probabilidad como extensión de proporción Históricamente las primeras ideas probabilísticas ocurrieron en el contexto de juegos de azar, y la consideración si una apuesta es “justa” o no. El concepto original fue formulado quizá por primera vez (Cardano) de la siguiente forma: Las apuestas en un juego de azar deben ser en proporción al número de maneras en que un jugador puede ganar. Por ejemplo, supongamos que yo apuesto a que una tirada de dado va salir un 1 o 2. Mi contrincante gana si sale 3, 4, 5, 6. Como hay el doble de resultados desfavorables para mi, el juego es justo si yo apuesto 10 pesos y mi contrincante 20. Implícitamente, esta regla razonable introduce el concepto de probabilidad o “verosimilitud” de un evento aleatorio. Las bases para la formalización de esta idea son las siguientes: Los resultados del experimento (o el juego de azar) son simétricos: nada los distingue excepto la etiqueta (por ejemplo el 1 y el 2 en el dado). En este caso decimos que estos resultados son “equiprobables”. La “probabilidad” de un conjunto de resultados es proporcional al tamaño del conjunto de resultados (1 y 2 son dos posibles resultados de 6). Ejemplos En el ejemplo del dado, no podríamos definir los resultados como “Tiro 1 o 2” o “No tiro 1 o 2”, porque no hay simetría entre los dos resultados: el dado tiene cuatro caras que corresponden al resultado “No tiro 1 o 2” y solo dos para “Tiro 1 o 2” Si tenemos 100 pelotas idénticas en una bolsa, las revolvemos bien, y sacamos sin ver una pelota, en el experimento aleatorio no hay nada que distinga una pelota de otra, así que tiene sentido el modelo equiprobable: todas las pelotas tienen la misma probabilidad de salir. Sin embargo, si hay unas pelotas más pesadas que otra, no revolvemos bien, etc. el experimento pierde la simetría y el modelo equiprobable puede nos ser apropiado. En esta familia de modelos, la probabilidad se ve como una extensión de la idea de proporción, o cociente de una parte con respecto a un todo. Este es uno de los modelos de probabilidad más fundamentales. Podemos definir algunos conceptos para tener una teoría matemática para este tipo de modelos. Espacio de resultados y eventos El espacio de resultados \\(\\Omega\\) es el conjunto de posibles resultados de un experimento aleatorio. A los puntos \\(\\omega \\in \\Omega\\) se les conoce como resultados muestrales, o realizaciones del experimento aleatorio. Ejemplo: Si lanzamos una moneda dos veces entonces el espacio de resultados es: \\[\\Omega = \\{AA, AS, SA, SS \\}\\] Un evento es un subconjunto del espacio muestral. Los eventos usualmente se denotan por letras mayúsculas. El evento: que la primer lanzamiento resulte águila es el evento \\[A=\\{AA, AS\\}\\] Espacios de probabilidad equiprobables Espacios equiprobables. Si todos los elementos en el espacio de resultados tienen la misma oportunidad de ser elegidos entonces la probabilidad del evento A es el número de resultados en A dividido entre el número total de posibles resultados: \\[P(A)=\\frac{\\#(A)}{\\#(\\Omega)},\\] de modo que calcular probabilidades se reduce a un ejercicio de conteo. Por ejemplo, la probabilidad de obtener \\(AA\\) si lanzamos una moneda dos veces es \\(1/4 = 0.25\\), y la probabilidad del evento que la primer lanzamiento resulte águila es \\(2/4 = 0.5\\). Lanzamos un dado y anotamos el número de la cara superior, después lanzamos otro dado y anotamos el número de la cara superior. ¿Cuál es el espacio de resultados? ¿Cuál es la probabilidad de que la suma de los números sea 5? ¿Cuál es la probabilidad de que el segundo número sea mayor que el primero? Repite las preguntas anteriores cuando lanzas 2 dados con \\(n\\) caras (\\(n \\ge 4\\)). Observaciones: Como explicamos arriba, este tipo de modelos es apropiado cuando podemos escribir el experimento aleatorio de forma que los resultados son equiprobables (existe simetría de los resultados). También podemos pensar en que el fundamento de estos modelos es el principio de razón insuficiente: si no hay nada que distinga los resultados del experimento, ningún modelo o cálculo que hagamos debe distingir entre los resultados. Ejemplo Supongamos que sacamos tres cartas de una baraja. ¿Cuál es la probabilidad de una cuarta carta que saquemos es un as? Utiiza el principio de razón insuficiente. Para problemas más complejos, podemos utilizar técnicas de conteo más avanzadas. Ejemplo: combinaciones Un comité de 5 personas será seleccionado de un grupo de 6 hombres y 9 mujeres. Si la selección es aleatoria, ¿cuál es la probabilidad de que el comité este conformado por 3 hombres y 2 mujeres? Hay \\(\\dbinom{15}{5}\\) posibles comités, cada uno tiene la misma posibilidad de ser seleccionado. Por otra parte, hay \\(\\dbinom{6}{3} \\dbinom{9}{2}\\) posibles comités que incluyen 3 hombres y 2 mujeres, por lo tanto, la probabilidad que buscamos es: \\[\\frac{\\dbinom{6}{3} \\dbinom{9}{2}}{\\dbinom{15}{5}} \\] y la función para calcular combinaciones en R es choose(n, r) choose(6, 3) * choose(9, 2) / choose(15, 5) #&gt; [1] 0.2397602 Los solución a problemas derivados de juegos de azar se complica rápidamente y suele ser necesario conocer técnicas de conteo para resolverlos. "],
["interpretación-frecuentista-de-probabilidad.html", "5.2 Interpretación frecuentista de probabilidad", " 5.2 Interpretación frecuentista de probabilidad La interpretación de la probabilidad como extensión a la idea de proporción es fundamental, pero deja sin responder una pregunta crítica: ¿Cómo sabemos que un modelo probabilístico de este tipo describe apropiadamente la realidad? ¿Qué pruebas empíricas podemos buscar que de soporte a tal modelo? El concepto subyacente que tardó más en formalizarse es el siguiente. En términos de juegos justos: A partir de una sola repetición del juego, no podemos determinar si un juego es justo o no. Sin embargo, si repetimos una gran cantidad de veces el juego justo, nuestras pérdidas y ganancias deberían equilibrarse, y con seguridad nuestro balance será cercano a 0. Ejemplo Según el concepto de juego justo, si mi probabilidad de ganar es \\(p\\), es justo que apueste una cantidad \\(Ap\\) y mi contrincante una cantidad \\(A(1-p)\\). Después de \\(n\\) repeticiones de este juego, supongamos que gané un número de veces \\(m\\). Entonces, tengo \\(A(1-p)m\\) pesos de las veces que gané, y perdí \\(Ap(n-m)\\) pesos por las veces que perdí. En total, mi neto de ganancias es \\(A(1-p)m - Ap(n-m)\\). Si esta cantidad es cercana a cero \\[A(1-p)m - Ap(n-m) \\approx 0,\\] entonces despejando obtenemos que \\[p \\approx \\frac{m}{n},\\] Es decir, aproximadamente tengo que ganar una fracción \\(m/n\\) de veces. Esta cantidad es una frecuencia relativa de ocurrencia. Una frecuencia relativa es una proporción que mide que tan seguido, o frecuente, ocurre una u otra cosa en una sucesión de observaciones. Pensemos en un experimento que se pueda repetir, por ejemplo, lanzar una moneda, lanzar un dado, el nacimiento de un bebé. Llamaremos ensayo a una repetición del experimento. Ahora, sea A un posible resultado del evento (obtener sol, obtener un 6, el bebé es niña), si A ocurre \\(m\\) veces en \\(n\\) ensayos, entonces la frecuencia relativa de A en \\(n\\) ensayos es \\(m/n\\). Ejemplo Supongamos que lanzamos una moneda 10 veces y obtenemos los siguientes resultados: set.seed(2881) lanzamientos_10 &lt;- sample(c(&quot;A&quot;, &quot;S&quot;), 10, replace = TRUE) lanzamientos_10 #&gt; [1] &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; &quot;S&quot; &quot;A&quot; &quot;A&quot; &quot;A&quot; Podemos calcular las secuencia de frecuencias relativas de águila: cumsum(lanzamientos_10 == &quot;A&quot;) # suma acumulada de águilas #&gt; [1] 1 2 3 4 5 6 6 7 8 9 cumsum(lanzamientos_10 == &quot;A&quot;) / 1:10 #&gt; [1] 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 1.0000000 0.8571429 #&gt; [8] 0.8750000 0.8888889 0.9000000 Resultados empíricos acerca de frecuencias relativas Una primera observación empírica es que las frecuencias relativas basadas en un número mayor de observaciones son menos fluctuantes comparado con las frecuencias relativas basadas en pocas observaciones. Este fenómeno se conoce a veces como la ley empírica de los promedios (que se formaliza después en las leyes de los grandes números): n &lt;- 1000 tibble(num_lanzamiento = 1:n, lanzamiento = sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE)) %&gt;% mutate(frec_rel = cummean(lanzamiento == &quot;A&quot;)) %&gt;% ggplot(aes(x = num_lanzamiento, y = frec_rel)) + geom_hline(yintercept = 0.5, color = &quot;red&quot;, alpha = 0.5) + geom_line(color = &quot;darkgray&quot;) + geom_point(size = 1.0) + labs(y = &quot;frecuencia relativa&quot;, title = &quot;1000 volados&quot;, x = &quot;lanzamiento&quot;) Veamos las frecuencias relativas para 3 series de 1000 lanzamientos. lanzar &lt;- function(n = 1000){ tibble(num_lanzamiento = 1:n, lanzamiento = sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE)) %&gt;% mutate(frec_rel = cummean(lanzamiento == &quot;A&quot;)) } head(lanzar()) #&gt; # A tibble: 6 x 3 #&gt; num_lanzamiento lanzamiento frec_rel #&gt; &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 1 A 1 #&gt; 2 2 S 0.5 #&gt; 3 3 A 0.667 #&gt; 4 4 S 0.5 #&gt; 5 5 S 0.4 #&gt; 6 6 S 0.333 set.seed(31287931) # usamos la función map_df del paquete purrr map_df(1:3, ~lanzar(), .id = &quot;serie&quot;) %&gt;% ggplot(aes(x = log(num_lanzamiento), y = frec_rel, color = as.character(serie))) + geom_hline(yintercept = 0.5, color = &quot;darkgray&quot;) + geom_line() + scale_x_continuous(&quot;lanzamiento&quot;, labels = exp, breaks = log(sapply(0:10, function(i) 2 ^ i))) + labs(color = &quot;serie&quot;, y = &quot;frecuencia relativa&quot;, title = &quot;1000 volados&quot;) La segunda observación empírica es que, cuando el modelo de resultados equiprobables es apropiado: En el modelo de resultados equiprobables, la frecuencia relativa a largo plazo de ocurrencia de un evento es su probabilidad \\(P(A)\\) de ocurrencia En la interpretación frecuentista, la probabilidad de un evento \\(A\\) es la estimación de la frecuencia relativa de \\(A\\) cuando el número de ensayos tiende a infinito. Si denotemos la proporción de veces que ocurre \\(A\\) en \\(n\\) ensayos por \\(P_n(A)\\), se espera que \\(P_n(A)\\) sea cercana a la probabilidad \\(P(A)\\) si \\(n\\) es grande: \\[P_n(A) \\approx P(A)\\] Esta interpretación es crucial, pues es la única que realmente nos permite conectar nuestros modelos probabilísticos con observaciones empíricas. Ejemplo: Lanzamiento de dos monedas Supongamos que lanzamos dos monedas de manera simultánea. ¿Cuál es la probabilidad de que las dos monedas sean águila? Las dos son águila o no, así que la posibilidad es 1/2. Si definimos el resultado como el número de caras que se leen en las monedas, puede haber 0, 1 o 2. Si suponemos que estos tres resultados son igualmente probables, entonces la posibilidad es 1/3. A pesar de que las monedas son similares supongamos que se pueden distinguir, llamémoslas moneda 1 y moneda 2. Ahora tenemos cuatro posibles resultados: AA, AS, SA, SS, (la primer letra corresponde a la cara observada en la moneda 1 y la segunda en la moneda 2). Si estos 4 resultados son igualmente probables entonces el evento AA tiene posibilidad de 1/4. ¿Cuál es la respuesta correcta? En cuanto a teoría formal todas son correctas, cada escenario tiene supuestos de resultados equiprobables claramente enunciados y en base a éstos determina una probabilidad de manera correcta; sin embargo, los supuestos son diferentes y por tanto también las conclusiones. Únicamente una de las soluciones puede ser consistente con la interpretación frecuentista, ¿cuál es? La primer respuesta es incorrecta pues supone probabilidad cero para el evento águila y sol. La solución dos, por otra parte, no es fácil de desacreditar, así que realicemos el experimento para encontrar la respuesta: n &lt;- 10000 moneda_1 &lt;- sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE) moneda_2 &lt;- sample(c(&quot;A&quot;, &quot;S&quot;), n, replace = TRUE) sum(moneda_1 == moneda_2 &amp; moneda_1 == &quot;A&quot;) / n #&gt; [1] 0.2482 La respuesta 3 es la correcta, y lo que vemos es que incluso cuando el supuesto de igualmente probables es apropiado a un cierto nivel de descripción determinado, este nivel no es algo que se pueda juzgar usando únicamente matemáticas, sino que se debe juzgar usando una interpretación de la probabilidad, como frecuencias relativas en ensayos. Más aún, hay ejemplos donde las monedas no son justas, o el sexo de un bebé recién nacido, donde el supuesto de equiprobabilidad no es adecuado. "],
["simulación-para-el-cálculo-de-probabilidades.html", "5.3 Simulación para el cálculo de probabilidades", " 5.3 Simulación para el cálculo de probabilidades En el ejemplo anterior vimos que puede ser sencillo usar simulación para calcular probabilidades, pues usando la interpretación de frecuencia relativa simplemente hace falta simular el experimento y contar los casos favorables entre el total de casos. Simulación para el cálculo de probabilidades Definir el modelo probabilístico: Definir el espacio de resultados. Describir el mecanismo que genera los resultados, esto incluye entender los pasos que involucran azar y los que no. Simular: Replicar el experimento con código, siguiendo el conocimiento elicitado en 1 y 2. Repetir el paso 3 \\(n\\) veces y calcular la frecuencia relativa de éxitos, estimando así la probabilidad. Para el paso 2 (resultados equiprobables), en R suelen ser de utilidad las funciones runif y sample(), revisa la ayuda de estas funciones. Ejemplo: comité Un comité de 5 personas será seleccionado de un grupo de 6 hombres y 9 mujeres. Si la selección es aleatoria, ¿cuál es la probabilidad de que el comité este conformado por 3 hombres y 2 mujeres? El espacio de resultados es \\(\\Omega = \\{M_1M_2M_3M_4M_5, M_2M_3M_4M_5M_6,... H_1,H_2H_3H_4H_5,H_2H_3H_4H_5H_6\\}\\). Se seleccionan 5 integrantes al azar del conjunto de hombres y mujeres, es claro que cada persona solo puee estar una vez. candidatos &lt;- c(paste(&quot;M&quot;, 1:9, sep = &quot;_&quot;), paste(&quot;H&quot;, 1:6, sep = &quot;_&quot;)) sample(candidatos, 5, replace = FALSE) #&gt; [1] &quot;M_9&quot; &quot;M_2&quot; &quot;M_8&quot; &quot;M_3&quot; &quot;M_6&quot; comite &lt;- function(){ candidatos &lt;- c(paste(&quot;M&quot;, 1:9, sep = &quot;_&quot;), paste(&quot;H&quot;, 1:6, sep = &quot;_&quot;)) comite &lt;- sample(candidatos, 5, replace = FALSE) n_mujeres &lt;- sum(substr(comite, 1, 1) == &quot;M&quot;) n_mujeres == 2 } rerun(1000, comite()) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.223 Ejemplo: La ruina del jugador Un jugador tiene $100, y va a apostar en un juego donde la probabilidad de ganar es p = 0.47 (e.g. una ruleta 18/38), si gana recibe el doble de lo que arriesgó, si no gana pierde todo lo que apostó. Cada vez que juega puede apostar cualquier cantidad siempre y cuando aún cuente con dinero. El jugador dejará de jugar cuando su capital sea $0 o cuando gane $200. El jugador busca una estrategia que le ayude a aumentar su probabilidad de ganar y te pregunta: ¿Cuál es la probabilidad de ganar si apuesto en incrementos de $5 cada vez que apuesto? Siguiendo los pasos enunciados: Cada elemento del espacio de resultados es una sucesión \\(\\omega_1, \\omega_2, \\omega_3, \\ldots\\) donde cada \\(\\omega_i\\) es \\(G\\) o \\(P\\) (gana o pierde). El jugador juega mientras tenga capital y este sea menor a $200, el monto de la apuesta está fijo en $5, no importa el capital en cada momento. La componente aleatoria involucra si gana cada uno de los juegos y esto ocurre con probabilidad 0.47. apostar &lt;- function(dinero = 100, apuesta = 5, tope = 200) { while (0 &lt; dinero &amp; dinero &lt; tope) { if (sample(1:38, 1) &lt;= 18) { dinero &lt;- dinero + apuesta } else { dinero &lt;- dinero - apuesta } } dinero &gt; 0 } set.seed(9923) n_juegos &lt;- 5000 juegos &lt;- rerun(n_juegos, apostar()) %&gt;% flatten_dbl() mean(juegos) #&gt; [1] 0.1126 # incrementos de 50? juegos &lt;- rerun(n_juegos, apostar(apuesta = 50)) %&gt;% flatten_dbl() mean(juegos) #&gt; [1] 0.454 La solución analítica la pueden leer en este documento de caminatas aleatorias: p &lt;- 0.474 1 - (1 - (p / (1 - p)) ^ (100 / 5)) / (1 - (p / (1 - p)) ^ (200 / 5)) # apostando de 5 en 5 #&gt; [1] 0.1108707 1 - (1 - (p / (1 - p)) ^ (100 / 50)) / (1 - (p / (1 - p)) ^ (200 / 50)) # apostando de 50 en 50 #&gt; [1] 0.4481402 Cumpleaños. ¿Cuántas personas debe haber en un salón para que la probabilidad de encontrar 2 con el mismo cumpleaños sea mayor a 0.5? Supuestos: Mismo cumpleaños implica mismo día y mes. No hay años bisiestos. La probabilidad de que alguien nazca un día dado es la misma para todos los días del año. Chabelo (Monty Hall) Supongamos que estamos jugando las catafixias de Chabelo, en este juego hay 3 catafixias: 2 de ellas están vacías y una tiene un premio: El juego comienza cuando escoges una catafixia. A continuación Chabelo abre una catafixia vacía de las dos catafixias restantes. Tu eliges si te mantienes con tu catafixia o cambias a la otra que continúa cerrada. Chabelo abre tu segunda elección de catafixia y se revela si ganaste. ¿Cuál es la probabilidad de que ganes si cambias de catafixia? Urna: 10 personas (con nombres distintos) escriben sus nombres y los ponen en una urna, después seleccionan un nombre (al azar). Sea A el evento en el que ninguna persona selecciona su nombre, ¿Cuál es la probabilidad del evento A? Supongamos que hay 3 personas con el mismo nombre, ¿Cómo calcularías la probabilidad del evento A en este nuevo experimento? El señor J. tiene 2 cachorros, el mayor es hembra. ¿Cuál es la probabilidad de que los dos sean hembra? La señora K. tiene 2 cachorros, al menos uno es macho. ¿Cuál es la probabilidad de que los dos sean macho? Espacios equiprobables continuos Podemos generalizar las definiciones de equiprobable al caso continuo, como ejemplo supongamos que se lanza un dardo a un tablero cuadrandgular de lado 2, ¿cuál es la probabilidad de que el dardo caiga en el círculo de radio 1 inscrito en un cuadrado de lado 2? tablero &lt;- ggplot() + ggforce::geom_circle(aes(x0 = 0, y0 = 0, r = 1)) + geom_rect(aes(xmin = -1, xmax = 1, ymin = -1, ymax = 1), fill = &quot;white&quot;, color = &quot;black&quot;, alpha = 0.5) + coord_equal() ggsave(&quot;img/tablero.png&quot;, tablero, width = 3, height = 3) knitr::include_graphics(&quot;img/tablero.png&quot;) En este caso usamos áreas relativas para calcular la probabilidad: denotemos C al evento tal que el dardo cae en el círculo, entonces: \\[P(B) = \\frac{Área(B)}{Área(\\Omega)}\\] ¿Y simulando? dentro_circunferencia &lt;- function(){ x &lt;- runif(1) * sample(c(-1, 1), 1) y &lt;- runif(1) * sample(c(-1, 1), 1) sqrt(x ^ 2 + y ^ 2) &lt; 1 } rerun(10000, dentro_circunferencia()) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.787 dardos &lt;- tibble(x = runif(1000, -1, 1), y = runif(1000, -1, 1), en_circulo = sqrt(x ^ 2 + y ^ 2) &lt; 1) tablero_dardos &lt;- tablero + geom_point(data = dardos, aes(x, y, color = en_circulo), alpha = 0.5, show.legend = FALSE) ggsave(&quot;img/tablero_dardos.png&quot;, tablero_dardos, width = 3, height = 3) knitr::include_graphics(&quot;img/tablero_dardos.png&quot;) ¿Qué defectos puedes ver en este modelo probabilístico para el lanzamiento de un dardo? "],
["modelos-de-probabilidad-definición-general.html", "5.4 Modelos de probabilidad (definición general)", " 5.4 Modelos de probabilidad (definición general) En muchos casos, el modelo equiprobable (discreto o continuo) no es tan fácil adaptar a problemas particulares de modelación probabilística. Por ejemplo: En el problema del dardo, como la mayor parte de la gente apunta hacia el centro, regiones centrales deberían tener probabilidad mayor que regiones cerca del borde. Si sacamos una pelota al azar de una bolsa revuelta, y las pelotas son de distintos tamaños, entonces las probabilidades de extraer cada pelota son diferentes. Modelos apropiados para estos experimentos son más complicados, pues no tenemos la simetría que nos da asignaciones automáticas de probabilidades. 5.4.1 Espacios discretos En el caso discreto, podríamos poner: Si los resultados posibles son \\(\\omega_1,\\ldots, \\omega_n\\), asignamos probabilidades \\(p_1\\ldots p_n\\) a cada resultado (pueden ser distintas), La probabilidad de un evento \\(A\\) es la suma de las probabilidades \\(p_i\\) de los elementos que están contenidos en \\(A\\). 5.4.1.1 Ejemplo: bolsa acumulada de lotería Imaginemos que tenemos una lotería tipo Melate. Para simplificar, pensemos que se escogen 3 números del 1 al 10, y que tenemos 200 concursantes. ¿Cuál es la probabilidad de la bolsa de este concurso se acumule (nadie tenga la combinación ganadora)? Podemos resolver este problema con conteo, pero veamos como hacerlo simulando. seleccionar_combinacion &lt;- function(){ sample(1:10, 3) %&gt;% sort } acumula_bolsa &lt;- function(n_concursantes = 200){ combinacion_ganadora &lt;- sample(1:10, 3) %&gt;% sort boletos &lt;- rerun(n_concursantes, seleccionar_combinacion()) map_lgl(boletos, ~ all(. == combinacion_ganadora)) %&gt;% any } # probabilidad de que alguien gane set.seed(99244) prob_ganador &lt;- rerun(500, acumula_bolsa()) %&gt;% flatten_lgl %&gt;% mean # probabilidad de acumulación 1 - prob_ganador #&gt; [1] 0.206 ¿Qué problema ves con este ejemplo? En realidad, la elección de los concursantes no es aleatoria. Existen números favoritos (por ejemplo, el 7 o el 13), patrones que atraen a concursantes (patrones aritméticos, formas en el boleto de melate, selección de números según fechas de cumpleaños, etc.) ¿Qué tanto puede afectar esta elección consciente? Intenamos una variación simple: supongamos que la probabilidad de escoger el número 7 es más alta que otros números. seleccionar_combinacion &lt;- function(prob = rep(1, 10)){ sample(1:10, 3, prob = prob) %&gt;% sort } # el 7 es 5 veces más probable: p_numeros &lt;- c(rep(1,6), 5, rep(1,3)) acumula_bolsa &lt;- function(n_concursantes = 200, prob = rep(1, 10)){ combinacion_ganadora &lt;- sample(1:10, 3) %&gt;% sort boletos &lt;- rerun(n_concursantes, seleccionar_combinacion(prob = prob)) map_lgl(boletos, ~ all(. == combinacion_ganadora)) %&gt;% any } # probabilidad de que alguien gane prob_ganador &lt;- rerun(500, acumula_bolsa(prob = p_numeros)) %&gt;% flatten_lgl %&gt;% mean # probabilidad de acumulación 1 - prob_ganador #&gt; [1] 0.408 5.4.1.2 Ejemplo: comité Supongamos que el proceso de selección del comité tiene sesgo, las mujeres se seleccionan con mayor probabilidad que los hombres: comite &lt;- function(){ candidatos &lt;- c(paste(&quot;M&quot;, 1:9, sep = &quot;_&quot;), paste(&quot;H&quot;, 1:6, sep = &quot;_&quot;)) comite &lt;- sample(candidatos, 5, replace = FALSE, prob = c(rep(2, 9), rep(1, 6))) n_mujeres &lt;- sum(substr(comite, 1, 1) == &quot;M&quot;) n_mujeres == 2 } rerun(1000, comite()) %&gt;% flatten_dbl() %&gt;% mean() #&gt; [1] 0.086 5.4.2 Espacios continuos En el caso de espacio de resultados continuos, también quisiéramos tener un concepto de resultados no equiprobables. Por ejemplo, para el dardo en el tablero, es más realista pensar que la probabilidad de que el dardo caiga en un segmento de la zona central no es la misma a que caiga en un segmento de igual área en las orillas. tablero_zonas &lt;- tablero + geom_rect(aes(xmin = -1, xmax = -0.8, ymin = -1, ymax = -0.8), fill = &quot;red&quot;, alpha = 0.5) + geom_rect(aes(xmin = -.1, xmax = 0.1, ymin = -0.1, ymax = 0.1), fill = &quot;red&quot;, alpha = 0.5) ggsave(&quot;img/tablero_zonas.png&quot;, tablero_zonas, width = 3, height = 3) knitr::include_graphics(&quot;img/tablero_zonas.png&quot;) La definición de probabilidad como área relativa no se puede usar en estos casos, sin embargo, el enfoque de simulación se continúa manteniendo. Comencemos con el caso de un dardo univariado. En este caso, la probabilidad se calcula como longitud relativa. La probabilidad de que el dardo caiga en el intervalo \\([a,b]\\subset [0,1]\\) es \\[P([a,b]) = \\frac{b-a}{1-0} = b-a\\] Y nótese ahora que podemos escribir este cálculo como la integral de una constante: \\[P([a, b]) = \\frac{b-a}{1} = \\int_a^b 1dx\\] donde clarametne \\(P([0,1]) = 1.\\) Ahora, si el dardo cae en ciertas zonas con mayor probabilidad podemos perturbar la función que integramos para asignar mayor probabilidad en zonas donde es más probable que el dardo caiga. Esto implica introducir una función \\(f\\), que llamamos densidad de probabilidad, cuya integral nos de valores de probabilidad: ggplot(tibble(x = c(0 , 1)), aes(x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2)) + geom_rect(data = NULL, aes(xmin = 0, xmax = 1, ymin = 0, ymax = 1), fill = &quot;red&quot;, alpha = 0.2) \\[P([a,b])=\\int_a^bf(x)dx\\] Nótese que la integral sobre \\([0,1]\\) debe ser 1 (probabilidad de que el dardo caiga entre 0 y 1). Podemos calcular probabilidades con simulación, por ejemplo la probabilidad de x en [0.2, 0.5]: curva &lt;- function(){ # Este método es simulación por rechazo x &lt;- runif(1) y &lt;- runif(1) * 1.5 while (dbeta(x, 2, 2) &lt; y) { x &lt;- runif(1) y &lt;- runif(1) * 2.5 } x } sims_x &lt;- rerun(5000, curva()) %&gt;% flatten_dbl() mean(sims_x &gt; 0.2 &amp; sims_x &lt; 0.5) #&gt; [1] 0.3954 Consideramos la siguiente gráfica para ayudar en la intuición del método: los puntos seleccionados por encima de la función de densidad son rechazados, de manera que es claro que la probabilidad que estimamos arriba de caer en el intervalo de interés es el área bajo la curva (integral) de nuestra función de densidad: tibble(x = runif(1000), y = runif(1000) * 1.5, dentro = dbeta(x, 2, 2) &gt; y, en_int = dentro * (x &gt; 0.2 &amp; x &lt; 0.5), cat = case_when(!dentro ~ &quot;a&quot;, dentro &amp; en_int ~ &quot;b&quot;, TRUE ~ &quot;c&quot;)) %&gt;% ggplot() + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2)) + geom_point(aes(x, y, color = cat), alpha = 0.5, show.legend = FALSE) "],
["probabilidad-definición-matemática.html", "5.5 Probabilidad: definición matemática", " 5.5 Probabilidad: definición matemática Podemos cubrir todos los casos que hemos visto hasta ahora (y otros adicionales) con la siguiente definición de probabilidad: Supongamos que tenemos un espacio de resultados \\(\\Omega\\), y que todos los eventos de interés están representados como subconjuntos de \\(\\Omega\\). Podemos pensar en \\(\\Omega\\) como una representación de todas las situaciones que pueden ocurrir, no suponemos que es finito, ni que los eventos son igualmente probables. Las reglas de la probabilidad involucran relaciones lógicas entre eventos; estas se traducen a relaciones de conjuntos. Por ejemplo, si C es el evento que ocurre si sucede A o si sucede B, entonces el conjunto de maneras en las que ocurre C es la unión del conjunto de maneras en que ocurre A y el conjunto de maneras en que ocurre B. Veamos como se traduce de eventos a conjuntos Lenguaje de eventos Lenguaje de conjuntos Notación de conjuntos Espacio de resultados conjunto universal \\(\\Omega\\) evento subconjunto de \\(\\Omega\\) \\(A,B,C,...\\) evento imposible conjunto vacío \\(\\emptyset\\) no A, opuesto de A complemento de A \\(A^c\\) A o B unión de A y B \\(A\\cup B\\) tanto A como B intersección de A y B \\(AB,A\\cap B\\) A y B mutuamente excluyentes A y B disjuntos \\(AB=\\emptyset\\) si A entonces B A es subconjunto de B \\(A\\subset B\\) Particiones y axiomas de probabilidad Decimos que un conjunto de \\(n\\) eventos \\(B_1,...,B_n\\) es una partición del evento \\(B\\) si \\(B=B_1 \\cup B_2 \\cup \\cdot\\cdot\\cdot \\cup B_n\\) y los eventos \\(B_1,...,B_n\\) son mutuamente excluyentes. Ahora podemos definir probabilidad: Una función \\(P\\) es una función de probabilidad si satisface las siguientes condiciones: Un valor de probabilidad debe ser no-negativo: \\[P(B) \\geq 0\\] para cualquier evento \\(B\\) La suma de las probabilidades a través de todos los posibles eventos en el espacio de resultados debe ser 1 (i.e. uno de los eventos en el espacio de resultados debe ocurrir). \\[P(\\Omega) = 1\\] Si \\(B_1,...,B_n\\) es una partición del evento \\(B\\) entonces, la probabilidad de que ocurra B es la suma de las probabilidades individuales: \\[P(B)=P(B_1)+P(B_2) + \\cdot\\cdot\\cdot +P(B_n)\\] Propiedades de la función de probabilidad: \\(P(A^c) = 1 - P(A)\\) \\(P(\\emptyset)=0\\) Si \\(A \\subset B\\) entonces \\(P(A) \\le P(B)\\) \\(0\\le P(A) \\le 1\\) La regla general de la suma: \\(P(A \\cup B) = P(A) + P(B) - P(A \\cap B)\\) "],
["variables-aleatorias.html", "5.6 Variables aleatorias", " 5.6 Variables aleatorias A partir de un experimento aleatorio se pueden definir muchas preguntas de probabilidad, por ejemplo, en el caso de la ruina del jugador podríamos preguntarnos: las ganancias después del tercer juego, probabilidad de ganar, duración del experimento (cuántos juegos se jugaron antes de alcanzar las reglas de término). Sin embargo, muchas veces nos centramos en estudiar un solo aspecto numérico del experimento. La variable aleatoria \\(X\\) es un mapeo entre el espacio de resultados y los números reales. Este enfoque tiene dos ventajas importantes: Simplifica muchas veces el ejercicio de modelación, pues en lugar de tener que entender y modelar la totalidad del experimento aleatorio, podemos concentrarnos en propiedades de la función \\(X\\). Usando probabilidades asociadas a la función \\(X\\) podemos resumir aspectos de interés el experimento aleatorio. El uso de variables aleatorias nos permite usar álgebra para derivar resultados y representar cómputos de manera conveniente. Ejemplo Supongamos que nuestro experimento aleatorio es tirar un dado 10 veces. Este experimento aleatorio tiene \\(10^6\\) posibles resultados. Supongamos que nos interesa principlamente \\(X =\\) número de seises que obtuvimos en le experimento. Entonces es posible demostrar (o puedes estimar con simulación), que las probabilidades asociadas a \\(X\\) podemos escribirlas como: X_prob &lt;- tibble(X = seq(0, 10)) %&gt;% mutate(prob = dbinom(X, 10, 1/6), prob_redondear = round(prob, 3)) X_prob #&gt; # A tibble: 11 x 3 #&gt; X prob prob_redondear #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.162 0.162 #&gt; 2 1 0.323 0.323 #&gt; 3 2 0.291 0.291 #&gt; 4 3 0.155 0.155 #&gt; 5 4 0.0543 0.054 #&gt; 6 5 0.0130 0.013 #&gt; 7 6 0.00217 0.002 #&gt; 8 7 0.000248 0 #&gt; 9 8 0.0000186 0 #&gt; 10 9 0.000000827 0 #&gt; 11 10 0.0000000165 0 Y con esta función podemos resolver otros problemas de interés sin tener que recurrir al modelo probabilístico completo. Por ejemplo, contesta la siguientes preguntas (puedes usar simulación si es necesario) : ¿Cuál es la probabilidad de que al jugar este juego obtengamos 2 o menos seises? ¿Cuál es la probabilidad de que tiremos un número par de seises? Si repetimos este juego 5 veces, ¿cuál es la probabilidad de obtener un número total de seises mayor a 5? Distribución de probabilidad En general, la distribución de probabilidad de una variable aleatoria \\(X\\) es la función que asigna a cada evento \\(X\\in A\\) una probabilidad \\(P(X\\in A)\\). A los eventos \\(X \\in A\\) les llamamos eventos asociados a una variable aleatoria. Nótese que no todos los eventos posibles asociados son de la forma \\(X\\in A\\) para una variable aleatoria fija. Ejemplo En el ejemplo anterior calculamos \\(P(X \\in \\{ 0, 1, 2 \\} = P(X \\geq 2)\\) y \\(P(X \\in \\{0,2,4,6,8,10\\}).\\) Sin embargo, el evento “tiramos al menos un 3” no se puede escribir en la forma \\(P(X\\in A)\\). Este tipo de eventos siempre está relacionado con el número de seises que obtuvimos en el experimento. Como vimos arriba, si tenemos una variable aleatoria que toma un número finito de valores, las probabilidades \\(P(X\\in A)\\) pueden calcularse sumando probabilidades individuales de los valores en A: \\[P(X\\in A) = \\sum_{a \\in A} P(X = a)\\] Y una argumento similar se puede utilizar para variables que toman un número infinito pero numerable de valores (por ejemplo, enteros de 0 a infinito). Variables aleatorias continuas Consideremos el ejemplo del dardo sesgado unidimensional que vimos arriba. Para este experimento aleatorio, definimos la variable aleatoria \\(X =\\) posición del dardo en el intervalo \\([0,1]\\), y propusimos un modelo donde las probabilidades se calculan integrando: \\[P(X\\in [a,b]) = \\int_a^b f(x)\\, dx\\] En general, para cualquier subconjunto \\(A\\subset [0,1]\\) podemos definir la integral que defina la función de destribución como: \\[P(X\\in A) = \\int_A f(x)\\, dx\\] que se calcula sumando las probabilidades de los intervalos individuales que componen \\(A\\) Ejemplo Si queremos calcular la probabilidad de que el dardo segado caiga en los extremos, por ejemplo en \\([0,0.1]\\) o \\([0.9,1]\\), podemos hacer \\[A = [0,0.1]\\cup[0.9,1]\\] y calcular \\[P(X\\in A) = \\int_0^{0.1} f(x)\\, dx + \\int_{0.9}^1 f(x)\\, dx\\] La función de distribución acumulada contiene la misma información que la función de distribución y se define como \\[P(X \\le x)\\] con la ventaja de que la definición aplica tanto al caso discreto como en el caso continuo. Ejemplo Calculamos la función de distribución acumulada para el ejemplo anterior: X_prob %&gt;% mutate(fda = cumsum(prob)) #&gt; # A tibble: 11 x 4 #&gt; X prob prob_redondear fda #&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0 0.162 0.162 0.162 #&gt; 2 1 0.323 0.323 0.485 #&gt; 3 2 0.291 0.291 0.775 #&gt; 4 3 0.155 0.155 0.930 #&gt; 5 4 0.0543 0.054 0.985 #&gt; 6 5 0.0130 0.013 0.998 #&gt; 7 6 0.00217 0.002 1.000 #&gt; 8 7 0.000248 0 1.000 #&gt; 9 8 0.0000186 0 1.000 #&gt; 10 9 0.000000827 0 1.000 #&gt; 11 10 0.0000000165 0 1.000 Pregunta: ¿cómo recuperamos la probabilidades de cada valor a partir de la función de distribución acumulada? Esperanza La esperanza (valor esperado o media) de una variable aleatoria \\(X\\), es la media de la distribución \\(X\\), esto es, \\[E(X)=\\sum_{x\\in \\Omega_x} x P(X=x)\\] el promedio de todos los posibles valores de \\(X\\) ponderados por sus probabilidades. Por ejemplo, si \\(X\\) toma únicamente dos posibles valores, \\(a,b\\) con probabilidad \\(P(a)\\) y \\(P(b)\\) entonces \\[E(X)=aP(a)+bP(b).\\] Ejemplo: Supongamos que \\(X\\) es el valor que se produce cuando tiro un dado justo. Entonces, \\[E(X)=1\\cdot P(X=1) +2\\cdot P(X=2) +3\\cdot P(X=3) +4\\cdot P(X=4) +5\\cdot P(X=5) +6\\cdot P(X=6) = 3.5\\] Lo que nos dice que si tiramos el dado muchas veces deberíamos esperar que el promedio de las tiradas sea cercano a 3.5. Esperanza como un promedio cuando n es grande. Si vemos las probabilidades de los valores de \\(X\\) como una aproximación de frecuencias relativas cuando n es grande, entonces \\(E(X)\\) es aproximadamente el valor promedio del valor de \\(X\\) cuando n es grande. x &lt;- rnorm(10000, mean = 10) mean(x) #&gt; [1] 9.997833 La esperanza cumple las siguientes reglas: Constantes. La esperanza de una variable aleatoria constante es su valor constante, \\[E(c) = c\\] Indicadoras. Si \\(I_A\\) es la función indicadora del evento \\(A\\), \\[E(I_A) = P(A)\\] Funciones. Típicamente, \\(E[g(X)]\\ne g[E(X)]\\), pero \\[E[g(X)] = \\sum_{x \\in \\Omega_X} g(x) P(X=x)\\] Factores constantes. Para una constante c, \\[E(cX)=cE(X)\\] Adición. Para cualquier par de variables aleatorias \\(X\\), \\(Y\\), \\[E(X+Y) = E(X)+E(Y)\\] Multiplicación. Típicamente \\(E(XY) \\ne E(X)E(Y)\\), pero si \\(X\\) y \\(Y\\) son independientes, entonces \\[E(XY)=E(X)E(Y)\\] Varianza y desviación estándar Si intentamos predecir el valor de una variable aleatoria usando su media \\(E(X)=\\mu\\), vamos a fallar por una cantidad aleatoria \\(X-\\mu\\). Suele ser importante tener una idea de que tan grande será esta desviación. Debido a que \\[E(X-\\mu) = E(X)-\\mu=0\\] es necesario considerar la diferencia absoluta o la diferencia al cuadrado de \\(X-\\mu\\) con el fin de tener una idea del tamaño de la desviación sin importar el signo de esta. Varianza y desviación estándar. La varianza de \\(X\\), denotada \\(var(X)=\\sigma^2\\) es la media de la desviación cuadrada de \\(X\\) respecto a su valor esperado \\(\\mu=E(X)\\): \\[\\sigma^2(X)=var(X)=E(X-\\mu)^2\\] La desviación estándar de \\(X\\), es la raíz cuadrada de la varianza de X: \\[\\sigma(X)=sd(X)=\\sqrt{var(X)}\\] Intuitivamente, \\(sd(X)\\) es una medida de la dispersión de la distribución de \\(X\\) alrededor de su media. Debido a que la varianza es el valor central de la distribución de \\((X-\\mu)^2\\), su raíz cuadrada da una idea del tamaño típico de la desviación absoluta \\(|X-\\mu|\\). Notemos que \\(E(X)\\), \\(var(X)\\) y \\(sd(X)\\) están determinados por \\(X\\), de tal manera que si dos variables aleatorias tienen la misma distribución, también tienen la misma media, varianza y desviación estándar. "],
["bootstrap-no-paramétrico.html", "Sección 6 Bootstrap no paramétrico", " Sección 6 Bootstrap no paramétrico Bootstrap: to pull oneself up by one’s bootstrap Estas notas se desarrollaron con base en Efron and Tibshirani (1993), adicionalmente se usaron ideas de Hesterberg (2015). Abordamos los siguientes temas: Muestras aleatorias El principio del plug-in Bootstrap no paramétrico Ejemplos: componentes principales, ajuste de curvas, muestreo. Ejemplo: aspirina y ataques cardiacos Como explican Efron y Tibshirani, las explicaciones del bootstrap y otros métodos computacionales involucran las ideas de inferencia estadistica tradicional. Las ideas báscias no han cambiado pero la implementación de estas sí. Los tres conceptos básicos de estadística son: Recolección de datos, resúmenes (o descriptivos) de datos y inferencia. Veamos un ejemplo de estos conceptos y como se introduce bootstrap. Usaremos datos de un estudio clínico de consumo de aspirina y ataques cardiacos cuyos resultados fueron publicados en el New York Times: Planteamiento: se diseñó un estudio para investigar si el consumo de dosis bajas de aspirina podía prevenir los ataques cardiacos en hombres sanos en edad media. Recolección de datos: Se hizo un diseño controlado, aleatorizado y doblemente ciego. La mitad de los participantes recibieron aspirina y la otra mitad un placebo. Descriptivos: Las estadísticas descriptivas del artículo son muy sencillas: grupo ataques cardiacos sujetos aspirina 104 11037 placebo 189 11034 De manera que la estimación del cociente de las tasas es \\[\\hat{\\theta}=\\frac{104/11037}{189/11034} = 0.55\\] En la muestra los individuos que toman aspirina tienen únicamente 55% de los ataques que los que toman placebo. Sin embargo, lo que realmente nos interesa es \\(\\theta\\): el cociente de tasas que observaríamos si pudieramos tratar a todos los hombres y no únicamente a una muestra. Inferencia: aquí es donde recurrimos a inferencia estadística: \\[0.43 &lt; \\theta &lt; 0.70\\] El verdadero valor de \\(\\theta\\) esta en el intervalo \\((0.43,0.70)\\) con una confianza del 95%. Ahora, el bootstrap es un método de simulación basado en datos para inferencia estadística. La idea detrás es que si una muestra es una aproximación de la población que la generó, entoces podemos hacer muestreos de la muestra para calcular una estadística de interés y medir la exactitud en la misma. En este caso tenemos los resultados del experimento en la variable trial. trial &lt;- tibble(patient = 1:22071, group = ifelse(patient &lt;= 11037, &quot;aspirin&quot;, &quot;control&quot;), heart_attack = c(rep(TRUE, 104), rep(FALSE, 10933), rep(TRUE, 189), rep(FALSE, 10845))) trial #&gt; # A tibble: 22,071 x 3 #&gt; patient group heart_attack #&gt; &lt;int&gt; &lt;chr&gt; &lt;lgl&gt; #&gt; 1 1 aspirin TRUE #&gt; 2 2 aspirin TRUE #&gt; 3 3 aspirin TRUE #&gt; 4 4 aspirin TRUE #&gt; 5 5 aspirin TRUE #&gt; 6 6 aspirin TRUE #&gt; 7 7 aspirin TRUE #&gt; 8 8 aspirin TRUE #&gt; 9 9 aspirin TRUE #&gt; 10 10 aspirin TRUE #&gt; # … with 22,061 more rows Y calculamos el cociente de las tasas: summary_stats &lt;- trial %&gt;% group_by(group) %&gt;% summarise( n_attacks = sum(heart_attack), n_subjects = n(), rate_attacks = n_attacks / n_subjects * 100 ) summary_stats #&gt; # A tibble: 2 x 4 #&gt; group n_attacks n_subjects rate_attacks #&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; #&gt; 1 aspirin 104 11037 0.942 #&gt; 2 control 189 11034 1.71 ratio_rates &lt;- summary_stats$rate_attacks[1] / summary_stats$rate_attacks[2] ratio_rates #&gt; [1] 0.550115 Después calculamos 1000 replicaciones bootstrap de \\(\\hat{\\theta*}\\) boot_ratio_rates &lt;- function(){ boot_sample &lt;- trial %&gt;% group_by(group) %&gt;% sample_frac(replace = TRUE) rates &lt;- boot_sample %&gt;% summarise(rate_attacks = sum(heart_attack) / n()) %&gt;% pull(rate_attacks) rates[1] / rates[2] } boot_ratio_rates &lt;- rerun(1000, boot_ratio_rates()) %&gt;% flatten_dbl() Las replicaciones se pueden utilizar para hacer inferencia de los datos. Por ejemplo, podemos estimar el error estándar de \\(\\theta\\): se &lt;- sd(boot_ratio_rates) comma(se) #&gt; [1] &quot;0.07&quot; Referencias "],
["el-principio-del-plug-in.html", "6.1 El principio del plug-in", " 6.1 El principio del plug-in Muestras aleatorias Supongamos que tenemos una población finita o universo \\(U\\), conformado por unidades individuales con propiedades que nos gustaría aprender (opinión política, nivel educativo, preferencias de consumo, …). Debido a que es muy difícil y caro examinar cada unidad en \\(U\\) seleccionamos una muestra aleatoria. Una muestra aleatoria de tamaño \\(n\\) se define como una colección de \\(n\\) unidades \\(u_1,...,u_n\\) seleccionadas aleatoriamente de una población \\(U\\). Una vez que se selecciona una muestra aleatoria, los datos observados son la colección de medidas \\(x_1,...,x_n\\), también denotadas \\(\\textbf{x} = (x_1,...,x_n)\\). En principio, el proceso de muestreo es como sigue: Seleccionamos \\(n\\) enteros de manera independiente (con probabilidad \\(1/N\\)), cada uno de ellos asociado a un número entre \\(1\\) y \\(N\\). Los enteros determinan las unidades que seleccionamos y tomamos medidas a cada unidad. En la práctica el proceso de selección suele ser más complicado y la definición de la población \\(U\\) suele ser deficiente; sin embargo, el marco conceptual sigue siendo útil para entender la inferencia estadística. Nuestra definición de muestra aleatoria comprende muestras con y sin reemplazo: muestra sin reemplazo: una unidad particular puede aparecer a lo más una vez. muestra con reemplazo: permite que una unidad aparezca más de una vez. Es más común tomar muestras sin remplazo, sin embargo, para hacer inferencia suele ser más sencillo permitir repeticiones (muestreo con remplazo) y si el tamaño de la muestra \\(n\\) es mucho más chico que la población \\(N\\), la probabilidad de muestrear la misma unidad más de una vez es chica. El caso particular en el que obtenemos las medidas de interés de cada unidad en la población se denomina censo, y denotamos al conjunto de datos observados de la población por \\(\\mathcal{X}\\). En general, no nos interesa simplemente describir la muestra que observamos sino que queremos aprender acerca de la población de donde se seleccionó la muestra: El objetivo de la inferencia estadística es expresar lo que hemos aprendido de la población \\(\\mathcal{X}\\) a partir de los datos observados \\(\\textbf{x}\\). Ejemplo: ENLACE Veamos un ejemplo donde tomamos una muestra de 300 escuelas primarias del Estado de México, de un universo de 7,518 escuelas, library(estcomp) # universo enlace &lt;- enlacep_2013 %&gt;% janitor::clean_names() %&gt;% mutate(id = 1:n()) %&gt;% select(id, cve_ent, turno, tipo, esp_3 = punt_esp_3, esp_6 = punt_esp_6, n_eval_3 = alum_eval_3, n_eval_6 = alum_eval_6) %&gt;% na.omit() %&gt;% filter(esp_3 &gt; 0, esp_6 &gt; 0, n_eval_3 &gt; 0, n_eval_6 &gt; 0, cve_ent == &quot;15&quot;) glimpse(enlace) #&gt; Observations: 7,518 #&gt; Variables: 8 #&gt; $ id &lt;int&gt; 38570, 38571, 38572, 38573, 38574, 38575, 38576, 38577, 3857… #&gt; $ cve_ent &lt;chr&gt; &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, &quot;15&quot;, … #&gt; $ turno &lt;chr&gt; &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, &quot;MATUTINO&quot;, … #&gt; $ tipo &lt;chr&gt; &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, &quot;INDêGENA&quot;, … #&gt; $ esp_3 &lt;dbl&gt; 550, 485, 462, 646, 508, 502, 570, 441, 597, 648, 535, 430, … #&gt; $ esp_6 &lt;dbl&gt; 483, 490, 385, 613, 452, 500, 454, 427, 582, 614, 443, 562, … #&gt; $ n_eval_3 &lt;dbl&gt; 13, 17, 9, 33, 26, 10, 65, 82, 132, 16, 16, 6, 10, 27, 10, 1… #&gt; $ n_eval_6 &lt;dbl&gt; 19, 18, 9, 26, 35, 13, 49, 78, 110, 18, 9, 2, 12, 34, 9, 6, … set.seed(16021) n &lt;- 300 # muestra enlace_muestra &lt;- sample_n(enlace, n) %&gt;% mutate(clase = &quot;muestra&quot;) para cada escuela en la muestra consideremos la medida \\(x_i\\), conformada por el promedio de las calificaciones en español de los alumnos de tercero y sexto de primaria (prueba ENLACE 2010): \\[x_i=(esp_{3i}, esp_{6i})\\] En este ejemplo contamos con un censo de las escuelas y tomamos la muestra aleatoria de la tabla de datos general, sin embargo, es común contar únicamente con la muestra. Para español 3o de primaria la media observada es mean(enlace_muestra$esp_3) #&gt; [1] 554.5867 La media muestral es una estadística descriptiva de la muestra, pero también la podemos usar para describir a la población de escuelas. Al usar la media observada para describir a la población estamos aplicando el principio del plug-in que dice que una característica dada de una distribución puede ser aproximada por la equivalente evaluada en la distribución empírica de una muestra aleatoria. Función de distribución empírica Dada una muestra aleatoria de tamaño \\(n\\) de una distribución de probabilidad \\(P\\), la función de distribución empírica \\(P_n\\) se define como la distribución que asigna probabilidad \\(1/n\\) a cada valor \\(x_i\\) con \\(i=1,2,...,n\\). En otras palabras, \\(P_n\\) asigna a un conjunto \\(A\\) en el espacio muestral de \\(x\\) la probabilidad empírica: \\[P_n(A)=\\#\\{x_i \\in A \\}/n\\] La función de distribución empírica \\(P_n\\) es una estimación de la distribución completa \\(P\\), por lo que una manera inmediata de estimar aspectos de \\(P\\) (e.g media o mediana) es calcular el aspecto correspondiente de \\(P_n\\). En cuanto a la teoría el principio del plug-in está soportado por el teorema de Glivenko Cantelli: Sea \\(X_1,...,X_n\\) una muestra aleatoria de una distribución \\(P\\), con distribución empírica \\(P_n\\) entonces \\[\\sup_{x \\in \\mathcal{R}}|P_n(x)-P(x)|\\to_p0\\] casi seguro. Regresando al ejemplo de las escuelas, comparemos la distribución poblacional y la distribución empírica. enlace_long &lt;- enlace %&gt;% mutate(clase = &quot;población&quot;) %&gt;% bind_rows(enlace_muestra) %&gt;% gather(grado, calif, esp_3:esp_6) ggplot(enlace_long, aes(x = calif)) + geom_histogram(aes(y = ..density..), binwidth = 20, fill = &quot;darkgray&quot;) + facet_grid(grado ~ clase) Podemos comparar la función de distribución acumulada empírica y la función de distribución acumulada poblacional: En la siguiente gráfica la curva roja representa la función de distribución acumulada empírica y la curva con relleno gris la función de distribución acumulada poblacional. ggplot() + stat_ecdf(data = filter(enlace_long, clase == &quot;población&quot;), aes(x = calif, ymin = 0, ymax = ..y..), geom = &quot;ribbon&quot;, pad = TRUE, alpha = 0.5, fill = &quot;gray&quot;, color = &quot;darkgray&quot;) + stat_ecdf(data = filter(enlace_long, clase == &quot;muestra&quot;), aes(x = calif), geom = &quot;step&quot;, color = &quot;red&quot;) + facet_grid(~ grado) + labs(color = &quot;&quot;) Cuando la variable de interés toma pocos valores es fácil ver la distribución empírica, supongamos que la medición de las unidades que nos interesa es la variable tipo de escuela, entonces la distribución empírica en la muestra es table(enlace_muestra$tipo) / n #&gt; #&gt; CONAFE GENERAL INDêGENA PARTICULAR #&gt; 0.01000000 0.82000000 0.02333333 0.14666667 Vale la pena notar que pasar de la muestra desagregada a la distribución empírica (lista de valores y la proporción que ocurre cada una en la muestra) no conlleva ninguna pérdida de información: el vector de frecuencias observadas es un estadístico suficiente para la verdadera distribución. Esto quiere decir que toda la información de \\(P\\) contenida en el vector de observaciones \\(\\textbf{x}\\) está también contenida en \\(P_n\\). Nota: el teorema de suficiencia asume que las observaciones \\(\\textbf{x}\\) son una muestra aleatoria de la distribución \\(P\\), este no es siempre el caso (e.g. si tenemos una serie de tiempo). Parámetros y estadísticas Cuando aplicamos teoría estadística a problemas reales, es común que las respuestas estén dadas en términos de distribuciones de probabilidad. Por ejemplo, podemos preguntarnos que tan correlacionados están los resultados de las pruebas de español correspondientes a 3o y 6o. Si conocemos la distribución de probabilidad \\(P\\) contestar esta pregunta es simplemente cuestión de aritmética, el coeficiente de correlación poblacional esta dado por: \\[corr(y,z) = \\frac{\\sum_{j=1}^{N}(Y_j - \\mu_y)(Z_j-\\mu_z)} {[\\sum_{j=1}^{N}(Y_j - \\mu_y)^2\\sum_{j=1}^{N}(Z_j - \\mu_z)^2]^{1/2}}\\] en nuestro ejemplo \\((Y_j,Z_j)\\) son el j-ésimo punto en la población de escuelas primarias \\(\\mathcal{X}\\), \\(\\mu_y=\\sum Y_j/3311\\) y \\(\\mu_z=\\sum Z_j/3311\\). ggplot(enlace, aes(x = esp_3, y = esp_6)) + geom_point(alpha = 0.5) cor(enlace$esp_3, enlace$esp_6) %&gt;% round(2) #&gt; [1] 0.49 Si no tenemos un censo debemos inferir, podríamos estimar la correlación \\(corr(y,z)\\) a través del coeficiente de correlación muestral: \\[\\hat{corr}(y,z) = \\frac{\\sum_{j=1}^{n}(y_j - \\hat{\\mu}_y)(z_j-\\hat{\\mu}_z)} {[\\sum_{j=1}^{n}(y_j - \\hat{\\mu}_y)^2\\sum_{j=1}^{n}(z_j - \\hat{\\mu}_z)^2]^{1/2}}\\] recordando que la distribución empírica es una estimación de la distribución completa. cor(enlace_muestra$esp_3, enlace_muestra$esp_6) #&gt; [1] 0.4392921 Al igual que la media esto es una estimación plug-in. Otros ejemplos son: Supongamos que nos interesa estimar la mediana de las calificaciones de español para 3^o de primaria: median(enlace_muestra$esp_3) #&gt; [1] 554.5 Supongamos que nos interesa estimar la probabilidad de que la calificación de español de una escuela sea mayor a 700: \\[\\theta=\\frac{1}{N}\\sum_{j=1}^N I_{\\{Y_i&gt;700\\}}\\] donde \\(I_{\\{\\cdot\\}}\\) es la función indicadora. La estimación plug-in de \\(\\hat{\\theta}\\) sería: sum(enlace_muestra$esp_3 &gt; 700) / n #&gt; [1] 0.01333333 Ejemplo: dado Observamos 100 lanzamientos de un dado, obteniendo la siguiente distribución empírica: dado &lt;- read.table(&quot;data/dado.csv&quot;, header = TRUE, quote = &quot;\\&quot;&quot;) prop.table(table(dado$x)) #&gt; #&gt; 1 2 3 4 5 6 #&gt; 0.13 0.19 0.10 0.17 0.14 0.27 En este caso no tenemos un censo, solo contamos con la muestra. Una pregunta de inferencia que surge de manera natural es si el dado es justo, esto es, si la distribución que generó esta muestra tiene una distribución \\(P = (1/6, 1/6, 1/6,1/6, 1/6, 1/6)\\). Para resolver esta pregunta, debemos hacer inferencia de la distribución empírica. Antes de proseguir repasemos dos conceptos importantes: parámetros y estadísticos: Un parámetro es una función de la distribución de probabilidad \\(\\theta=t(P)\\), mientras que una estadística es una función de la muestra \\(\\textbf{x}\\). Por ejemplo, la \\(corr(x,y)\\) es un parámetro de \\(P\\) y \\(\\hat{corr}(x,y)\\) es una estadística con base en \\(\\textbf{x}\\) y \\(\\textbf{y}\\). Entonces: El principio del plug-in es un método para estimar parámetros a partir de muestras; la estimación plug-in de un parámetro \\(\\theta=t(P)\\) se define como: \\[\\hat{\\theta}=t(P_n).\\] Es decir, estimamos la función \\(\\theta = t(P)\\) de la distribución de probabilidad \\(P\\) con la misma función aplicada en la distribución empírica \\(\\hat{\\theta}=t(P_n)\\). ¿Qué tan bien funciona el principio del plug-in? Suele ser muy bueno cuando la única información disponible de \\(P\\) es la muestra \\(\\textbf{x}\\), bajo esta circunstancia \\(\\hat{\\theta}=t(P_n)\\) no puede ser superado como estimador de \\(\\theta=t(P)\\), al menos no en el sentido asintótico de teoría estadística \\((n\\to\\infty)\\). El principio del plug-in provee de una estimación más no habla de precisión: usaremos el bootstrap para estudiar el sesgo y el error estándar del estimador plug-in \\(\\hat{\\theta}=t(P_n)\\). Distribuciones muestrales y errores estándar La distribución muestral de una estadística es la distribución de probabilidad de la misma, considerada como una variable aleatoria. Es así que la distribución muestral depende de: 1) La distribución poblacional, 2) la estadística que se está considerando, y 3) la muestra aleatoria: cómo se seleccionan las unidades de la muestra y cuántas. En teoría para obtener la distribución muestral uno seguiría los siguientes pasos: Selecciona muestras de una población (todas las posibles o un número infinito de muestras). Calcula la estadística de interés para cada muestra. La distribución de la estadística es la distribución muestral. library(LaplacesDemon) library(patchwork) # En este ejemplo la población es una mezcla de normales pob_plot &lt;- ggplot(data_frame(x = -15:20), aes(x)) + stat_function(fun = dnormm, args = list(p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3)), alpha = 0.8) + geom_vline(aes(color = &quot;mu&quot;, xintercept = 5), alpha = 0.5) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;), name = &#39;&#39;, labels = expression(mu)) + labs(x = &quot;&quot;, subtitle = &quot;Población&quot;, color = &quot;&quot;) samples &lt;- data_frame(sample = 1:3) %&gt;% mutate( sims = rerun(3, rnormm(30, p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3))), x_bar = map_dbl(sims, mean)) muestras_plot &lt;- samples %&gt;% unnest() %&gt;% ggplot(aes(x = sims)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_segment(aes(x = x_bar, xend = x_bar, y = 0, yend = 0.8), color = &quot;blue&quot;) + xlim(-15, 20) + facet_wrap(~ sample) + geom_text(aes(x = x_bar, y = 0.95, label = &quot;bar(x)&quot;), parse = TRUE, color = &quot;blue&quot;, alpha = 0.2, hjust = 1) + labs(x = &quot;&quot;, subtitle = &quot;Muestras&quot;) samples_dist &lt;- data_frame(sample = 1:10000) %&gt;% mutate( sims = rerun(10000, rnormm(100, p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3))), mu_hat = map_dbl(sims, mean)) dist_muestral_plot &lt;- ggplot(samples_dist, aes(x = mu_hat)) + geom_density(adjust = 2) + labs(x = &quot;&quot;, subtitle = expression(&quot;Distribución muestral de &quot;~hat(mu))) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) (pob_plot | plot_spacer()) / (muestras_plot | dist_muestral_plot) Para hacer inferencia necesitamos describir la forma de la distribución muestral, es natural pensar en la desviación estándar pues es una medida de la dispersión de la distribución de la estadística alrededor de su media: El error estándar es la desviación estándar de la distribución muestral de una estadística. Ejemplo: el error estándar de una media Supongamos que \\(x\\) es una variable aleatoria que toma valores en los reales con distribución de probabilidad \\(P\\). Denotamos por \\(\\mu_P\\) y \\(\\sigma_P^2\\) la media y varianza de \\(P\\), \\[\\mu_P = E_P(x),\\] \\[\\sigma_P^2=var_P(x)=E_P[(x-\\mu_P)^2]\\] en la notación enfatizamos la dependencia de la media y varianza en la distribución \\(P\\). Ahora, sea \\((x_1,...,x_n)\\) una muestra aleatoria de \\(P\\), de tamaño \\(n\\), la media de la muestra \\(\\bar{x}=\\sum_{i=1}^nx_i/n\\) tiene: esperanza \\(\\mu_P\\), varianza \\(\\sigma_P^2/n\\). En palabras: la esperanza de \\(\\bar{x}\\) es la misma que la esperanza de \\(x\\), pero la varianza de \\(\\bar{x}\\) es \\(1/n\\) veces la varianza de \\(x\\), así que entre mayor es la \\(n\\) tenemos una mejor estimación de \\(\\mu_P\\). En el caso de la media \\(\\bar{x}\\), el error estándar, que denotamos \\(se_P(\\bar{x})\\), es la raíz de la varianza de \\(\\bar{x}\\), \\[se_P(\\bar{x}) = [var_P(\\bar{x})]^{1/2}= \\sigma_P/ \\sqrt{n}.\\] En este punto podemos usar el principio del plug-in, simplemente sustituimos \\(P_n\\) por \\(P\\) y obtenemos, primero, una estimación de \\(\\sigma_P\\): \\[\\hat{\\sigma}=\\hat{\\sigma}_{P_n} = \\bigg\\{\\frac{1}{n}\\sum_{i=1}^n(x_i-\\bar{x})^2\\bigg\\}^{1/2}\\] de donde se sigue la estimación del error estándar: \\[\\hat{se}(\\bar{x})=\\hat{\\sigma}_{P_n}/\\sqrt{n}=\\bigg\\{\\frac{1}{n^2}\\sum_{i=1}^n(x_i-\\bar{x})^2\\bigg\\}^{1/2}\\] Notemos que usamos el principio del plug-in en dos ocasiones, primero para estimar la esperanza \\(\\mu_P\\) mediante \\(\\mu_{P_n}\\) y luego para estimar el error estándar \\(se_P(\\bar{x})\\). Consideramos los datos de ENLACE edo. de México (enlace), y la columna de calificaciones de español 3o de primaria (esp_3). Selecciona una muestra de tamaño \\(n = 10, 100, 1000\\). Para cada muestra calcula media y el error estándar de la media usando el principio del plug-in: \\(\\hat{\\mu}=\\bar{x}\\), y \\(\\hat{se}(\\bar{x})=\\hat{\\sigma}_{P_n}/\\sqrt{n}\\). Ahora aproximareos la distribución muestral, para cada tamaño de muestra \\(n\\): simula 10,000 muestras aleatorias, ii) calcula la media en cada muestra, iii) Realiza un histograma de la distribución muestral de las medias (las medias del paso anterior) iv) aproxima el error estándar calculando la desviación estándar de las medias del paso ii. Calcula el error estándar de la media para cada tamaño de muestra usando la información poblacional (ésta no es una aproximación), usa la fórmula: \\(se_P(\\bar{x}) = \\sigma_P/ \\sqrt{n}\\). ¿Cómo se comparan los errores estándar correspondientes a los distintos tamaños de muestra? ¿Por qué bootstrap? En el caso de la media \\(\\hat{\\theta}=\\bar{x}\\) la aplicación del principio del plug-in para el cálculo de errores estándar es inmediata; sin embargo, hay estadísticas para las cuáles no es fácil aplicar este método. El método de aproximarlo con simulación, como lo hicimos en el ejercicio de arriba no es factible pues en la práctica no podemos seleccionar un número arbitrario de muestras de la población, sino que tenemos únicamente una muestra. La idea del bootstrap es replicar el método de simulación para aproximar el error estándar, esto es seleccionar muchas muestras y calcular la estadística de interés en cada una, con la diferencia que las muestras se seleccionan de la distribución empírica a falta de la distribución poblacional. "],
["el-estimador-bootstrap-del-error-estándar.html", "6.2 El estimador bootstrap del error estándar", " 6.2 El estimador bootstrap del error estándar Entonces, los pasos para calcular estimador bootstrap del error estándar son: Tenemos una muestra aleatoria \\(\\textbf{x}=(x_1,x_2,...,x_n)\\) proveniente de una distribución de probabilidad desconocida \\(P\\), Seleccionamos muestras aleatorias con reemplazo de la distribución empírica. Calculamos la estadística de interés para cada muestra: \\[\\hat{\\theta}=s(\\textbf{x})\\] la estimación puede ser la estimación plug-in \\(t(P_n)\\) pero también puede ser otra. La distribución de la estadística es la distribución bootstrap, y el estimador bootstrap del error estándar es la desviación estándar de la distribución bootstrap. dist_empirica &lt;- tibble(id = 1:30, obs = samples$sims[[1]]) dist_empirica_plot &lt;- ggplot(dist_empirica, aes(x = obs)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(aes(color = &quot;mu&quot;, xintercept = 5), alpha = 0.5) + geom_vline(aes(xintercept = samples$x_bar[1], color = &quot;x_bar&quot;), alpha = 0.8, linetype = &quot;dashed&quot;) + xlim(-15, 20) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + labs(x = &quot;&quot;, subtitle = expression(&quot;Distribución empírica&quot;~P[n])) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;, &#39;x_bar&#39; = &#39;blue&#39;), name = &#39;&#39;, labels = c(expression(mu), expression(bar(x)))) samples_boot &lt;- data_frame(sample_boot = 1:3) %&gt;% mutate( sims_boot = rerun(3, sample(dist_empirica$obs, replace = TRUE)), x_bar_boot = map_dbl(sims_boot, mean) ) muestras_boot_plot &lt;- samples_boot %&gt;% unnest() %&gt;% ggplot(aes(x = sims_boot)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(aes(xintercept = samples$x_bar[1]), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_segment(aes(x = x_bar_boot, xend = x_bar_boot, y = 0, yend = 0.8), color = &quot;black&quot;) + xlim(-15, 20) + facet_wrap(~ sample_boot) + geom_text(aes(x = x_bar_boot, y = 0.95, label = &quot;bar(x)^&#39;*&#39;&quot;), parse = TRUE, color = &quot;black&quot;, alpha = 0.3, hjust = 1) + labs(x = &quot;&quot;, subtitle = &quot;Muestras bootstrap&quot;) boot_dist &lt;- data_frame(sample = 1:10000) %&gt;% mutate( sims_boot = rerun(10000, sample(dist_empirica$obs, replace = TRUE)), mu_hat_star = map_dbl(sims_boot, mean)) boot_muestral_plot &lt;- ggplot(boot_dist, aes(x = mu_hat_star)) + geom_histogram(alpha = 0.5, fill = &quot;darkgray&quot;) + labs(x = &quot;&quot;, subtitle = expression(&quot;Distribución bootstrap de &quot;~hat(mu)^&#39;*&#39;)) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_vline(aes(xintercept = samples$x_bar[1]), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) (dist_empirica_plot | plot_spacer()) / (muestras_boot_plot | boot_muestral_plot) Describamos la notación y conceptos: Definimos una muestra bootstrap como una muestra aleatoria de tamaño \\(n\\) que se obtiene de la distribución empírica \\(P_n\\) y la denotamos \\[\\textbf{x}^* = (x_1^*,...,x_n^*).\\] La notación de estrella indica que \\(\\textbf{x}^*\\) no son los datos \\(\\textbf{x}\\) sino una versión de remuestreo de \\(\\textbf{x}\\). Otra manera de frasearlo: Los datos bootsrtap \\(x_1^*,...,x_n^*\\) son una muestra aleatoria de tamaño \\(n\\) seleccionada con reemplazo de la población de \\(n\\) objetos \\((x_1,...,x_n)\\). A cada muestra bootstrap \\(\\textbf{x}^*\\) le corresponde una replicación \\(\\hat{\\theta}^*=s(\\textbf{x}^*).\\) el estimador bootstrap de \\(se_P(\\hat{\\theta})\\) se define como: \\[se_{P_n}(\\hat{\\theta}^*)\\] en otras palabras, la estimación bootstrap de \\(se_P(\\hat{\\theta})\\) es el error estándar de \\(\\hat{\\theta}\\) para conjuntos de datos de tamaño \\(n\\) seleccionados de manera aleatoria de \\(P_n\\). La fórmula \\(se_{P_n}(\\hat{\\theta}^*)\\) no existe para casi ninguna estimación diferente de la media, por lo que recurrimos a la técnica computacional bootstrap: Algoritmo bootstrap para estimar errores estándar Selecciona \\(B\\) muestras bootstrap independientes: \\[\\textbf{x}^{*1},..., \\textbf{x}^{*B}\\]. Evalúa la replicación bootstrap correspondiente a cada muestra bootstrap: \\[\\hat{\\theta}^{*b}=s(\\textbf{x}^{*b})\\] para \\(b=1,2,...,B.\\) Estima el error estándar \\(se_P(\\hat{\\theta})\\) usando la desviación estándar muestral de las \\(B\\) replicaciones: \\[\\hat{se}_B = \\bigg\\{\\frac{\\sum_{b=1}^B[\\hat{\\theta}^{*}(b)-\\hat{\\theta}^*(\\cdot)]^2 }{B-1}\\bigg\\}^{1/2}\\] donde \\[\\hat{\\theta}^*(\\cdot)=\\sum_{b=1}^B \\theta^{*}(b)/B \\]. Notemos que: La estimación bootstrap de \\(se_{P}(\\hat{\\theta})\\), el error estándar de una estadística \\(\\hat{\\theta}\\), es un estimador plug-in que usa la función de distribución empírica \\(P_n\\) en lugar de la distribución desconocida \\(P\\). Conforme el número de replicaciones \\(B\\) aumenta \\[\\hat{se}_B\\approx se_{P_n}(\\hat{\\theta})\\] este hecho equivale a decir que la desviación estándar empírica se acerca a la desviación estándar poblacional conforme crece el número de muestras. La población en este caso es la población de valores \\(\\hat{\\theta}^*=s(x^*)\\). Al estimador de bootstrap ideal \\(se_{P_n}(\\hat{\\theta})\\) y su aproximación \\(\\hat{se}_B\\) se les denota estimadores bootstrap no paramétricos ya que estan basados en \\(P_n\\), el estimador no paramétrico de la población \\(P\\). Ejemplo: Error estándar bootstrap de una media mediaBoot &lt;- function(x){ # x: variable de interés # n: número de replicaciones bootstrap n &lt;- length(x) muestra_boot &lt;- sample(x, size = n, replace = TRUE) mean(muestra_boot) # replicacion bootstrap de theta_gorro } thetas_boot &lt;- rerun(10000, mediaBoot(enlace_muestra$esp_3)) %&gt;% flatten_dbl() sd(thetas_boot) #&gt; [1] 3.238793 y se compara con \\(\\hat{se}(\\bar{x})\\) (estimador plug-in del error estándar): se &lt;- function(x) sqrt(sum((x - mean(x)) ^ 2)) / length(x) se(enlace_muestra$esp_3) #&gt; [1] 3.264511 Nota: Conforme \\(B\\) aumenta \\(\\hat{se}_{B}(\\bar{x})\\to \\{\\sum_{i=1}^n(x_i - \\bar{x})^2 / n \\}^{1/2}\\), se demuestra con la ley débil de los grandes números. Considera el coeficiente de correlación muestral entre la calificación de \\(y=\\)esp_3 y la de \\(z=\\)esp_6: \\(\\hat{corr}(y,z)=0.9\\). ¿Qué tan precisa es esta estimación? Variación en distribuciones bootstrap En el proceso de estimación bootstrap hay dos fuentes de variación pues: La muestra original se selecciona con aleatoriedad de una población. Las muestras bootstrap se seleccionan con aleatoriedad de la muestra original. Esto es: La estimación bootstrap ideal es un resultado asintótico \\(B=\\infty\\), en esta caso \\(\\hat{se}_B\\) iguala la estimación plug-in \\(se_{P_n}\\). En el proceso de bootstrap podemos controlar la variación del segundo aspecto, conocida como implementación de muestreo Monte Carlo, y la variación Monte Carlo decrece conforme incrementamos el número de muestras. Podemos eliminar la variación Monte Carlo si seleccionamos todas las posibles muestras con reemplazo de tamaño \\(n\\), hay \\({2n-1}\\choose{n}\\) posibles muestras y si seleccionamos todas obtenemos \\(\\hat{se}_\\infty\\) (bootstrap ideal), sin embargo, en la mayor parte de los problemas no es factible proceder así. set.seed(8098) pob_plot &lt;- ggplot(data_frame(x = -15:20), aes(x)) + stat_function(fun = dnormm, args = list(p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3)), alpha = 0.8) + geom_vline(aes(color = &quot;mu&quot;, xintercept = 5), alpha = 0.5) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;), name = &#39;&#39;, labels = expression(mu)) + labs(x = &quot;&quot;, y = &quot;&quot;, subtitle = &quot;Población&quot;, color = &quot;&quot;) + theme(axis.text.y = element_blank()) samples &lt;- data_frame(sample = 1:6) %&gt;% mutate( sims = rerun(6, rnormm(50, p = c(0.3, 0.7), mu = c(-2, 8), sigma = c(3.5, 3))), x_bar = map_dbl(sims, mean)) means_boot &lt;- function(n, sims) { rerun(n, mean(sample(sims, replace = TRUE))) %&gt;% flatten_dbl() } samples_boot &lt;- samples %&gt;% mutate( medias_boot_30_1 = map(sims, ~means_boot(n = 30, .)), medias_boot_30_2 = map(sims, ~means_boot(n = 30, .)), medias_boot_1000_1 = map(sims, ~means_boot(n = 1000, .)), medias_boot_1000_2 = map(sims, ~means_boot(n = 1000, .)) ) emp_dists &lt;- samples_boot %&gt;% unnest(cols = sims) %&gt;% rename(obs = sims) emp_dists_plots &lt;- ggplot(emp_dists, aes(x = obs)) + geom_histogram(binwidth = 2, alpha = 0.5, fill = &quot;darkgray&quot;) + geom_vline(aes(color = &quot;mu&quot;, xintercept = 5), alpha = 0.5, show.legend = FALSE) + geom_vline(aes(xintercept = x_bar, color = &quot;x_bar&quot;), show.legend = FALSE, alpha = 0.8, linetype = &quot;dashed&quot;) + xlim(-15, 20) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + labs(x = &quot;&quot;, y = &quot;&quot;, subtitle = expression(&quot;Distribución empírica&quot;~P[n])) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;, &#39;x_bar&#39; = &#39;blue&#39;), name = &#39;&#39;, labels = c(expression(mu), expression(bar(x)))) + facet_wrap(~ sample, ncol = 1) + theme(strip.background = element_blank(), strip.text.x = element_blank(), axis.text.y = element_blank()) boot_dists_30 &lt;- samples_boot %&gt;% unnest(cols = c(medias_boot_30_1, medias_boot_30_2)) %&gt;% pivot_longer(cols = c(medias_boot_30_1, medias_boot_30_2), values_to = &quot;mu_hat_star&quot;, names_to = &quot;boot_trial&quot;, names_prefix = &quot;medias_boot_30_&quot;) boot_dists_30_plot &lt;- ggplot(boot_dists_30, aes(x = mu_hat_star)) + geom_histogram(alpha = 0.5, fill = &quot;darkgray&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;, subtitle = expression(&quot;Distribución bootstrap B = 30&quot;)) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_vline(aes(xintercept = x_bar), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) + facet_grid(sample~boot_trial) + theme(strip.background = element_blank(), strip.text.y = element_blank(), axis.text.y = element_blank()) boot_dists_1000 &lt;- samples_boot %&gt;% unnest(cols = c(medias_boot_1000_1, medias_boot_1000_2)) %&gt;% pivot_longer(cols = c(medias_boot_1000_1, medias_boot_1000_2), values_to = &quot;mu_hat_star&quot;, names_to = &quot;boot_trial&quot;, names_prefix = &quot;medias_boot_1000_&quot;) boot_dists_1000_plot &lt;- ggplot(boot_dists_1000, aes(x = mu_hat_star)) + geom_histogram(alpha = 0.5, fill = &quot;darkgray&quot;) + labs(subtitle = expression(&quot;Distribución bootstrap B = 1000&quot;), x = &quot;&quot;, y = &quot;&quot;) + geom_vline(xintercept = 5, color = &quot;red&quot;, alpha = 0.5) + geom_vline(aes(xintercept = x_bar), color = &quot;blue&quot;, linetype = &quot;dashed&quot;, alpha = 0.8) + facet_grid(sample~boot_trial) + scale_colour_manual(values = c(&#39;mu&#39; = &#39;red&#39;, &#39;x_bar&#39; = &#39;blue&#39;), name = &#39;&#39;, labels = c(expression(mu), expression(bar(x)))) + theme(strip.background = element_blank(), strip.text.y = element_blank(), strip.text.x = element_blank(), axis.text.y = element_blank()) (pob_plot | plot_spacer() | plot_spacer()) / (emp_dists_plots | boot_dists_30_plot | boot_dists_1000_plot) + plot_layout(heights = c(1, 5)) En la siguiente gráfica mostramos 6 posibles muestras de tamaño 50 simuladas de la población, para cada una de ellas se graficó la distribución empírica y se se realizan histogramas de la distribución bootstrap con \\(B=30\\) y \\(B=1000\\), en cada caso hacemos dos repeticiones, notemos que cuando el número de muestras bootstrap es grande las distribuciones bootstrap son muy similares (para una muestra de la población dada), esto es porque disminuimos el erro Monte Carlo. También vale la pena recalcar que la distribución bootstrap está centrada en el valor observado en la muestra (línea azúl punteada) y no en el valor poblacional sin embargo la forma de la distribución es similar a lo largo de las filas. Entonces, ¿cuántas muestras bootstrap? Incluso un número chico de replicaciones bootstrap, digamos \\(B=25\\) es informativo, y \\(B=50\\) con frecuencia es suficiente para dar una buena estimación de \\(se_P(\\hat{\\theta})\\) (Efron and Tibshirani (1993)). Cuando se busca estimar error estándar Hesterberg (2015) recomienda \\(B=1000\\) muestras, o \\(B=10,000\\) muestras dependiendo la presición que se busque. seMediaBoot &lt;- function(x, B){ thetas_boot &lt;- rerun(B, mediaBoot(x)) %&gt;% flatten_dbl() sd(thetas_boot) } B_muestras &lt;- data_frame(n_sims = c(5, 25, 50, 100, 200, 400, 1000, 1500, 3000, 5000, 10000, 20000)) %&gt;% mutate(est = map_dbl(n_sims, ~seMediaBoot(x = enlace_muestra$esp_3, B = .))) #&gt; Warning: `data_frame()` is deprecated, use `tibble()`. #&gt; This warning is displayed once per session. B_muestras #&gt; # A tibble: 12 x 2 #&gt; n_sims est #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 5 3.08 #&gt; 2 25 3.20 #&gt; 3 50 3.17 #&gt; 4 100 3.04 #&gt; 5 200 3.36 #&gt; 6 400 3.24 #&gt; 7 1000 3.17 #&gt; 8 1500 3.38 #&gt; 9 3000 3.27 #&gt; 10 5000 3.29 #&gt; 11 10000 3.27 #&gt; 12 20000 3.26 Ejemplo componentes principales: calificaciones en exámenes Los datos marks (Mardia, Kent y Bibby, 1979) contienen los puntajes de 88 estudiantes en 5 pruebas: mecánica, vectores, álgebra, análisis y estadística. Cada renglón corresponde a la calificación de un estudiante en cada prueba. data(marks, package = &quot;ggm&quot;) glimpse(marks) #&gt; Observations: 88 #&gt; Variables: 5 #&gt; $ mechanics &lt;dbl&gt; 77, 63, 75, 55, 63, 53, 51, 59, 62, 64, 52, 55, 50, 65, 31… #&gt; $ vectors &lt;dbl&gt; 82, 78, 73, 72, 63, 61, 67, 70, 60, 72, 64, 67, 50, 63, 55… #&gt; $ algebra &lt;dbl&gt; 67, 80, 71, 63, 65, 72, 65, 68, 58, 60, 60, 59, 64, 58, 60… #&gt; $ analysis &lt;dbl&gt; 67, 70, 66, 70, 70, 64, 65, 62, 62, 62, 63, 62, 55, 56, 57… #&gt; $ statistics &lt;dbl&gt; 81, 81, 81, 68, 63, 73, 68, 56, 70, 45, 54, 44, 63, 37, 73… Entonces un análisis de componentes principales proseguiría como sigue: pc_marks &lt;- princomp(marks) summary(pc_marks) #&gt; Importance of components: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; Standard deviation 26.0600955 14.1291852 10.13060363 9.15149631 5.63935825 #&gt; Proportion of Variance 0.6191097 0.1819910 0.09355915 0.07634838 0.02899179 #&gt; Cumulative Proportion 0.6191097 0.8011007 0.89465983 0.97100821 1.00000000 loadings(pc_marks) #&gt; #&gt; Loadings: #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; mechanics 0.505 0.749 0.301 0.295 #&gt; vectors 0.368 0.207 -0.419 -0.781 0.190 #&gt; algebra 0.346 -0.146 -0.924 #&gt; analysis 0.451 -0.301 -0.594 0.521 0.286 #&gt; statistics 0.535 -0.547 0.600 -0.178 0.151 #&gt; #&gt; Comp.1 Comp.2 Comp.3 Comp.4 Comp.5 #&gt; SS loadings 1.0 1.0 1.0 1.0 1.0 #&gt; Proportion Var 0.2 0.2 0.2 0.2 0.2 #&gt; Cumulative Var 0.2 0.4 0.6 0.8 1.0 plot(pc_marks, type = &quot;lines&quot;) biplot(pc_marks) Los cálculos de un análisis de componentes principales involucran la matriz de covarianzas empírica \\(G\\) (estimaciones plug-in) \\[G_{jk} = \\frac{1}{88}\\sum_{i=1}^88(x_{ij}-\\bar{x_j})(x_{ik}-\\bar{x_k})\\] para \\(j,k=1,2,3,4,5\\), y donde \\(\\bar{x_j} = \\sum_{i=1}^88 x_{ij} / 88\\) (la media de la i-ésima columna). G &lt;- cov(marks) * 87 / 88 G #&gt; mechanics vectors algebra analysis statistics #&gt; mechanics 302.2147 125.59969 100.31599 105.11415 116.15819 #&gt; vectors 125.5997 170.87810 84.18957 93.59711 97.88688 #&gt; algebra 100.3160 84.18957 111.60318 110.83936 120.48567 #&gt; analysis 105.1142 93.59711 110.83936 217.87603 153.76808 #&gt; statistics 116.1582 97.88688 120.48567 153.76808 294.37177 Los pesos y las componentes principales no son mas que los eigenvalores y eigenvectores de la matriz de covarianzas \\(G\\), estos se calculan a través de una serie de de manipulaciones algebraicas que requieren cálculos del orden de p3 (cuando G es una matriz de tamaño p\\(\\times\\)p). eigen_G &lt;- eigen(G) lambda &lt;- eigen_G$values v &lt;- eigen_G$vectors lambda #&gt; [1] 679.12858 199.63388 102.62913 83.74988 31.80236 v #&gt; [,1] [,2] [,3] [,4] [,5] #&gt; [1,] 0.5053373 0.74917585 0.3006046 0.294631757 -0.07873256 #&gt; [2,] 0.3682215 0.20692361 -0.4185473 -0.781332853 -0.18955902 #&gt; [3,] 0.3456083 -0.07622065 -0.1457830 -0.003348995 0.92384059 #&gt; [4,] 0.4512152 -0.30063472 -0.5944322 0.520724416 -0.28551729 #&gt; [5,] 0.5347961 -0.54747360 0.5998773 -0.177611847 -0.15121842 Proponemos el siguiente modelo simple para puntajes correlacionados: \\[\\textbf{x}_i = Q_i \\textbf{v}\\] donde \\(\\textbf{x}_i\\) es la tupla de calificaciones del i-ésimo estudiante, \\(Q_i\\) es un número que representa la habilidad del estudiante y \\(\\textbf{v}\\) es un vector fijo con 5 números que aplica a todos los estudiantes. Si este modelo simple fuera cierto, entonces únicamente el \\(\\hat{\\lambda}_1\\) sería positivo y \\(\\textbf{v} = \\hat{v}_1\\). Sea \\[\\hat{\\theta}=\\sum_{i=1}^5\\hat{\\lambda}_i\\] el modelo propuesto es equivalente a \\(\\hat{\\theta}=1\\), inculso si el modelo es correcto, no esperamos que \\(\\hat{\\theta}\\) sea exactamente uno pues hay ruido en los datos. theta_hat &lt;- lambda[1]/sum(lambda) theta_hat #&gt; [1] 0.6191097 El valor de \\(\\hat{\\theta}\\) mide el porcentaje de la varianza explicada en la primer componente principal, ¿qué tan preciso es \\(\\hat{\\theta}\\)? La complejidad matemática en el cálculo de \\(\\hat{\\theta}\\) es irrelevante siempre y cuando podamos calcular \\(\\hat{\\theta}^*\\) para una muestra bootstrap, en esta caso una muestra bootsrtap es una base de datos de 88 \\(\\times\\) 5 \\(\\textbf{X}^*\\), donde las filas \\(\\bf{x_i}^*\\) de \\(\\textbf{X}^*\\) son una muestra aleatoria de tamaño 88 de la verdadera matriz de datos. pc_boot &lt;- function(){ muestra_boot &lt;- sample_n(marks, size = 88, replace = TRUE) G &lt;- cov(muestra_boot) * 87 / 88 eigen_G &lt;- eigen(G) theta_hat &lt;- eigen_G$values[1] / sum(eigen_G$values) } B &lt;- 1000 thetas_boot &lt;- rerun(B, pc_boot()) %&gt;% flatten_dbl() Veamos un histograma de las replicaciones de \\(\\hat{\\theta}\\): ggplot(data_frame(theta = thetas_boot)) + geom_histogram(aes(x = theta, y = ..density..), binwidth = 0.02, fill = &quot;gray40&quot;) + geom_vline(aes(xintercept = mean(theta)), color = &quot;red&quot;) + labs(x = expression(hat(theta)^&quot;*&quot;), y = &quot;&quot;) Estas tienen un error estándar theta_se &lt;- sd(thetas_boot) theta_se #&gt; [1] 0.04657769 y media mean(thetas_boot) #&gt; [1] 0.6199592 la media de las replicaciones es muy similar a la estimación \\(\\hat{\\theta}\\), esto indica que \\(\\hat{\\theta}\\) es cercano a insesgado. El eigenvetor \\(\\hat{v}_1\\) correspondiente al mayor eigenvalor se conoce como primera componente de \\(G\\), supongamos que deseamos resumir la calificación de los estudiantes mediante un único número, entonces la mejor combinación lineal de los puntajes es \\[y_i = \\sum_{k = 1}^5 \\hat{v}_{1k}x_{ik}\\] esto es, la combinación lineal que utiliza las componentes de \\(\\hat{v}_1\\) como ponderadores. Si queremos un resumen compuesto por dos números \\((y_i,z_i)\\), la segunda combinación lineal debería ser: \\[z_i = \\sum_{k = 1}^5 \\hat{v}_{2k}x_{ik}\\] Las componentes principales \\(\\hat{v}_1\\) y \\(\\hat{v}_2\\) son estadísticos, usa bootstrap para dar una medición de su variabilidad calculando el error estándar de cada una. Referencias "],
["intervalos-de-confianza.html", "6.3 Intervalos de confianza", " 6.3 Intervalos de confianza Hasta ahora hemos discutido la idea detrás del bootstrap y como se puede usar para estimar errores estándar. Comenzamos con el error estándar pues es la manera más común para describir la precisión de una estadística. En términos generales, esperamos que \\(\\bar{x}\\) este a una distancia de \\(\\mu_P\\) menor a un error estándar el 68% del tiempo, y a menos de 2 errores estándar el 95% del tiempo. Estos porcentajes están basados el teorema central del límite que nos dice que bajo ciertas condiciones (bastante generales) de \\(P\\) la distribución de \\(\\bar{x}\\) se aproximará a una distribución normal: \\[\\bar{x} \\overset{\\cdot}{\\sim} N(\\mu_P,\\sigma_P^2/n)\\] Veamos algunos ejemplos de como funciona el Teorema del Límite Central, buscamos ver como se aproxima la distribución muestral de la media (cuando las observaciones provienen de distintas distribuciones) a una Normal conforme aumenta el tamaño de muestra. Para esto, aproximamos la distribución muestral de la media usando simulación de la población. Vale la pena observar que hay distribuciones que requieren un mayor tamaño de muestra \\(n\\) para lograr una buena aproximación (por ejemplo la log-normal), ¿a qué se debe esto? Para la opción de Elecciones tenemos una población de tamaño \\(N=143,437\\) y el objetivo es estimar la media del tamaño de la lista nominal de las casillas (datos de las elecciones presidenciales de 2012). Podemos ver como mejora la aproximación Normal de la distribución muestral conforme aumenta el tamaño de muestra \\(n\\); sin embargo, también sobresale que no es necesario tomar una muestra demasiado grande (\\(n = 60\\) ya es razonable). knitr::include_app(&quot;https://tereom.shinyapps.io/15-TLC/&quot;, height = &quot;1000px&quot;) En lo que sigue veremos distintas maneras de construir intervalos de confianza usando bootstrap. Un intervalo de confianza \\((1-2\\alpha)\\)% para un parámetro \\(\\theta\\) es un intervalo \\((a,b)\\) tal que \\(P(a \\le \\theta \\le b) = 1-2\\alpha\\) para todo \\(\\theta \\in \\Theta\\). Y comenzamos con la versión bootstrap del intervalo más popular. Intervalo Normal con error estándar bootstrap. El intervalo para \\(\\hat{\\theta}\\) con un nivel de confianza de \\(100\\cdot(1-2\\alpha)\\%\\) se define como: \\[(\\hat{\\theta}-z^{(1-\\alpha)}\\cdot \\hat{se}_B, \\hat{\\theta}+z^{(1-\\alpha)}\\cdot \\hat{se})\\]. donde \\(z^{(\\alpha)}\\) denota el percentil \\(100\\cdot \\alpha\\) de una distribución \\(N(0,1)\\). este intervalo está soportado por el Teorema Central del Límite, sin embargo, no es adecuado cuando \\(\\hat{\\theta}\\) no se distribuye aproximadamente Normal. Ejemplo: kurtosis Supongamos que queremos estimar la kurtosis de una base de datos que consta de 799 tiempos de espera entre pulsasiones de un nervio (Cox, Lewis 1976). \\[\\hat{\\theta} = t(P_n) =\\frac{1/n \\sum_{i=1}^n(x_i-\\hat{\\mu})^3}{\\hat{\\sigma}^3}\\] library(ACSWR) data(&quot;nerve&quot;) head(nerve) #&gt; [1] 0.21 0.03 0.05 0.11 0.59 0.06 kurtosis &lt;- function(x){ n &lt;- length(x) 1 / n * sum((x - mean(x)) ^ 3) / sd(x) ^ 3 } theta_hat &lt;- kurtosis(nerve) theta_hat #&gt; [1] 1.757943 kurtosis_boot &lt;- function(x){ x_boot &lt;- sample(x, replace = TRUE) kurtosis(x_boot) } B &lt;- 10000 kurtosis &lt;- rerun(B, kurtosis_boot(nerve)) %&gt;% flatten_dbl() Usando el intervalo normal tenemos: li_normal &lt;- round(theta_hat - 1.96 * sd(kurtosis), 2) ls_normal &lt;- round(theta_hat + 1.96 * sd(kurtosis), 2) c(li_normal, ls_normal) #&gt; [1] 1.44 2.08 Una modificación común del intervalo normal es el intervalo t, estos intervalos son mejores en caso de muestras pequeñas (\\(n\\) chica). Intervalo \\(t\\) con error estándar bootstrap. Para una muestra de tamaño \\(n\\) el intervalo \\(t\\) con un nivel de confianza de \\(100\\cdot(1-2\\alpha)\\%\\) se define como: \\[(\\hat{\\theta}-t^{(1-\\alpha)}_{n-1}\\cdot \\hat{se}_B, \\hat{\\theta}+t^{(1-\\alpha)}_{n-1}\\cdot \\hat{se}_B)\\]. donde \\(t^{(\\alpha)}_{n-1}\\) denota denota el percentil \\(100\\cdot \\alpha\\) de una distribución \\(t\\) con \\(n-1\\) grados de libertad. n_nerve &lt;- length(nerve) li_t &lt;- round(theta_hat + qt(0.025, n_nerve - 1) * sd(kurtosis), 2) ls_t &lt;- round(theta_hat - qt(0.025, n_nerve - 1) * sd(kurtosis), 2) c(li_t, ls_t) #&gt; [1] 1.44 2.08 Los intervalos normales y \\(t\\) se valen de la estimación bootstrap del error estándar; sin embargo, el bootstrap se puede usar para estimar la función de distribución de \\(\\hat{\\theta}\\) por lo que no es necesario hacer supuestos distribucionales para \\(\\hat{\\theta}\\) sino que podemos estimarla como parte del proceso de construir intervalos de confianza. Veamos un histograma de las replicaciones bootstrap de \\(\\hat{\\theta}^*\\) library(gridExtra) nerve_kurtosis &lt;- tibble(kurtosis) hist_nerve &lt;- ggplot(nerve_kurtosis, aes(x = kurtosis)) + geom_histogram(binwidth = 0.05, fill = &quot;gray30&quot;) + geom_vline(xintercept = c(li_normal, ls_normal, theta_hat), color = c(&quot;black&quot;, &quot;black&quot;, &quot;red&quot;), alpha = 0.5) qq_nerve &lt;- ggplot(nerve_kurtosis) + geom_abline(color = &quot;red&quot;, alpha = 0.5) + stat_qq(aes(sample = kurtosis), dparams = list(mean = mean(kurtosis), sd = sd(kurtosis))) grid.arrange(hist_nerve, qq_nerve, ncol = 2, newpage = FALSE) En el ejemplo anterior el supuesto de normalidad parece razonable, veamos como se comparan los cuantiles de la estimación de la distribución de \\(\\hat{\\theta}\\) con los cuantiles de una normal: comma(q_kurt &lt;- quantile(kurtosis, probs = c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975))) comma(qnorm(p = c(0.025, 0.05, 0.1, 0.9, 0.95, 0.975), mean = theta_hat, sd = sd(kurtosis))) #&gt; 2.5% 5% 10% 90% 95% 97.5% #&gt; &quot;1.4&quot; &quot;1.5&quot; &quot;1.5&quot; &quot;2.0&quot; &quot;2.0&quot; &quot;2.1&quot; #&gt; [1] &quot;1.4&quot; &quot;1.5&quot; &quot;1.5&quot; &quot;2.0&quot; &quot;2.0&quot; &quot;2.1&quot; Esto sugiere usar los cuantiles del histograma bootstrap para definir los límites de los intervalos de confianza: Percentiles. Denotemos por \\(G\\) la función de distribución acumulada de \\(\\hat{\\theta}^*\\) el intervalo percentil de \\(1-2\\alpha\\) se define por los percentiles \\(\\alpha\\) y \\(1-\\alpha\\) de \\(G\\) \\[(\\theta^*_{\\%,inf}, \\theta^*_{\\%,sup}) = (G^{-1}(\\alpha), G^{-1}(1-\\alpha))\\] Por definición \\(G^{-1}(\\alpha)=\\hat{\\theta}^*(\\alpha)\\), esto es, el percentil \\(100\\cdot \\alpha\\) de la distribución bootstrap, por lo que podemos escribir el intervalo bootstrap como \\[(\\theta^*_{\\%,inf}, \\theta^*_{\\%,sup})=(\\hat{\\theta}^*(\\alpha),\\hat{\\theta}^*(1-\\alpha))\\] ggplot(arrange(nerve_kurtosis, kurtosis)) + stat_ecdf(aes(x = kurtosis)) + geom_segment(data = data_frame(x = c(-Inf, -Inf, q_kurt[c(1, 6)]), xend = q_kurt[c(1, 6, 1, 6)], y = c(0.025, 0.975, 0, 0), yend = c(0.025, 0.975, 0.025, 0.975)), aes(x = x, xend = xend, y = y, yend = yend), color = &quot;red&quot;, size = 0.4, alpha = 0.5) + labs(x = &quot;Cuantiles muestrales&quot;, y = &quot;ecdf&quot;) Las expresiones de arriba hacen referencia a la situación bootstrap ideal donde el número de replicaciones bootstrap es infinito, en la práctica usamos aproximaciones. Y se procede como sigue: Intervalo percentil: Generamos B muestras bootstrap independientes \\(\\textbf{x}^{*1},..., \\textbf{x}^{*B}\\) y calculamos las replicaciones \\(\\hat{\\theta}^{*b}=s(x^{*b}).\\) Sea \\(\\hat{\\theta}^{*}_B(\\alpha)\\) el percentil \\(100\\cdot\\alpha\\) de la distribución empírica de \\(\\hat{\\theta}^{*}\\), y \\(\\hat{\\theta}^{*}_B(1-\\alpha)\\) el correspondiente al percentil \\(100\\cdot (1-\\alpha)\\), escribimos el intervalo de percentil \\(1-2\\alpha\\) como \\[(\\theta^*_{\\%,inf}, \\theta^*_{\\%,sup})\\approx(\\hat{\\theta}^*_B(\\alpha),\\hat{\\theta}^*_B(1-\\alpha))\\] ls_per &lt;- round(quantile(kurtosis, probs = 0.975), 2) li_per &lt;- round(quantile(kurtosis, probs = 0.025), 2) stringr::str_c(li_normal, ls_normal, sep = &quot;,&quot;) stringr::str_c(li_per, ls_per, sep = &quot;,&quot;) #&gt; [1] &quot;1.44,2.08&quot; #&gt; [1] &quot;1.43,2.07&quot; Si la distribución de \\(\\hat{\\theta}^*\\) es aproximadamente normal, entonces los intervalos normales y de percentiles serán similares. Con el fin de comparar los intervalos creamos un ejemplo de simulación (ejemplo tomado de Efron and Tibshirani (1993)), generamos una muestra de tamaño 10 de una distribución normal estándar, supongamos que el parámetro de interés es \\(e^{\\mu}\\) donde \\(\\mu\\) es la media poblacional. set.seed(137612) x &lt;- rnorm(10) boot_sim_exp &lt;- function(){ x_boot &lt;- sample(x, size = 10, replace = TRUE) exp(mean(x_boot)) } theta_boot &lt;- rerun(1000, boot_sim_exp()) %&gt;% flatten_dbl() theta_boot_df &lt;- data_frame(theta_boot) hist_emu &lt;- ggplot(theta_boot_df, aes(x = theta_boot)) + geom_histogram(fill = &quot;gray30&quot;, binwidth = 0.08) qq_emu &lt;- ggplot(theta_boot_df) + geom_abline(color = &quot;red&quot;, alpha = 0.5) + stat_qq(aes(sample = theta_boot), dparams = list(mean = mean(theta_boot), sd = sd(theta_boot))) grid.arrange(hist_emu, qq_emu, ncol = 2, newpage = FALSE) La distribución empírica de \\(\\hat{\\theta}^*\\) es asimétrica, por lo que no esperamos que coincidan los intervalos. # Normal round(exp(mean(x)) - 1.96 * sd(theta_boot), 2) #&gt; [1] 0.36 round(exp(mean(x)) + 1.96 * sd(theta_boot), 2) #&gt; [1] 1.6 #Percentil round(quantile(theta_boot, prob = 0.025), 2) #&gt; 2.5% #&gt; 0.53 round(quantile(theta_boot, prob = 0.975), 2) #&gt; 97.5% #&gt; 1.79 La inspección del histograma deja claro que la aproximación normal no es conveniente en este caso, veamos que ocurre cuando aplicamos la transformación logarítmica. hist_log &lt;- ggplot(data_frame(theta_boot), aes(x = log(theta_boot))) + geom_histogram(fill = &quot;gray30&quot;, binwidth = 0.08) qq_log &lt;- ggplot(data_frame(theta_boot)) + geom_abline(color = &quot;red&quot;, alpha = 0.5) + stat_qq(aes(sample = log(theta_boot)), dparams = list(mean = mean(log(theta_boot)), sd = sd(log(theta_boot)))) grid.arrange(hist_log, qq_log, ncol = 2, newpage = FALSE) Y los intervalos se comparan: # Normal round(mean(x) - 1.96 * sd(log(theta_boot)), 2) #&gt; [1] -0.63 round(mean(x) + 1.96 * sd(log(theta_boot)), 2) #&gt; [1] 0.58 #Percentil round(quantile(log(theta_boot), prob = 0.025), 2) #&gt; 2.5% #&gt; -0.63 round(quantile(log(theta_boot), prob = 0.975), 2) #&gt; 97.5% #&gt; 0.58 La transformación logarítmica convierte la distribución de \\(\\hat{\\theta}\\) en normal y por tanto los intervalos de \\(\\hat{\\phi}^*=log(\\hat{\\theta}^*)\\) son similares. La forma normal no es sorprendente pues \\(\\hat{\\phi}^*=\\bar{x}^*\\). Si mapeamos los intervalos normales calculados para \\(log(\\hat{\\theta}^*)\\) de regreso a la escala de \\(\\theta\\) obtenemos intervalos similares a los calculados para \\(\\hat{\\theta}^*\\) usando percentiles: exp(round(mean(x) - 1.96 * sd(log(theta_boot)), 2)) #&gt; [1] 0.5325918 exp(round(mean(x) + 1.96 * sd(log(theta_boot)), 2)) #&gt; [1] 1.786038 Podemos ver que el método de aplicar una transformación, calcular intervalos usando la normal y aplicar la transformación inversa para volver a la escala original genera intervalos de confianza atractivos, el problema con este método es que requiere que conozcamos la transformación adecuada para cada parámetro. Por otra parte, podemos pensar en el método del percentil como un algoritmo que incorpora la transformación de manera automática. Lema. Supongamos que la transformación \\(\\hat{\\phi}=m(\\hat{\\theta})\\) normaliza la distribución de \\(\\hat{\\theta}\\) de manera perfecta, \\[\\hat{\\phi} \\approx N(\\phi, c^2)\\] para alguna desviación estándar \\(c\\). Entonces el intervalo de percentil basado en \\(\\hat{\\theta}\\) es igual a \\[(m^{-1} (\\hat{\\phi}-z^{(1-\\alpha)}c), m^{-1}(\\hat{\\phi}-z^{(\\alpha)}c))\\] Se dice que el intervalo de confianza de percentiles es invariante a transformaciones. Existen otras alternativas al método del percentil y cubren otras fallas del intervalo normal. Por ejemplo, hay ocasiones en que \\(\\hat{\\theta}\\) tiene una distribución normal sesgada: \\[\\hat{\\theta} \\approx N(\\theta + sesgo, \\hat{se}^2)\\] en este caso no existe una transformación \\(m(\\theta)\\) que arregle el intervalo. Intervalos acelerados y corregidos por sesgo. Esta es una versión mejorada del intervalo de percentil, la denotamos \\(BC_{a}\\) (bias-corrected and accelerated). Usaremos un ejemplo de Efron and Tibshirani (1993), los datos constan de los resultados en dos pruebas espaciales de 26 niños con discapacidad neurológico. Supongamos que queremos calcular un intervalo de confianza de 90% para \\(\\theta=var(A)\\). El estimador plugin es: \\[\\hat{\\theta}=\\sum_{i=1}^n(A_i-\\bar{A})^2/n\\] notemos que el estimador plug-in es ligeramente menor que el estimador usual insesgado: \\[\\hat{\\theta}=\\sum_{i=1}^n(A_i-\\bar{A})^2/(n-1)\\] library(bootstrap) spatial #&gt; A B #&gt; V1 48 42 #&gt; V2 36 33 #&gt; V3 20 16 #&gt; V4 29 39 #&gt; V5 42 38 #&gt; V6 42 36 #&gt; V7 20 15 #&gt; V8 42 33 #&gt; V9 22 20 #&gt; V10 41 43 #&gt; V11 45 34 #&gt; V12 14 22 #&gt; V13 6 7 #&gt; V14 0 15 #&gt; V15 33 34 #&gt; V16 28 29 #&gt; V17 34 41 #&gt; V18 4 13 #&gt; V19 32 38 #&gt; V20 24 25 #&gt; V21 47 27 #&gt; V22 41 41 #&gt; V23 24 28 #&gt; V24 26 14 #&gt; V25 30 28 #&gt; V26 41 40 ggplot(spatial) + geom_point(aes(A, B)) El estimador plug-in de \\(\\theta\\) es sum((spatial$A - mean(spatial$A)) ^ 2) / nrow(spatial) #&gt; [1] 171.534 Notemos que es ligeramente menor que el estimador insesgado: sum((spatial$A - mean(spatial$A)) ^ 2) / (nrow(spatial) - 1) #&gt; [1] 178.3954 El método \\(BC_{a}\\) corrige el sesgo de manera automática, lo cuál es una de sus prinicipales ventajas comparado con el método del percentil. Los extremos en los intervalos \\(BC_{a}\\) están dados por percentiles de la distribución bootstrap, los percentiles usados dependen de dos números \\(\\hat{a}\\) y \\(\\hat{z}_0\\), que se denominan la aceleración y la corrección del sesgo: \\[BC_a : (\\hat{\\theta}_{inf}, \\hat{\\theta}_{sup})=(\\hat{\\theta}^*(\\alpha_1), \\hat{\\theta}^*(\\alpha_2))\\] donde \\[\\alpha_1= \\Phi\\bigg(\\hat{z}_0 + \\frac{\\hat{z}_0 + z^{(\\alpha)}}{1- \\hat{a}(\\hat{z}_0 + z^{(\\alpha)})}\\bigg)\\] \\[\\alpha_2= \\Phi\\bigg(\\hat{z}_0 + \\frac{\\hat{z}_0 + z^{(1-\\alpha)}}{1- \\hat{a}(\\hat{z}_0 + z^{(1-\\alpha)})}\\bigg)\\] y \\(\\Phi\\) es la función de distribución acumulada de la distribución normal estándar y \\(z^{\\alpha}\\) es el percentil \\(100 \\cdot \\alpha\\) de una distribución normal estándar. Notemos que si \\(\\hat{a}\\) y \\(\\hat{z}_0\\) son cero entonces \\(\\alpha_1=\\alpha\\) y \\(\\alpha_2=1-\\alpha\\), obteniendo así los intervalos de percentiles. El valor de la corrección por sesgo \\(\\hat{z}_0\\) se obtiene de la propoción de de replicaciones bootstrap menores a la estimación original \\(\\hat{\\theta}\\), \\[z_0=\\Phi^{-1}\\bigg(\\frac{\\#\\{\\hat{\\theta}^*(b) &lt; \\hat{\\theta} \\} }{B} \\bigg)\\] a grandes razgos \\(\\hat{z}_0\\) mide la mediana del sesgo de \\(\\hat{\\theta}^*\\), esto es, la discrepancia entre la mediana de \\(\\hat{\\theta}^*\\) y \\(\\hat{\\theta}\\) en unidades normales. Por su parte la aceleración \\(\\hat{a}\\) se refiere a la tasa de cambio del error estándar de \\(\\hat{\\theta}\\) respecto al verdadero valor del parámetro \\(\\theta\\). La aproximación estándar usual \\(\\hat{\\theta} \\approx N(\\theta, se^2)\\) supone que el error estándar de \\(\\hat{\\theta}\\) es el mismo para toda \\(\\hat{\\theta}\\), esto puede ser poco realista, en nuestro ejemplo, donde \\(\\hat{\\theta}\\) es la varianza si los datos provienen de una normal \\(se(\\hat{\\theta})\\) depende de \\(\\theta\\). Una manera de calcular \\(\\hat{a}\\) es \\[\\hat{a}=\\frac{\\sum_{i=1}^n (\\hat{\\theta}(\\cdot) - \\hat{\\theta}(i))^3}{6\\{\\sum_{i=1}^n (\\hat{\\theta}(\\cdot) - \\hat{\\theta}(i))^2\\}^{3/2}}\\] Los intervalos \\(BC_{a}\\) tienen 2 ventajas teóricas: Respetan transformaciones, esto nos dice que los extremos del intervalo se transforman de manera adecuada si cambiamos el parámetro de interés por una función del mismo. Su exactitud, los intervalos \\(BC_{a}\\) tienen precisión de segundo orden, esto es, los errores de cobertura se van a cero a una tasa de 1/n. Los intervalos \\(BC_{a}\\) están implementados en el paquete boot (boot.ci()) y en el paquete bootstrap (bcanon()). La desventaja de los intervalos \\(BC_{a}\\) es que requieren intenso cómputo estadístico, de acuerdo a Efron and Tibshirani (1993) al menos \\(B= 1000\\) replicaciones son necesarias para reducir el error de muestreo. Ante esto surgen los intervalos ABC (approximate bootstrap confidence intervals), que es un método para aproximar \\(BC_{a}\\) analíticamente (usando expansiones de Taylor), estos intervalos requieren que la estadística \\(\\hat{\\theta} = s(x)\\) este definida de manera suave sobre x (la mediana, por ejemplo, no es suave). Usando la implementación del paquete bootstrap: var_sesgada &lt;- function(x) sum((x - mean(x)) ^ 2) / length(x) bcanon(x = spatial[, 1], nboot = 2000, theta = var_sesgada, alpha = c(0.025, 0.975)) #&gt; $confpoints #&gt; alpha bca point #&gt; [1,] 0.025 103.8402 #&gt; [2,] 0.975 274.0533 #&gt; #&gt; $z0 #&gt; [1] 0.1383042 #&gt; #&gt; $acc #&gt; [1] 0.06124012 #&gt; #&gt; $u #&gt; [1] 164.3936 176.7200 174.5184 178.3776 172.0544 172.0544 174.5184 172.0544 #&gt; [9] 175.9584 173.0400 168.5984 168.2016 155.1200 141.8144 177.9296 178.2816 #&gt; [17] 177.6096 151.0176 178.1664 177.0656 165.8784 173.0400 177.0656 177.8400 #&gt; [25] 178.3904 173.0400 #&gt; #&gt; $call #&gt; bcanon(x = spatial[, 1], nboot = 2000, theta = var_sesgada, alpha = c(0.025, #&gt; 0.975)) Comapara el intervalo anterior con los intervalos normal y de percentiles. Otros intervalos basados en bootstrap incluyen los intervalos pivotales y los intervalos bootstrap-t. Sin embargo, BC y ABC son mejores alternativas. Intervalos pivotales. Sea \\(\\theta=s(P)\\) y \\(\\hat{\\theta}=s(P_n)\\) definimos el pivote \\(R=\\hat{\\theta}-\\theta\\). Sea \\(H(r)\\) la función de distribución acumulada del pivote: \\[H(r) = P(R&lt;r)\\] Definimos \\(C_n^*=(a,b)\\) donde: \\[a=\\hat{\\theta}-H^{-1}(1-\\alpha), b=\\hat{\\theta}-H^{-1}(\\alpha)\\] \\(C_n^*\\) es un intervalo de confianza de \\(1-2\\alpha\\) para \\(\\theta\\); sin embargo, \\(a\\) y \\(b\\) dependen de la distribución desconocida \\(H\\), la podemos estimar usando bootstrap: \\[\\hat{H}(r)=\\frac{1}{B}\\sum_{b=1}^B I(R^*_b \\le r)\\] y obtenemos \\[C_n=(2\\hat{\\theta} - \\hat{\\theta}^*_{1-\\alpha}, 2\\hat{\\theta} + \\hat{\\theta}^*_{1-\\alpha})\\] Exactitud en intervalos de confianza. Un intervalo de \\(95%\\) de confianza exacto no captura el verdadero valor \\(2.5%\\) de las veces, en cada lado. Un intervalo que sub-cubre un lado y sobre-cubre el otro es sesgado. Los intervalos estándar y de percentiles tienen exactitud de primer orden: los errores de cobertura se van a cero a una tasa de \\(1/\\sqrt{n}\\). Suelen ser demasido estrechos resultando en cobertura real menor a la nominal, sobretodo con muestras chicas. Los intervalos \\(BC_a\\) tienen exactitud de segundo orden: los errores de cobertura se van a cero a una tasa de \\(1/n\\). A pesar de que los intervalos \\(BC_a\\) pueden ser superiores a los intervalos normales y de percentiles, en la práctica es más común utilizar intervalos normales o de percentiles pues su implementación es más sencilla y son adecuados para un gran número de casos. Referencias "],
["más-alla-de-muestras-aleatorias-simples.html", "6.4 Más alla de muestras aleatorias simples", " 6.4 Más alla de muestras aleatorias simples Introdujimos el bootstrap en el contexto de muestras aleatorias, esto es, suponiendo que las observaciones son independientes; en este escenario basta con aproximar la distribución desconocida \\(P\\) usando la dsitribución empírica \\(P_n\\), y el cálculo de los estadísticos es inmediato. Hay casos en los que el mecanismo que generó los datos es más complicado, por ejemplo, cuando tenemos dos muestras, en diseños de encuestas complejas o en series de tiempo. Ejemplo: Dos muestras En el ejemplo de experimentos clínicos de aspirina y ataques de de corazón, podemos pensar el modelo probabilístico \\(P\\) como compuesto por dos distribuciones de probabilidad \\(G\\) y \\(Q\\) una correspondiente al grupo control y otra al grupo de tratamiento, entonces las observaciones de cada grupo provienen de distribuciones distintas y el método bootstrap debe tomar en cuenta esto al generar las muestras, en este caso implica seleccionar muesreas de manera independiente dentro de cada grupo. Ejemplo: Bootstrap en muestreo de encuestas La necesidad de estimaciones confiables junto con el uso eficiente de recursos conllevan a diseños de muestras complejas. Estos diseños típicamente usan las siguientes técnicas: muestreo sin reemplazo de una población finita, muestreo sistemático, estratificación, conglomerados, ajustes a no-respuesta, postestratificación. Como consecuencia, los valores de la muestra suelen no ser independientes. La complejidad de los diseños de encuestas conlleva a que el cálculo de errores estándar sea muy complicado, para atacar este problema hay dos técnicas básicas: 1) un enfoque analítico usando linearización, 2) métodos de remuestreo como bootstrap. El incremento en el poder de cómputo ha favorecido los métodos de remuestreo pues la linearización requiere del desarrollo de una fórmula para cada estimación y supuestos adicionales para simplificar. En 1988 Rao and Wu (1988) propusieron un método de bootstrap para diseños estratificados multietápicos con reemplazo de UPMs que describimos a continuación. ENIGH. Usaremos como ejemplo la Encuesta Nacional de Ingresos y Gastos de los Hogares, ENIGH 2018 (INEGI 2018), esta encuesta usa un diseño de conglomerados estratificado. Antes de proceder a bootstrap debemos entender como se seleccionaron los datos, esto es, el diseño de la muestra: Unidad primaria de muestreo (UPM). Las UPMs están constituidas por agrupaciones de viviendas. Se les denomina unidades primarias pues corresponden a la primera etapa de selección, las unidades secundarias (USMs) serían los hogares. Estratificación. Los estratos se construyen en base a estado, ámbito (urbano, complemento urbano, rural), características sociodemográficas de los habitantes de las viviendas, características físicas y equipamiento. El proceso de estratificación resulta en 888 subestratos en todo el ámbito nacional. La selección de la muestra es independiente para cada estrato, y una vez que se obtiene la muestra se calculan los factores de expansión que reflejan las distintas probabilidades de selección. Después se llevan a cabo ajustes por no respuesta y por proyección (calibración), esta última busca que distintos dominios de la muestra coincidan con la proyección de población de INEGI. library(usethis) use_zip(&quot;https://www.inegi.org.mx/contenidos/programas/enigh/nc/2018/datosabiertos/conjunto_de_datos_enigh_2018_ns_csv.zip&quot;, &quot;data&quot;) library(here) concentrado_hogar &lt;- read_csv(here(&quot;data&quot;, &quot;conjunto_de_datos_enigh_2018_ns_csv&quot;, &quot;conjunto_de_datos_concentradohogar_enigh_2018_ns&quot;, &quot;conjunto_de_datos&quot;, &quot;conjunto_de_datos_concentradohogar_enigh_2018_ns.csv&quot;)) glimpse(concentrado_hogar) #&gt; Observations: 74,647 #&gt; Variables: 126 #&gt; $ folioviv &lt;dbl&gt; 100013601, 100013602, 100013603, 100013604, 100013606, 100… #&gt; $ foliohog &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… #&gt; $ ubica_geo &lt;dbl&gt; 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001, 1001… #&gt; $ tam_loc &lt;dbl&gt; 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1… #&gt; $ est_socio &lt;dbl&gt; 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3… #&gt; $ est_dis &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2… #&gt; $ upm &lt;dbl&gt; 1, 1, 1, 1, 1, 2, 2, 2, 2, 3, 3, 3, 3, 4, 4, 4, 4, 4, 5, 5… #&gt; $ factor &lt;dbl&gt; 175, 175, 175, 175, 175, 189, 189, 189, 189, 186, 186, 186… #&gt; $ clase_hog &lt;dbl&gt; 2, 2, 2, 2, 2, 2, 1, 2, 2, 3, 2, 1, 2, 2, 2, 2, 3, 1, 3, 2… #&gt; $ sexo_jefe &lt;dbl&gt; 1, 1, 1, 2, 2, 2, 2, 1, 1, 1, 2, 1, 1, 1, 1, 1, 1, 1, 1, 1… #&gt; $ edad_jefe &lt;dbl&gt; 74, 48, 39, 70, 51, 41, 57, 53, 30, 69, 76, 77, 70, 29, 68… #&gt; $ educa_jefe &lt;dbl&gt; 4, 11, 10, 8, 4, 11, 9, 11, 6, 4, 3, 4, 6, 6, 9, 7, 6, 8, … #&gt; $ tot_integ &lt;dbl&gt; 3, 5, 2, 2, 4, 4, 1, 2, 3, 4, 2, 1, 2, 4, 4, 2, 5, 1, 4, 3… #&gt; $ hombres &lt;dbl&gt; 2, 2, 1, 1, 1, 2, 0, 1, 2, 4, 0, 1, 1, 2, 3, 1, 2, 1, 2, 1… #&gt; $ mujeres &lt;dbl&gt; 1, 3, 1, 1, 3, 2, 1, 1, 1, 0, 2, 0, 1, 2, 1, 1, 3, 0, 2, 2… #&gt; $ mayores &lt;dbl&gt; 3, 5, 2, 2, 3, 4, 1, 2, 2, 3, 2, 1, 2, 2, 4, 2, 5, 1, 3, 3… #&gt; $ menores &lt;dbl&gt; 0, 0, 0, 0, 1, 0, 0, 0, 1, 1, 0, 0, 0, 2, 0, 0, 0, 0, 1, 0… #&gt; $ p12_64 &lt;dbl&gt; 1, 5, 2, 1, 3, 4, 1, 2, 2, 2, 1, 0, 0, 2, 2, 0, 5, 1, 3, 2… #&gt; $ p65mas &lt;dbl&gt; 2, 0, 0, 1, 0, 0, 0, 0, 0, 1, 1, 1, 2, 0, 2, 2, 0, 0, 0, 1… #&gt; $ ocupados &lt;dbl&gt; 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 0, 4, 1, 2, 2… #&gt; $ percep_ing &lt;dbl&gt; 3, 5, 2, 2, 2, 2, 1, 2, 2, 4, 2, 1, 2, 1, 4, 2, 4, 1, 2, 3… #&gt; $ perc_ocupa &lt;dbl&gt; 2, 2, 2, 0, 2, 2, 1, 2, 2, 2, 1, 1, 1, 1, 2, 0, 4, 1, 2, 2… #&gt; $ ing_cor &lt;dbl&gt; 76403.70, 42987.73, 580697.74, 46252.71, 53837.09, 237742.… #&gt; $ ingtrab &lt;dbl&gt; 53114.74, 15235.06, 141885.21, 0.00, 43229.49, 129836.03, … #&gt; $ trabajo &lt;dbl&gt; 53114.74, 0.00, 141885.21, 0.00, 8852.45, 129836.03, 23606… #&gt; $ sueldos &lt;dbl&gt; 53114.74, 0.00, 133770.48, 0.00, 8852.45, 95901.63, 23606.… #&gt; $ horas_extr &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ comisiones &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 22131.14, 0.00, 0.00, 0.00, … #&gt; $ aguinaldo &lt;dbl&gt; 0.00, 0.00, 3934.42, 0.00, 0.00, 11803.26, 0.00, 22131.14,… #&gt; $ indemtrab &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ otra_rem &lt;dbl&gt; 0.00, 0.00, 4180.31, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… #&gt; $ remu_espec &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ negocio &lt;dbl&gt; 0.00, 13759.66, 0.00, 0.00, 34377.04, 0.00, 0.00, 0.00, 0.… #&gt; $ noagrop &lt;dbl&gt; 0.00, 13759.66, 0.00, 0.00, 34377.04, 0.00, 0.00, 0.00, 0.… #&gt; $ industria &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ comercio &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 34377.04, 0.00, 0.00, 0.00, 0.00, … #&gt; $ servicios &lt;dbl&gt; 0.00, 13759.66, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, … #&gt; $ agrope &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ agricolas &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ pecuarios &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ reproducc &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ pesca &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ otros_trab &lt;dbl&gt; 0.0, 1475.4, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, … #&gt; $ rentas &lt;dbl&gt; 0.00, 0.00, 29508.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, … #&gt; $ utilidad &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ arrenda &lt;dbl&gt; 0.00, 0.00, 29508.19, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, … #&gt; $ transfer &lt;dbl&gt; 11288.96, 3752.67, 391304.34, 34252.71, 107.60, 89906.51, … #&gt; $ jubilacion &lt;dbl&gt; 9147.54, 0.00, 0.00, 23606.55, 0.00, 23606.55, 0.00, 0.00,… #&gt; $ becas &lt;dbl&gt; 0.0, 491.8, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0… #&gt; $ donativos &lt;dbl&gt; 0.00, 147.54, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 23… #&gt; $ remesas &lt;dbl&gt; 0.00, 98.36, 0.00, 5901.63, 0.00, 0.00, 0.00, 0.00, 0.00, … #&gt; $ bene_gob &lt;dbl&gt; 1622.95, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… #&gt; $ transf_hog &lt;dbl&gt; 0.00, 3014.97, 0.00, 0.00, 107.60, 61714.26, 0.00, 0.00, 0… #&gt; $ trans_inst &lt;dbl&gt; 518.47, 0.00, 391304.34, 4744.53, 0.00, 4585.70, 0.00, 0.0… #&gt; $ estim_alqu &lt;dbl&gt; 12000.00, 24000.00, 18000.00, 12000.00, 10500.00, 18000.00… #&gt; $ otros_ing &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ gasto_mon &lt;dbl&gt; 18551.47, 55470.99, 103106.89, 19340.06, 13605.03, 33627.7… #&gt; $ alimentos &lt;dbl&gt; 5618.47, 20930.29, 37594.06, 2892.84, 7367.09, 0.00, 11455… #&gt; $ ali_dentro &lt;dbl&gt; 4075.63, 8587.46, 25251.25, 2892.84, 4795.67, 0.00, 8344.1… #&gt; $ cereales &lt;dbl&gt; 964.25, 2689.65, 3728.53, 385.71, 257.14, 0.00, 437.13, 10… #&gt; $ carnes &lt;dbl&gt; 0.00, 1401.41, 2828.56, 2121.42, 2931.41, 0.00, 1787.13, 3… #&gt; $ pescado &lt;dbl&gt; 745.71, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.… #&gt; $ leche &lt;dbl&gt; 0.00, 443.55, 4345.70, 0.00, 0.00, 0.00, 2841.41, 1491.40,… #&gt; $ huevo &lt;dbl&gt; 719.98, 0.00, 411.42, 0.00, 0.00, 0.00, 308.57, 629.99, 30… #&gt; $ aceites &lt;dbl&gt; 0.00, 0.00, 1928.57, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… #&gt; $ tuberculo &lt;dbl&gt; 0.00, 257.14, 385.71, 0.00, 128.57, 0.00, 231.42, 411.42, … #&gt; $ verduras &lt;dbl&gt; 745.70, 1893.29, 2635.66, 0.00, 835.70, 0.00, 861.38, 1028… #&gt; $ frutas &lt;dbl&gt; 0.00, 533.16, 1864.27, 0.00, 0.00, 0.00, 244.27, 809.98, 0… #&gt; $ azucar &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 257.14, 0.00, 0.00, 0.… #&gt; $ cafe &lt;dbl&gt; 0.00, 462.85, 1414.28, 0.00, 0.00, 0.00, 964.28, 0.00, 0.0… #&gt; $ especias &lt;dbl&gt; 0.00, 167.14, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.… #&gt; $ otros_alim &lt;dbl&gt; 0.00, 392.13, 2545.71, 385.71, 514.28, 0.00, 0.00, 2699.99… #&gt; $ bebidas &lt;dbl&gt; 899.99, 347.14, 3162.84, 0.00, 128.57, 0.00, 411.42, 385.7… #&gt; $ ali_fuera &lt;dbl&gt; 771.42, 12342.83, 12342.81, 0.00, 2571.42, 0.00, 1928.57, … #&gt; $ tabaco &lt;dbl&gt; 771.42, 0.00, 0.00, 0.00, 0.00, 0.00, 1182.84, 0.00, 0.00,… #&gt; $ vesti_calz &lt;dbl&gt; 0.00, 401.06, 2015.21, 97.82, 0.00, 0.00, 0.00, 1565.20, 5… #&gt; $ vestido &lt;dbl&gt; 0.00, 224.98, 2015.21, 97.82, 0.00, 0.00, 0.00, 293.47, 53… #&gt; $ calzado &lt;dbl&gt; 0.00, 176.08, 0.00, 0.00, 0.00, 0.00, 0.00, 1271.73, 0.00,… #&gt; $ vivienda &lt;dbl&gt; 3912.00, 2495.00, 4475.00, 1458.00, 300.00, 2801.00, 4405.… #&gt; $ alquiler &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 3900.00, 1… #&gt; $ pred_cons &lt;dbl&gt; 0.00, 1250.00, 1250.00, 0.00, 0.00, 140.00, 250.00, 0.00, … #&gt; $ agua &lt;dbl&gt; 312.00, 750.00, 750.00, 600.00, 0.00, 741.00, 630.00, 690.… #&gt; $ energia &lt;dbl&gt; 3600.00, 495.00, 2475.00, 858.00, 300.00, 1920.00, 3525.00… #&gt; $ limpieza &lt;dbl&gt; 522.00, 412.16, 3318.26, 5514.00, 3300.00, 5682.00, 2496.6… #&gt; $ cuidados &lt;dbl&gt; 522.00, 375.00, 2340.00, 5514.00, 3300.00, 5682.00, 2301.0… #&gt; $ utensilios &lt;dbl&gt; 0.00, 37.16, 978.26, 0.00, 0.00, 0.00, 195.65, 391.30, 0.0… #&gt; $ enseres &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 5901.63, 0.00, 0… #&gt; $ salud &lt;dbl&gt; 0.00, 1348.99, 28858.68, 322.82, 56.73, 0.00, 4695.64, 0.0… #&gt; $ atenc_ambu &lt;dbl&gt; 0.00, 1007.59, 28858.68, 0.00, 56.73, 0.00, 4695.64, 0.00,… #&gt; $ hospital &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 8217… #&gt; $ medicinas &lt;dbl&gt; 0.00, 341.40, 0.00, 322.82, 0.00, 0.00, 0.00, 0.00, 0.00, … #&gt; $ transporte &lt;dbl&gt; 8400.00, 7628.56, 12325.68, 7350.00, 600.00, 18235.70, 580… #&gt; $ publico &lt;dbl&gt; 0.00, 578.56, 4255.68, 0.00, 0.00, 1285.70, 0.00, 0.00, 0.… #&gt; $ foraneo &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 590.16, 0.00, 0.00, 70… #&gt; $ adqui_vehi &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ mantenim &lt;dbl&gt; 7200.00, 3600.00, 4500.00, 6000.00, 0.00, 13200.00, 4253.1… #&gt; $ refaccion &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 2213.11, 0.00, 0.00, 0… #&gt; $ combus &lt;dbl&gt; 7200.00, 3600.00, 4500.00, 6000.00, 0.00, 13200.00, 2040.0… #&gt; $ comunica &lt;dbl&gt; 1200.00, 3450.00, 3570.00, 1350.00, 600.00, 3750.00, 960.0… #&gt; $ educa_espa &lt;dbl&gt; 0.00, 17567.05, 0.00, 639.34, 0.00, 1800.00, 627.00, 3600.… #&gt; $ educacion &lt;dbl&gt; 0.00, 8547.39, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… #&gt; $ esparci &lt;dbl&gt; 0.00, 167.21, 0.00, 639.34, 0.00, 1800.00, 627.00, 3600.00… #&gt; $ paq_turist &lt;dbl&gt; 0.00, 8852.45, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0… #&gt; $ personales &lt;dbl&gt; 99.00, 4663.29, 8520.00, 1065.24, 1686.13, 5109.00, 3335.6… #&gt; $ cuida_pers &lt;dbl&gt; 99.00, 1497.00, 8520.00, 180.00, 1647.00, 4509.00, 1500.00… #&gt; $ acces_pers &lt;dbl&gt; 0.00, 166.29, 0.00, 0.00, 39.13, 0.00, 0.00, 0.00, 0.00, 0… #&gt; $ otros_gas &lt;dbl&gt; 0.00, 3000.00, 0.00, 885.24, 0.00, 600.00, 1835.65, 0.00, … #&gt; $ transf_gas &lt;dbl&gt; 0.00, 24.59, 6000.00, 0.00, 295.08, 0.00, 491.80, 23606.55… #&gt; $ percep_tot &lt;dbl&gt; 0.00, 6073.09, 3857.14, 1380.55, 0.00, 1928.57, 489.13, 20… #&gt; $ retiro_inv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ prestamos &lt;dbl&gt; 0.00, 7.37, 0.00, 737.70, 0.00, 0.00, 0.00, 0.00, 491.80, … #&gt; $ otras_perc &lt;dbl&gt; 0.00, 462.28, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.… #&gt; $ ero_nm_viv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ ero_nm_hog &lt;dbl&gt; 0.00, 5603.44, 3857.14, 642.85, 0.00, 1928.57, 489.13, 205… #&gt; $ erogac_tot &lt;dbl&gt; 0.00, 9009.82, 81147.53, 0.00, 0.00, 14754.09, 0.00, 58229… #&gt; $ cuota_viv &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 12000, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, … #&gt; $ mater_serv &lt;dbl&gt; 0.00, 147.54, 0.00, 0.00, 0.00, 0.00, 0.00, 7868.85, 0.00,… #&gt; $ material &lt;dbl&gt; 0.00, 147.54, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.… #&gt; $ servicio &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 7868.85, 0.00, 0… #&gt; $ deposito &lt;dbl&gt; 0.00, 9.83, 66393.44, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, … #&gt; $ prest_terc &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ pago_tarje &lt;dbl&gt; 0.00, 8852.45, 0.00, 0.00, 0.00, 14754.09, 0.00, 0.00, 0.0… #&gt; $ deudas &lt;dbl&gt; 0.00, 0.00, 14754.09, 0.00, 0.00, 0.00, 0.00, 38360.65, 0.… #&gt; $ balance &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… #&gt; $ otras_erog &lt;dbl&gt; 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00, 0.00… #&gt; $ smg &lt;dbl&gt; 7952.4, 7952.4, 7952.4, 7952.4, 7952.4, 7952.4, 7952.4, 79… # seleccionar variable de ingreso corriente hogar &lt;- concentrado_hogar %&gt;% mutate( upm = as.integer(upm), jefe_hombre = sexo_jefe == 1, edo = str_sub(ubica_geo, 1, 2), jefa_50 = (sexo_jefe == 2) &amp; (edad_jefe &gt; 50) ) %&gt;% select(folioviv, foliohog, est_dis, upm, factor, ing_cor, sexo_jefe, edad_jefe, edo, jefa_50) %&gt;% group_by(est_dis) %&gt;% mutate(n = n_distinct(upm)) %&gt;% # número de upms por estrato ungroup() hogar #&gt; # A tibble: 74,647 x 11 #&gt; folioviv foliohog est_dis upm factor ing_cor sexo_jefe edad_jefe edo #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; #&gt; 1 1.00e8 1 2 1 175 76404. 1 74 10 #&gt; 2 1.00e8 1 2 1 175 42988. 1 48 10 #&gt; 3 1.00e8 1 2 1 175 580698. 1 39 10 #&gt; 4 1.00e8 1 2 1 175 46253. 2 70 10 #&gt; 5 1.00e8 1 2 1 175 53837. 2 51 10 #&gt; 6 1.00e8 1 2 2 189 237743. 2 41 10 #&gt; 7 1.00e8 1 2 2 189 32607. 2 57 10 #&gt; 8 1.00e8 1 2 2 189 169918. 1 53 10 #&gt; 9 1.00e8 1 2 2 189 17311. 1 30 10 #&gt; 10 1.00e8 1 2 3 186 120488. 1 69 10 #&gt; # … with 74,637 more rows, and 2 more variables: jefa_50 &lt;lgl&gt;, n &lt;int&gt; Para el cálculo de estadísticos debemos usar los factores de expansión, por ejemplo el ingreso trimestral total sería: sum(hogar$factor * hogar$ing_cor / 1000) #&gt; [1] 1723700566 y ingreso trimestral medio (miles pesos) sum(hogar$factor * hogar$ing_cor / 1000) / sum(hogar$factor) #&gt; [1] 49.61029 La estimación del error estándar, por otro lado, no es sencilla y requiere usar aproximaciones, en la metodología de INEGI proponen una aproximación con series de Taylor. Figure 6.1: Extracto de estimación de errores de muestreo, ENIGH 2018. Veamos ahora como calcular el error estándar siguiendo el bootstrap de Rao y Wu: En cada estrato se seleccionan con reemplazo \\(m_h\\) UPMs de las \\(n_h\\) de la muestra original. Denotamos por \\(m_{hi}^*\\) el número de veces que se seleccionó la UPM \\(i\\) en el estrato \\(h\\) (de tal manera que \\(\\sum m_{hi}^*=m_h\\)). Creamos una replicación del ponderador correspondiente a la \\(k\\)-ésima unidad (USM) como: \\[d_k^*=d_k \\bigg[\\bigg(1-\\sqrt{\\frac{m_h}{n_h - 1}}\\bigg) + \\bigg(\\sqrt{\\frac{m_h}{n_h - 1}}\\frac{n_h}{m_h}m_{h}^*\\bigg)\\bigg]\\] donde \\(d_k\\) es el inverso de la probabilidad de selección. Si \\(m_h&lt;(n_h -1)\\) todos los pesos definidos de esta manera serán no negativos. Calculamos el peso final \\(w_k^*\\) aplicando a \\(d_k^*\\) los mismos ajustes que se hicieron a los ponderadores originales. Calculamos el estadístico de interés \\(\\hat{\\theta}\\) usando los ponderadores \\(w_k^*\\) en lugar de los originales \\(w_k\\). Repetimos los pasos 1 y 2 \\(B\\) veces para obtener \\(\\hat{\\theta}^{*1},\\hat{\\theta}^{*2},...,\\hat{\\theta}^{*B}\\). Calculamos el error estándar como: \\[\\hat{se}_B = \\bigg\\{\\frac{\\sum_{b=1}^B[\\hat{\\theta}^*(b)-\\hat{\\theta}^*(\\cdot)]^2 }{B}\\bigg\\}^{1/2}\\] Podemos elegir cualquier valor de \\(m_h \\geq 1\\), el más sencillo es elegir \\(m_h=n_h-1\\), en este caso: \\[d_k^*=d_k \\frac{n_h}{n_h-1}m_{hi}^*\\] en este escenario las unidades que no se incluyen en la muestra tienen un valor de cero como ponderador. Si elegimos \\(n_h \\ne n_h-1\\) las unidades que no están en la muestra tienen ponderador distinto a cero, si \\(m_h=n_h\\) el ponderador podría tomar valores negativos. Implementemos el bootstrap de Rao y Wu a la ENIGH, usaremos \\(m_h=n_h-1\\) # creamos una tabla con los estratos y upms est_upm &lt;- hogar %&gt;% distinct(est_dis, upm, n) hogar_factor &lt;- est_upm %&gt;% split(.$est_dis) %&gt;% # dentro de cada estrato tomamos muestra (n_h-1) map_df(~sample_n(., size = first(.$n) - 1, replace = TRUE)) %&gt;% add_count(upm, name = &quot;m_hi&quot;) %&gt;% # calculamos m_hi* left_join(hogar, by = c(&quot;est_dis&quot;, &quot;upm&quot;, &quot;n&quot;)) %&gt;% mutate(factor_b = factor * m_hi * n / (n - 1)) # unimos los pasos anteriores en una función para replicar en cada muestra bootstrap svy_boot &lt;- function(est_upm, hogar){ m_hi &lt;- est_upm %&gt;% split(.$est_dis) %&gt;% map(~sample(.$upm, size = first(.$n) - 1, replace = TRUE)) %&gt;% flatten_int() %&gt;% plyr::count() %&gt;% select(upm = x, m_h = freq) m_hi %&gt;% left_join(hogar, by = c(&quot;upm&quot;)) %&gt;% mutate(factor_b = factor * m_h * n / (n - 1)) } set.seed(1038984) boot_rep &lt;- rerun(500, svy_boot(est_upm, hogar)) # Aplicación a ingreso medio wtd_mean &lt;- function(w, x, na.rm = FALSE) { sum(w * x, na.rm = na.rm) / sum(w, na.rm = na.rm) } # La media es: hogar %&gt;% summarise(media = wtd_mean(factor, ing_cor)) #&gt; # A tibble: 1 x 1 #&gt; media #&gt; &lt;dbl&gt; #&gt; 1 49610. Y el error estándar: map_dbl(boot_rep, ~wtd_mean(w = .$factor_b, x = .$ing_cor)) %&gt;% sd() #&gt; [1] 441.0439 El método bootstrap está implementado en el paquete survey y más recientemente en srvyr que es una versión tidy que utiliza las funciones en survey. Podemos comparar nuestros resultados con la implementación en survey. # 1. Definimos el diseño de la encuesta library(survey) library(srvyr) enigh_design &lt;- hogar %&gt;% as_survey_design(ids = upm, weights = factor, strata = est_dis) # 2. Elegimos bootstrap como el método para el cálculo de errores estándar set.seed(7398731) enigh_boot &lt;- enigh_design %&gt;% as_survey_rep(type = &quot;subbootstrap&quot;, replicates = 500) # 3. Así calculamos la media enigh_boot %&gt;% srvyr::summarise(mean_ingcor = survey_mean(ing_cor)) #&gt; # A tibble: 1 x 2 #&gt; mean_ingcor mean_ingcor_se #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 49610. 459. enigh_boot %&gt;% group_by(edo) %&gt;% srvyr::summarise(mean_ingcor = survey_mean(ing_cor)) #&gt; # A tibble: 30 x 3 #&gt; edo mean_ingcor mean_ingcor_se #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 10 50161. 942. #&gt; 2 11 46142. 1252. #&gt; 3 12 29334. 1067. #&gt; 4 13 38783. 933. #&gt; 5 14 60541. 1873. #&gt; 6 15 48013. 1245. #&gt; 7 16 42653. 1239. #&gt; 8 17 42973. 1675. #&gt; 9 18 48148. 1822. #&gt; 10 19 68959. 3625. #&gt; # … with 20 more rows # cuantiles svyquantile(~ing_cor, enigh_boot, quantiles = seq(0.1, 1, 0.1), interval.type = &quot;quantile&quot;) #&gt; Statistic: #&gt; ing_cor #&gt; q0.1 13155.75 #&gt; q0.2 18895.37 #&gt; q0.3 24041.89 #&gt; q0.4 29358.29 #&gt; q0.5 35505.47 #&gt; q0.6 42695.44 #&gt; q0.7 52426.32 #&gt; q0.8 66594.08 #&gt; q0.9 94613.04 #&gt; q1 4501830.28 #&gt; SE: #&gt; ing_cor #&gt; q0.1 114.2707 #&gt; q0.2 110.1885 #&gt; q0.3 130.8151 #&gt; q0.4 152.8712 #&gt; q0.5 199.3702 #&gt; q0.6 241.1244 #&gt; q0.7 339.4501 #&gt; q0.8 479.4980 #&gt; q0.9 908.6814 #&gt; q1 384477.9727 Supongamos que queremos calcular la media para los hogares con jefe de familia mujer mayor a 50 años. # Creamos datos con filter y repetimos lo de arriba hogar_mujer &lt;- filter(hogar, jefa_50) est_upm_mujer &lt;- hogar_mujer %&gt;% distinct(est_dis, upm, n) # bootstrap boot_rep_mujer &lt;- rerun(500, svy_boot(est_upm_mujer, hogar_mujer)) # media y error estándar hogar_mujer %&gt;% summarise(media = wtd_mean(factor, ing_cor)) #&gt; # A tibble: 1 x 1 #&gt; media #&gt; &lt;dbl&gt; #&gt; 1 44356. # usamos bootstrap para calcular los errores estándar map_dbl(boot_rep_mujer, ~wtd_mean(w = .$factor_b, x = .$ing_cor, na.rm = TRUE)) %&gt;% sd() #&gt; [1] 546.8034 Comparemos con los resultados de srvyr. ¿qué pasa? library(srvyr) enigh_boot %&gt;% srvyr::group_by(jefa_50) %&gt;% srvyr::summarise(mean_ingcor = survey_mean(ing_cor)) #&gt; # A tibble: 2 x 3 #&gt; jefa_50 mean_ingcor mean_ingcor_se #&gt; &lt;lgl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 FALSE 50574. 502. #&gt; 2 TRUE 44356. 726. Sub-poblaciones como “jefas de familia mayores a 50” se conocen como un dominio, esto es un subgrupo cuyo tamaño de muestra es aleatorio, este ejemplo nos recalca la importancia de considerar el proceso en que se generó la muestra para calcular los errores estándar bootstrap. map_dbl(boot_rep, function(x){hm &lt;- filter(x, jefa_50); wtd_mean(w = hm$factor_b, x = hm$ing_cor)}) %&gt;% sd() #&gt; [1] 715.9535 Resumiendo: El bootstrap de Rao y Wu genera un estimador consistente y aproximadamente insesgado de la varianza de estadísticos no lineales y para la varianza de un cuantil. Este método supone que la seleccion de UPMs es con reemplazo; hay variaciones del estimador bootstrap de Rao y Wu que extienden el método que acabamos de estudiar; sin embargo, es común ignorar este aspecto, por ejemplo Mach et al estudian las propiedades del estimador de varianza bootstrap de Rao y Wu cuando la muestra se seleccionó sin reemplazo. Referencias "],
["bootstrap-en-r.html", "6.5 Bootstrap en R", " 6.5 Bootstrap en R Es común crear nuestras propias funciones cuando usamos bootstrap, sin embargo, en R también hay alternativas que pueden resultar convenientes, mencionamos 3: El paquete rsample (forma parte de la colección tidymodels) y tiene una función bootsrtraps() que regresa un arreglo cuadrangular (tibble, data.frame) que incluye una columna con las muestras bootstrap y un identificador del número y tipo de muestra. Veamos un ejemplo donde seleccionamos muestras del conjunto de datos muestra_computos que contiene 10,000 observaciones. library(rsample) library(estcomp) muestra_computos &lt;- sample_n(election_2012, 10000) muestra_computos #&gt; # A tibble: 10,000 x 23 #&gt; state_code state_name state_abbr district_loc_17 district_fed_17 polling_id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 27 Tabasco TAB 5 5 120510 #&gt; 2 15 México MEX 32 24 70680 #&gt; 3 09 Ciudad de… CDMX 20 17 29697 #&gt; 4 21 Puebla PUE 16 12 102052 #&gt; 5 12 Guerrero GRO 17 1 43875 #&gt; 6 30 Veracruz VER 8 7 131313 #&gt; 7 11 Guanajuato GTO 18 7 42010 #&gt; 8 12 Guerrero GRO 4 4 45915 #&gt; 9 30 Veracruz VER 25 19 137278 #&gt; 10 15 México MEX 29 15 66959 #&gt; # … with 9,990 more rows, and 17 more variables: section &lt;int&gt;, region &lt;chr&gt;, #&gt; # polling_type &lt;chr&gt;, section_type &lt;chr&gt;, pri_pvem &lt;int&gt;, pan &lt;int&gt;, #&gt; # panal &lt;int&gt;, prd_pt_mc &lt;int&gt;, otros &lt;int&gt;, total &lt;int&gt;, nominal_list &lt;int&gt;, #&gt; # pri_pvem_pct &lt;dbl&gt;, pan_pct &lt;dbl&gt;, panal_pct &lt;dbl&gt;, prd_pt_mc_pct &lt;dbl&gt;, #&gt; # otros_pct &lt;dbl&gt;, winner &lt;chr&gt; Generamos 100 muestras bootstrap, y la función nos regresa un arreglo con 100 renglones, cada uno corresponde a una muestra bootstrap. set.seed(839287482) computos_boot &lt;- bootstraps(muestra_computos, times = 100) computos_boot #&gt; # Bootstrap sampling #&gt; # A tibble: 100 x 2 #&gt; splits id #&gt; &lt;list&gt; &lt;chr&gt; #&gt; 1 &lt;split [10K/3.6K]&gt; Bootstrap001 #&gt; 2 &lt;split [10K/3.6K]&gt; Bootstrap002 #&gt; 3 &lt;split [10K/3.7K]&gt; Bootstrap003 #&gt; 4 &lt;split [10K/3.7K]&gt; Bootstrap004 #&gt; 5 &lt;split [10K/3.7K]&gt; Bootstrap005 #&gt; 6 &lt;split [10K/3.7K]&gt; Bootstrap006 #&gt; 7 &lt;split [10K/3.7K]&gt; Bootstrap007 #&gt; 8 &lt;split [10K/3.7K]&gt; Bootstrap008 #&gt; 9 &lt;split [10K/3.6K]&gt; Bootstrap009 #&gt; 10 &lt;split [10K/3.7K]&gt; Bootstrap010 #&gt; # … with 90 more rows La columna splits tiene información de las muestras seleccionadas, para la primera vemos que de 10,000 observaciones en la muestra original la primera muestra bootstrap contiene 10000-3647=6353. first_computos_boot &lt;- computos_boot$splits[[1]] first_computos_boot #&gt; &lt;10000/3647/10000&gt; Y podemos obtener los datos de la muestra bootstrap con la función as.data.frame() as.data.frame(first_computos_boot) #&gt; # A tibble: 10,000 x 23 #&gt; state_code state_name state_abbr district_loc_17 district_fed_17 polling_id #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 01 Aguascali… AGS 18 3 1308 #&gt; 2 15 México MEX 17 18 68217 #&gt; 3 02 Baja Cali… BC 10 6 4504 #&gt; 4 16 Michoacán MICH 8 2 78876 #&gt; 5 09 Ciudad de… CDMX 7 9 24951 #&gt; 6 05 Coahuila COAH 15 7 10402 #&gt; 7 26 Sonora SON 20 7 119582 #&gt; 8 09 Ciudad de… CDMX 4 2 23677 #&gt; 9 30 Veracruz VER 14 12 133755 #&gt; 10 30 Veracruz VER 24 19 136925 #&gt; # … with 9,990 more rows, and 17 more variables: section &lt;int&gt;, region &lt;chr&gt;, #&gt; # polling_type &lt;chr&gt;, section_type &lt;chr&gt;, pri_pvem &lt;int&gt;, pan &lt;int&gt;, #&gt; # panal &lt;int&gt;, prd_pt_mc &lt;int&gt;, otros &lt;int&gt;, total &lt;int&gt;, nominal_list &lt;int&gt;, #&gt; # pri_pvem_pct &lt;dbl&gt;, pan_pct &lt;dbl&gt;, panal_pct &lt;dbl&gt;, prd_pt_mc_pct &lt;dbl&gt;, #&gt; # otros_pct &lt;dbl&gt;, winner &lt;chr&gt; Una de las principales ventajas de usar este paquete es que es eficiente en el uso de memoria. library(pryr) #&gt; Registered S3 method overwritten by &#39;pryr&#39;: #&gt; method from #&gt; print.bytes Rcpp #&gt; #&gt; Attaching package: &#39;pryr&#39; #&gt; The following objects are masked from &#39;package:purrr&#39;: #&gt; #&gt; compose, partial object_size(muestra_computos) #&gt; 1.41 MB object_size(computos_boot) #&gt; 5.49 MB # tamaño por muestra object_size(computos_boot)/nrow(computos_boot) #&gt; 54.9 kB # el incremento en tamaño es &lt;&lt; 100 as.numeric(object_size(computos_boot)/object_size(muestra_computos)) #&gt; [1] 3.894717 El paquete boot está asociado al libro Bootstrap Methods and Their Applications (Davison and Hinkley (1997)) y tiene, entre otras, funciones para calcular replicaciones bootstrap y para construir intervalos de confianza usando bootstrap: calculo de replicaciones bootstrap con la función boot(), intervalos normales, de percentiles y \\(BC_a\\) con la función boot.ci(), intevalos ABC con la función `abc.ci(). El paquete bootstrap contiene datos usados en Efron and Tibshirani (1993), y la implementación de funciones para calcular replicaciones y construir intervalos de confianza: calculo de replicaciones bootstrap con la función bootstrap(), intervalos \\(BC_a\\) con la función bcanon(), intevalos ABC con la función `abcnon(). Referencias "],
["conclusiones-y-observaciones.html", "6.6 Conclusiones y observaciones", " 6.6 Conclusiones y observaciones El principio fundamental del Bootstrap no paramétrico es que podemos estimar la distribución poblacional con la distribución empírica. Por tanto para hacer inferencia tomamos muestras con reemplazo de la distribución empírica y analizamos la variación de la estadística de interés a lo largo de las muestras. El bootstrap nos da la posibilidad de crear intervalos de confianza cuando no contamos con fórmulas para hacerlo de manera analítica y sin supuestos distribucionales de la población. Hay muchas opciones para construir intervalos bootstrap, los que tienen mejores propiedades son los intervalos \\(BC_a\\), sin embargo los más comunes son los intervalos normales con error estándar bootstrap y los intervalos de percentiles de la distribución bootstrap. Antes de hacer intervalos normales (o con percentiles de una t) vale la pena graficar la distribución bootstrap y evaluar si el supuesto de normalidad es razonable. En cuanto al número de muestras bootstrap se recomienda al menos \\(1,000\\) al hacer pruebas, y \\(10,000\\) o \\(15,000\\) para los resultados finales, sobre todo cuando se hacen intervalos de confianza de percentiles. La función de distribución empírica es una mala estimación en las colas de las distribuciones, por lo que es difícil construir intervalos de confianza (usando bootstrap no paramétrico) para estadísticas que dependen mucho de las colas. "],
["teoría-básica-de-simulación.html", "Sección 7 Teoría básica de simulación", " Sección 7 Teoría básica de simulación “Any one who considers arithmetical methods of producing random digits is, of course, in a state of sin.” -Von Neuman La simulación de un modelo consiste en la construcción de un programa computacional que permite obtener los valores de las varibles de salida para distintos valores de las variables de entrada con el objetivo de obtener conclusiones del sistema que apoyen la toma de decisiones (explicar y/o predecir el comportamiento del sistema). Requerimientos prácticos de un proyecto de simulación: Fuente de números aleatorios \\(U(0,1)\\) (números pseudoaleatorios). Transformar los números aleatorios en variables de entrada del modelo (e.g. generación de muestras con cierta distribución). Construir el programa computacional de simulación. Analizar las distintas simulaciones del modelo para obtener conclusiones acerca del sistema. Comenzaremos introduciendo el primer punto, ¿cómo generar números \\(U(0,1)\\)? "],
["números-pseudoaleatorios.html", "7.1 Números pseudoaleatorios", " 7.1 Números pseudoaleatorios El objetivo es generar sucesiones \\(\\{u_i\\}_{i=1}^N\\) de números independientes que se puedan considerar como observaciones de una distribución uniforme en el intervalo \\((0, 1)\\). Verdaderos números aleatorios. Los números completamente aleatorios (no determinísticos) son fáciles de imaginar conceptualmente, por ejemplo podemos imaginar lanzar una moneda, lanzar un dado o una lotería. En general los números aleatorios se basan en alguna fuente de aleatoreidad física que puede ser teóricamente impredecible (cuántica) o prácticamente impredecible (caótica). Por ejemplo: random.org genera aleatoreidad a través de ruido atmosférico (el paquete random contiene funciones para obtener números de random.org), ERNIE, usa ruido térmico en transistores y se utiliza en la lotería de bonos de Reino Unido. RAND Corporation En 1955 publicó una tabla de un millón de números aleatorios que fue ampliamente utilizada. Los números en la tabla se obtuvieron de una ruleta electrónica. La desventaja de éstos métodos es que son costosos, tardados y no reproducibles. Números pseudoaleatorios. Los números pseudoaleatorios se generan de manera secuencial con un algoritmo determinístico, formalmente se definen por: Función de inicialización. Recibe un número (la semilla) y pone al generador en su estado inicial. Función de transición. Transforma el estado del generador. Función de salidas. Transforma el estado para producir un número fijo de bits (0 ó 1). Una sucesión de bits pseudoaleatorios se obtiene definiendo la semilla y llamando repetidamente la función de transición y la función de salidas. Esto implica, entre otras cosas, que una sucesión de números pseudoaletorios esta completamente determinada por la semilla. Ahora, buscamos que una secuencia de números pseudoaleatorios: no muestre ningún patrón o regularidad aparente desde un punto de vista estadístico, y dada una semilla inicial, se puedan generar muchos valores antes de repetir el ciclo. Construir un buen algoritmo de números pseudoaleatorios es complicado, como veremos en los siguientes ejemplos. Ejemplo: método de la parte media del cuadrado En 1946 Jon Von Neuman sugirió usar las operaciones aritméticas de una computadora para generar secuencias de número pseudoaleatorios. Sugirió el método middle square, para generar secuencias de dígitos pseudoaleatorios de 4 dígitos propuso, Se inicia con una semilla de 4 dígitos: seed = 9731 La semilla se eleva al cuadrado, produciendo un número de 8 dígitos (si el resultado tiene menos de 8 dígitos se añaden ceros al inicio): value = 94692361 Los 4 números del centro serán el siguiente número en la secuencia, y se devuelven como resultado: value = 6923 mid_square &lt;- function(seed, n) { seeds &lt;- numeric(n) values &lt;- numeric(n) for (i in 1:n) { x &lt;- seed ^ 2 seed = case_when( nchar(x) &gt; 2 ~ (x %/% 1e2) %% 1e4, TRUE ~ 0) values[i] &lt;- x seeds[i] &lt;- seed } cbind(seeds, values) } x &lt;- mid_square(1931, 10) print(x, digits = 4) #&gt; seeds values #&gt; [1,] 7287 3728761 #&gt; [2,] 1003 53100369 #&gt; [3,] 60 1006009 #&gt; [4,] 36 3600 #&gt; [5,] 12 1296 #&gt; [6,] 1 144 #&gt; [7,] 0 1 #&gt; [8,] 0 0 #&gt; [9,] 0 0 #&gt; [10,] 0 0 x &lt;- mid_square(9731, 100) print(x, digits = 4) #&gt; seeds values #&gt; [1,] 6923 94692361 #&gt; [2,] 9279 47927929 #&gt; [3,] 998 86099841 #&gt; [4,] 9960 996004 #&gt; [5,] 2016 99201600 #&gt; [6,] 642 4064256 #&gt; [7,] 4121 412164 #&gt; [8,] 9826 16982641 #&gt; [9,] 5502 96550276 #&gt; [10,] 2720 30272004 #&gt; [11,] 3984 7398400 #&gt; [12,] 8722 15872256 #&gt; [13,] 732 76073284 #&gt; [14,] 5358 535824 #&gt; [15,] 7081 28708164 #&gt; [16,] 1405 50140561 #&gt; [17,] 9740 1974025 #&gt; [18,] 8676 94867600 #&gt; [19,] 2729 75272976 #&gt; [20,] 4474 7447441 #&gt; [21,] 166 20016676 #&gt; [22,] 275 27556 #&gt; [23,] 756 75625 #&gt; [24,] 5715 571536 #&gt; [25,] 6612 32661225 #&gt; [26,] 7185 43718544 #&gt; [27,] 6242 51624225 #&gt; [28,] 9625 38962564 #&gt; [29,] 6406 92640625 #&gt; [30,] 368 41036836 #&gt; [31,] 1354 135424 #&gt; [32,] 8333 1833316 #&gt; [33,] 4388 69438889 #&gt; [34,] 2545 19254544 #&gt; [35,] 4770 6477025 #&gt; [36,] 7529 22752900 #&gt; [37,] 6858 56685841 #&gt; [38,] 321 47032164 #&gt; [39,] 1030 103041 #&gt; [40,] 609 1060900 #&gt; [41,] 3708 370881 #&gt; [42,] 7492 13749264 #&gt; [43,] 1300 56130064 #&gt; [44,] 6900 1690000 #&gt; [45,] 6100 47610000 #&gt; [46,] 2100 37210000 #&gt; [47,] 4100 4410000 #&gt; [48,] 8100 16810000 #&gt; [49,] 6100 65610000 #&gt; [50,] 2100 37210000 #&gt; [51,] 4100 4410000 #&gt; [52,] 8100 16810000 #&gt; [53,] 6100 65610000 #&gt; [54,] 2100 37210000 #&gt; [55,] 4100 4410000 #&gt; [56,] 8100 16810000 #&gt; [57,] 6100 65610000 #&gt; [58,] 2100 37210000 #&gt; [59,] 4100 4410000 #&gt; [60,] 8100 16810000 #&gt; [61,] 6100 65610000 #&gt; [62,] 2100 37210000 #&gt; [63,] 4100 4410000 #&gt; [64,] 8100 16810000 #&gt; [65,] 6100 65610000 #&gt; [66,] 2100 37210000 #&gt; [67,] 4100 4410000 #&gt; [68,] 8100 16810000 #&gt; [69,] 6100 65610000 #&gt; [70,] 2100 37210000 #&gt; [71,] 4100 4410000 #&gt; [72,] 8100 16810000 #&gt; [73,] 6100 65610000 #&gt; [74,] 2100 37210000 #&gt; [75,] 4100 4410000 #&gt; [76,] 8100 16810000 #&gt; [77,] 6100 65610000 #&gt; [78,] 2100 37210000 #&gt; [79,] 4100 4410000 #&gt; [80,] 8100 16810000 #&gt; [81,] 6100 65610000 #&gt; [82,] 2100 37210000 #&gt; [83,] 4100 4410000 #&gt; [84,] 8100 16810000 #&gt; [85,] 6100 65610000 #&gt; [86,] 2100 37210000 #&gt; [87,] 4100 4410000 #&gt; [88,] 8100 16810000 #&gt; [89,] 6100 65610000 #&gt; [90,] 2100 37210000 #&gt; [91,] 4100 4410000 #&gt; [92,] 8100 16810000 #&gt; [93,] 6100 65610000 #&gt; [94,] 2100 37210000 #&gt; [95,] 4100 4410000 #&gt; [96,] 8100 16810000 #&gt; [97,] 6100 65610000 #&gt; [98,] 2100 37210000 #&gt; [99,] 4100 4410000 #&gt; [100,] 8100 16810000 Este generador cae rápidamente en ciclos cortos, por ejemplo, si aparece un cero se propagará por siempre. A inicios de 1950s se exploró el método y se propusieron mejoras, por ejemplo para evitar caer en cero. Metrópolis logró obtener una secuencia de 750,000 números distintos al usar semillas de 38 bits (usaba sistema binario), además la secuencia de Metrópolis mostraba propiedades deseables. No obstante, el método del valor medio no es considerado un método bueno por lo común de los ciclos cortos. Ejemplo: rand Por muchos años (antes de 1995) el generador de la función rand en Matlab fue el generador congruencial: \\[X_{n+1} = (7^5)X_n mod(2^{31}-1)\\] Construyamos sucesiones de longitud \\(1,500\\) usando el algoritmo de rand: sucesion &lt;- function(n = 1500, semilla = runif(1, 0, 2 ^ 31 - 1)){ x &lt;- rep(NA, n) u &lt;- rep(NA, n) x[1] &lt;- semilla u[1] &lt;- x[1] / (2 ^ 31 - 1) # transformamos al (0, 1) for (i in 2:n) { x[i] &lt;- (7 ^ 5 * x[i - 1]) %% (2 ^ 31 - 1) u[i] &lt;- x[i] / (2 ^ 31 - 1) } u } u_rand &lt;- sucesion(n = 150000) sucesiones &lt;- map_df(1:12, ~tibble(serie = ., sim = sucesion(), ind = 1:length(sim))) sucesiones #&gt; # A tibble: 18,000 x 3 #&gt; serie sim ind #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; #&gt; 1 1 0.299 1 #&gt; 2 1 0.404 2 #&gt; 3 1 0.237 3 #&gt; 4 1 0.110 4 #&gt; 5 1 0.640 5 #&gt; 6 1 0.546 6 #&gt; 7 1 0.894 7 #&gt; 8 1 0.768 8 #&gt; 9 1 0.668 9 #&gt; 10 1 0.297 10 #&gt; # … with 17,990 more rows Una propiedad deseable es que la sucesión de \\(u_i\\) parezca una sucesión de observaciones independientes de una \\(Uniforme(0,1)\\). Veamos una gráfica del índice de simulación contra el valor obtenido ggplot(sucesiones, aes(x = ind, y = sim)) + geom_point(alpha = 0.5, size = 1.5) + # alpha controla la transparencia facet_wrap(~ serie) + geom_smooth(method = &quot;loess&quot;, se = FALSE, color = &quot;white&quot;, size = 0.7) Comparemos con los cuantiles de una uniforme: ggplot(sucesiones) + stat_qq(aes(sample = sim), distribution = qunif) + geom_abline(color = &quot;white&quot;, size = 0.6, alpha = 0.6) + facet_wrap(~ serie) Ejemplo: RANDU RANDU fue generador de números aleatorios ampliamente utilizado en los 60´s y 70´s, se define como: \\[X_{n + 1}= (2 ^ {16} + 3)X_n mod(2^{31})\\] A primera vista las sucesiones se asemejan a una uniforme, sin embargo, cuando se grafican ternas emergen patrones no deseados. library(tourr) library(plotly) n &lt;- 150000 # longitud de la sucesión x &lt;- rep(NA, n) u &lt;- rep(NA, n) x[1] &lt;- 4798373 # semilla u[1] &lt;- x[1] / (2 ^ 31 - 1) # transformamos al (0, 1) for (i in 2:n) { x[i] &lt;- ((2 ^ 16 + 3) * x[i - 1]) %% (2 ^ 31) u[i] &lt;- x[i] / (2 ^ 31) } u_randu &lt;- u set.seed(8111938) mat &lt;- matrix(u[1:1500], ncol = 3, byrow = TRUE) tour &lt;- new_tour(mat, grand_tour(), NULL) steps &lt;- seq(0, 1, 0.01) names(steps) &lt;- steps mat_xy &lt;- map_dfr(steps, ~data.frame(center(mat %*% tour(.)$proj)), .id = &quot;steps&quot;) # step 0.72 mat_xy %&gt;% mutate(steps = as.numeric(steps)) %&gt;% plot_ly(x = ~X1, y = ~X2, frame = ~steps, type = &#39;scatter&#39;, mode = &#39;markers&#39;, showlegend = F, marker = list(size = 5, color = &quot;black&quot;), opacity = 0.5) %&gt;% animation_opts(frame = 250) Veamos los resultados enteros del generador, ¿qué notas? options(digits = 5) n &lt;- 50 x &lt;- rep(NA, n) x[1] &lt;- 1 # semilla u[1] &lt;- x[1] # transformamos al (0, 1) for (i in 2:n) { x[i] &lt;- ((2 ^ 16 + 3) * x[i - 1]) %% (2 ^ 31) } x #&gt; [1] 1 65539 393225 1769499 7077969 26542323 #&gt; [7] 95552217 334432395 1146624417 1722371299 14608041 1766175739 #&gt; [13] 1875647473 1800754131 366148473 1022489195 692115265 1392739779 #&gt; [19] 2127401289 229749723 1559239569 845238963 1775695897 899541067 #&gt; [25] 153401569 1414474403 663781353 1989836731 1670020913 701529491 #&gt; [31] 2063890617 1774610987 662584961 888912771 1517695625 1105958811 #&gt; [37] 1566426833 1592415347 1899101529 1357838347 1792534561 682145891 #&gt; [43] 844966185 1077967739 1010594417 656824147 1288046073 1816859115 #&gt; [49] 1456223681 975544643 Generadores congruenciales y Mersenne-Twister Los generadores como rand y RANDU (\\(X_{n+1} = (7^5)X_n mod(2^{31}-1)\\) y \\(X_{n+1}= (2 ^ {16} + 3)X_n mod(2^{31})\\)) se denominan generadores congruenciales. Los Generadores Congruenciales Lineales (GCL) tienen la forma \\[X_{n+1} = (aX_n + c)mod(m)\\] Están determinados por los parámetros: * Módulo: \\(m &gt; 0\\) * Multiplicador \\(0\\le a &lt; m\\) * Incremento \\(c \\le m\\) * Semilla \\(0\\le X_0 &lt; m\\) Los GCL se introdujeron en 1949 por D.H. Lehemer y aún son usados. La elección de los parámetros determina la calidad del generador: Queremos \\(m\\) grande pues el periodo (longitud del ciclo) del generador no puede tener más de \\(m\\) elementos. Queremos velocidad, en este caso, un valor conveniente para \\(m\\) es el tamaño de palabra (máximo número de bits que puede procesar el CPU en un ciclo) de la computadora. Los GCL más eficientes tienen \\(m\\) igual a una potencia de 2 (es usual 232 o 264) de esta manera la operación módulo se calcula truncando todos los dígitos excepto los últimos 32 ó 64 bits. ¿podemos elegir \\(a\\) y \\(c\\) de tal manera que logremos alcanzar el periodo máximo (\\(m\\))? Un generador congruencial mixto (\\(c&gt;0\\)) tendrá periodo completo para todas las semillas sí y sólo sí: \\(m\\) y \\(c\\) son primos relativos. \\(a-1\\) es divisible por todos los factores primos de \\(m\\). \\(a-1\\) es divisible por 4 si \\(m\\) es divisible por 4. Vale la pena notar que un periodo grande no determina que el generador congruencial es bueno, debemos verificar que los números que generan se comportan como si fueran aleatorios. Los GCLs tienden a exhibir defectos, por ejemplo, si se utiliza un GCL para elegir puntos en un espacio de dimensión \\(k\\) los puntos van a caer en a lo más \\((k!m)^{1/k}\\) hiperplanos paralelos \\(k\\) dimensionales (como observamos con RANDU), donde \\(k\\) se refiere a la dimensión de \\([0,1]^k\\). Los GCLs continuan siendo utilizados en muchas aplicaciones porque con una elección cuidadosa de los parámetros pueden pasar muchas pruebas de aleatoriedad, son rápidos y requiren poca memoria. Un ejemplo es Java, su generador de números aleatorios default java.util.Random es un GCL con multiplicador \\(a=25214903917\\), incremento \\(c=11\\) y módulo \\(m=2^{48}\\), sin embargo, actualmente el generador default de R es el Mersenne-Twister que no pertenece a la clase de GCLs (se puede elegir usar otros generadores para ver los disponible teclea ?Random). El generador Mersenne-Twister se desarrolló en 1997 por Makoto Matsumoto y Takuji Nishimura (Matsumoto and Nishimura (1998)), es el generador default en muchos programas, por ejemplo, en Python, Ruby, C++ estándar, Excel y Matlab (más aquí). Este generador tiene propiedades deseables como un periodo largo (2^19937-1) y el hecho que pasa muchas pruebas de aleatoriedad; sin embargo, es lento (relativo a otros generadores) y falla algunas pruebas de aleatoreidad. . Pruebas de aleatoriedad Hasta ahora hemos graficado las secuencias de números aleatorios para evaluar su aleatoriedad, sin embargo, el ojo humano no es muy bueno discriminando aleatoriedad y las gráficas no escalan. Es por ello que resulta conveniente hacer pruebas estadísticas para evaluar la calidad de los generadores de números pseudoaleatorios. Hay dos tipos de pruebas: empíricas: evalúan estadísticas de sucesiones de números. teóricas: se establecen las características de las sucesiones usando métodos de teoría de números con base en la regla de recurrencia que generó la sucesión. Veremos 2 ejemplos de la primera clase: prueba de bondad de ajuste \\(\\chi^2\\) y prueba de espera. Ejemplo: prueba de bondad de ajuste \\(\\chi^2\\) \\(H_0:\\) Los datos son muestra de una cierta distribución \\(F\\). \\(H_1:\\) Los datos no son una muestra de \\(F\\). Procedimiento general: Partir el soporte de \\(F\\) en \\(c\\) celdas que son exhaustivas y mutuamente exculyentes. Contar el número de observaciones en cada celda \\(O_i\\). Calcular el valor esperado en cada celda bajo \\(F\\): \\(e_i=np_i\\). Calcular la estadística de prueba: \\[\\chi^2 = \\sum_{i=1}^c \\frac{(o_i - e_i)^2}{e_i} \\sim \\chi^2_{c-k-1}\\] donde \\(c\\) es el número de celdas y \\(k\\) el número de parámetros estimados en \\(F\\) a partir de los datos observados. u_rand_cat &lt;- cut(u_rand, breaks = seq(0, 1, 0.1)) u_randu_cat &lt;- cut(u_randu, breaks = seq(0, 1, 0.1)) u_mt &lt;- runif(150000) u_mt_cat &lt;- cut(u_mt, breaks = seq(0, 1, 0.1)) chisq_test &lt;- function(o_i) { expected &lt;- rep(15000, 10) observed &lt;- table(o_i) x2 &lt;- sum((observed - expected) ^ 2 / expected) list(x2 = x2, p_value = pchisq(x2, 9)) } chisq_test(u_rand_cat) #&gt; $x2 #&gt; [1] 15.368 #&gt; #&gt; $p_value #&gt; [1] 0.91869 chisq_test(u_randu_cat) #&gt; $x2 #&gt; [1] 3.0491 #&gt; #&gt; $p_value #&gt; [1] 0.037682 chisq_test(u_mt_cat) #&gt; $x2 #&gt; [1] 17.23 #&gt; #&gt; $p_value #&gt; [1] 0.95477 Una variación de esta prueba de bondad de ajuste \\(\\chi^2\\), es la prueba de uniformidad k-dimensional: \\(H_0:\\) Distribución uniforme en \\([0,1]^k\\), con \\(k = 1,2,...\\) En este caso se divide el espacio \\([0,1]^k\\) en celdas exhaustivas y mutuamente excluyentes, y se aplica la prueba \\(\\chi^2\\) a los vectores sucesivos \\((u_1,u_2,...,u_k),(u_{k+1},u_{k+2},...,u_{2k}),...\\) 7.1.0.1 Ejemplo: prueba de espera \\(H_0:\\) independencia y uniformidad Procedimiento: Seleccionar un subintervalo del \\((0,1)\\). Calcular la probabilidad del subintervalo. Ubicar en la sucesión las posiciones de los elementos que pertenezcan al subintervalo. Calcular el número de elementos consecutivos de la sucesión entre cada una de las ocurrencias consecutivas de elementos del subintervalo (tiempos de espera). La distribución de los tiempos de espera es geométrica con parámetro calculado en 2. Aplicar una prueba \\(\\chi^2\\) a los tiempos de espera. # 1 intervalo &lt;- c(0, 0.5) # 2 p_intervalo &lt;- 0.5 # 3 evaluamos u_rand gap_u_rand &lt;- tibble(posicion = 1:length(u_rand), u_rand = u_rand, en_intervalo = (0.5 &lt; u_rand &amp; u_rand &lt; 1)) posiciones &lt;- gap_u_rand %&gt;% filter(en_intervalo) %&gt;% mutate( espera = posicion - lag(posicion), espera = ifelse(is.na(espera), posicion, espera), espera = factor(espera, levels = 1:18) ) # 4 gap_frec &lt;- as.data.frame(table(posiciones$espera)) %&gt;% rename(espera = Var1, obs = Freq) # esperados geométrica gap_frec$geom &lt;- (dgeom(0:18, p_intervalo) * length(u_rand)) [-1] x2 &lt;- sum((gap_frec$obs - gap_frec$geom) ^ 2 / gap_frec$geom) pchisq(x2, 16) #&gt; [1] 0.9956 Otras pruebas de aleatoriedad son prueba de rachas, Kolmogorov-Smirnov, prueba de poker, puedes leer más de generadores aleatorios y pruebas en Knuth (1997). En R hay pruebas implementadas en los paquetes randtoolboxy RDieHarder. Referencias "],
["variables-aleatorias-1.html", "7.2 Variables aleatorias", " 7.2 Variables aleatorias El segundo requisito práctico de un proyecto de simulación es: Transformar los números aleatorios en variables de entrada del modelo (e.g. generación de muestras con cierta distribución). En nuestro caso, usamos simulación aplicada a modelos estadísticos: Un modelo estadístico \\(F\\) es un conjuto de distribuciones (o densidades o funciones de regresión). Un modelo paramétrico es un conjunto \\(F\\) que puede ser parametrizado por un número finito de parámetros. Por ejemplo, si suponemos que los datos provienen de una distribución Normal, el modelo es: \\[F=\\bigg\\{p(x;\\mu, \\sigma) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\bigg), \\mu \\in \\mathbb{R}, \\sigma&gt;0\\bigg\\}\\] En general, un modelo paramétrico tiene la forma \\[F=\\bigg\\{p(x;\\theta):\\theta \\in \\Theta \\bigg\\}\\] donde \\(\\theta\\) es un parámetro desconocido (o un vector de parámetros) que puede tomar valores en el espacio paramétrico \\(\\Theta\\). En lo que resta de esta sección estudiaremos simulación de modelos paramétricos. Comencemos repasando algunos conceptos: Una variable aleatoria es un mapeo entre el espacio de resultados y los números reales. Ejemplo. Lanzamos una moneda justa dos veces, definimos \\(X\\) como en el número de soles, entonces la variable aleatoria se pueden resumir como: \\(\\omega\\) \\(P(\\{\\omega\\})\\) \\(X(\\omega)\\) AA 1/4 0 AS 1/4 1 SA 1/4 1 SS 1/4 2 La función de distribución acumulada es la función \\(P_X:\\mathbb{R}\\to[0,1]\\) definida como: \\[P_X(x)=P(X\\leq x)\\] En el ejemplo: \\[ P_X(x) = \\left\\{ \\begin{array}{lr} 0 &amp; x &lt; 0\\\\ 1/4 &amp; 0 \\leq x &lt; 1 \\\\ 3/4 &amp; 1 \\leq x &lt; 2 \\\\ 1 &amp; x \\ge 2 \\end{array} \\right. \\] Una variable aleatoria \\(X\\) es discreta si toma un número contable de valores \\(\\{x_1,x_2,...\\}\\). En este caso definimos la función de probabilidad o la función masa de probabilidad de X como \\(p_X(x)=P(X=x)\\). Notemos que \\(p_X(x)\\geq 0\\) para toda \\(x \\in \\mathbb{R}\\) y \\(\\sum_i p_X(x)=1\\). Más aún, la función de distribución acumulada esta relacionada con \\(p_X\\) por \\[P_X(x)=P(X \\leq x)= \\sum_{x_i\\leq x} = \\sum_{x_i\\leq x}p_{X}(x_i)\\] \\[ p_X(x) = \\left\\{ \\begin{array}{lr} 1/4 &amp; x = 0 \\\\ 1/2 &amp; x = 1 \\\\ 1/4 &amp; x = 2\\\\ 0 &amp; e.o.c. \\end{array} \\right. \\] Sea \\(X\\) una variable aleatoria con FDA \\(P_X\\). La función de distribución acumulada inversa o función de cuantiles se define como: \\[P_X^{-1}(q) = inf\\{x:P_X(x)&gt;q\\}\\] para \\(q \\in [0,1]\\). Llamamos a \\(P^{-1}(1/4)\\) el primer cuartil, a \\(P^{-1}(1/2)\\) la mediana y \\(P^{-1}(3/4)\\) el tercer cuartil. Familias discretas importantes Muchas variables aleatorias provienen de familias o tipos de experimentos similares lo que nos ahorra tener que determinar las funciones de distribución y sus propiedades cada vez. Por ejemplo, el resultado de interés en muchos experimentos es un resultado que solo puede tomar dos valores: una moneda puede ser águila o sol, un persona puede estar empleada o desempleada, un transistor puede estar defectuoso o no,… La misma distribución de probabilidad que describe a una variable aleatoria que puede tomar el valor de \\(1\\) con probabilidad \\(p\\) y \\(0\\) con probabilidad \\(q=1-p\\), se conoce como distribución Bernoulli. Distribución Bernoulli Sea \\(X\\) la variable aleatoria que representa un lanzamiento de moneda, con \\(P(X=1)=p\\) y \\(P(X=0)=1-p\\) para alguna \\(p\\in[0,1].\\) Decimos que \\(X\\) tiene una distribución Bernoulli (\\(X \\sim Bernoulli(p)\\)), y su función de distribución es: \\[ p(x) = \\left\\{ \\begin{array}{lr} p^x(1-p)^{1-x} &amp; x \\in \\{0,1\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X) = p, Var(X)=p(1-p)\\) Notemos que un experimento Bernoulli es una repetición del experimento que involucra solo dos posibles salidas. Es común que nos interese el resultado de la repetición de experimentos Bernoulli independientes, en este caso se usa la distribución Binomial. Distribución Binomial Supongamos que tenemos una moneda que cae en sol con probabilidad \\(p\\), para alguna \\(p\\in[0,1].\\) Lanzamos la moneda \\(n\\) veces y sea \\(X\\) el número de soles. Suponemos que los lanzamientos son independientes, entonces la función de distribución es: \\[ p(x) = \\left\\{ \\begin{array}{lr} {n \\choose x}p^x(1-p)^{n-x} &amp; x \\in \\{0,1,...,n\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X) = np, Var(X)=np(1-p)\\) Además, si \\(X_1 \\sim Binomial(n_1, p)\\) y \\(X_2 \\sim Binomial(n_2,p)\\) entonces, \\[X_1 + X_2 \\sim Binomial(n_1+n_2, p)\\] En general la distribución binomial describe el comportamiento de una variable \\(X\\) que cuenta número de éxitos tal que: el número de observaciones \\(n\\) esta fijo, cada observación es independiente, cada observación representa uno de dos posibles eventos (éxito o fracaso) y, la probabilidad de éxito \\(p\\) es la misma en cada observación. library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine densidades &lt;- ggplot(data.frame(x = -1:20)) + geom_point(aes(x = x, y = dbinom(x, size = 20, prob = 0.5), color = &quot;n=20;p=0.5&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dbinom(x, size = 20, prob = 0.5), color = &quot;n=20;p=0.5&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + geom_point(aes(x = x, y = dbinom(x, size = 20, prob = 0.1), color = &quot;n=20;p=0.1&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dbinom(x, size = 20, prob = 0.1), color = &quot;n=20;p=0.1&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + labs(color = &quot;&quot;, y = &quot;&quot;, title = &quot;Distribución binomial&quot;) dists &lt;- ggplot(data_frame(x = -1:20), aes(x)) + stat_function(fun = pbinom, args = list(size = 20, prob = 0.5), aes(colour = &quot;n=20;p=0.5&quot;), alpha = 0.8) + stat_function(fun = pbinom, args = list(size = 20, prob = 0.1), aes(colour = &quot;n=20;p=0.1&quot;), alpha = 0.8) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color = &quot;&quot;) #&gt; Warning: `data_frame()` is deprecated, use `tibble()`. #&gt; This warning is displayed once per session. grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Distribución Uniforme Decimos que \\(X\\) tiene una distribución uniforme en \\(\\{a,...,b\\}\\) (\\(a,b\\) enteros) si tiene una función de probailidad dada por: \\[ p(x) = \\left\\{ \\begin{array}{lr} 1/n &amp; x \\in \\{a,...,b\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] donde \\(n = b-a+1\\), \\(E(X) = (a+b)/2, Var(X)=(n^2-1)/12\\) El ejemplo más común es el lanzamiento de un dado. Distribución Poisson \\(X\\) tienen una distribución Poisson con parámetro \\(\\lambda\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} e^{-\\lambda} \\frac{\\lambda^x}{x!} &amp; x \\in \\{0,1,...\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X) = \\lambda, Var(X)=\\lambda\\) La distribución Poisson se utiliza con frecuencia para modelar conteos de eventos raros, por ejemplo número de accidentes de tráfico. La distribución Poisson es un caso límite de la distribución binomial cuando el número de casos es muy grande y la probabilidad de éxito \\(p\\) es chica. Una propiedad de la distribución Poisson es: \\(X_1 \\sim Poisson(\\lambda_1)\\) y \\(X_2 \\sim Poisson(\\lambda_2)\\) entonces \\(X_1 + X_2 \\sim Poisson(\\lambda_1 + \\lambda_2)\\). densidades &lt;- ggplot(data.frame(x = -1:20)) + geom_point(aes(x = x, y = dpois(x, lambda = 4), color = &quot;lambda=4&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dpois(x, lambda = 4), color = &quot;lambda=4&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + geom_point(aes(x = x, y = dpois(x, lambda = 10), color = &quot;lambda=10&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dpois(x, lambda = 10), color = &quot;lambda=10&quot;), alpha = 0.6, linetype = &quot;dashed&quot;, show.legend = FALSE) + labs(color = &quot;&quot;, y = &quot;&quot;, title = &quot;Distribución Poisson&quot;) dists &lt;- ggplot(data_frame(x = -1:20), aes(x)) + stat_function(fun = ppois, args = list(lambda = 4), aes(colour = &quot;lambda=4&quot;), alpha = 0.8) + stat_function(fun = ppois, args = list(lambda = 10), aes(colour = &quot;lambda=10&quot;), alpha = 0.8) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color = &quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Distribución geométrica \\(X\\) tiene distribución geométrica con parámetro \\(p \\in (0,1)\\), \\(X \\sim Geom(p)\\) si, \\[ p(x) = \\left\\{ \\begin{array}{lr} p(1-p)^{k-1} &amp; x \\in \\{1,2,...\\}\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=1/p, Var(X)=(1-p)/p^2\\) con \\(k \\geq 1\\). Podemos pensar en \\(X\\) como el número de lanzamientos necesarios hasta que obtenemos el primer sol en los lanzamientos de una moneda. densidades &lt;- ggplot(data.frame(x = -1:20)) + geom_point(aes(x = x, y = dgeom(x, prob = 0.5), color = &quot;p=0.5&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dgeom(x, prob = 0.5), color = &quot;p=0.5&quot;), show.legend = FALSE, alpha = 0.6, linetype = &quot;dashed&quot;) + geom_point(aes(x = x, y = dgeom(x, prob = 0.1), color = &quot;p=0.1&quot;), show.legend = FALSE) + geom_path(aes(x = x, y = dgeom(x, prob = 0.1), color = &quot;p=0.1&quot;), show.legend = FALSE, alpha = 0.6, linetype = &quot;dashed&quot;) + labs(title = &quot;Distribución geométrica&quot;, y = &quot;&quot;) dists &lt;- ggplot(data_frame(x = -1:20), aes(x)) + stat_function(fun = pgeom, args = list(p = 0.5), aes(colour = &quot;p=0.5&quot;), alpha = 0.8) + stat_function(fun = pgeom, args = list(p = 0.1), aes(colour = &quot;p=0.1&quot;), alpha = 0.8) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color = &quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Variables aleatorias continuas Una variable aleatoria \\(X\\) es continua si existe una función \\(p_x\\) tal que \\(p_X(x) \\geq 0\\) para toda \\(x\\), \\(\\int_{-\\infty}^{\\infty}p_X(x)dx=1\\) y para toda \\(a\\leq b\\), \\[P(a &lt; X &lt; b) = \\int_{a}^b p_X(x)dx\\] La función \\(p_X(x)\\) se llama la función de densidad de probabilidad (fdp). Tenemos que \\[P_X(x)=\\int_{-\\infty}^x p_X(t)dt\\] y \\(p_X(x)=P_X^{\\&#39;}(x)\\) en todos los puntos \\(x\\) en los que la FDA \\(P_X\\) es diferenciable. Ejemplo. Supongamos que elegimos un número al azar entre cero y uno, entonces \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{b-a} &amp; x \\in [0, 1]\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] es claro que \\(p_X(x) \\geq 0\\) para toda \\(x\\) y \\(\\int_{-\\infty}^{\\infty}p_X(x)dx=1\\), la FDA esta dada por \\[ P_X(x) = \\left\\{ \\begin{array}{lr} 0 &amp; x &lt; 0 \\\\ x &amp; x \\in [0,1]\\\\ 1 &amp; x&gt;b \\\\ \\end{array} \\right. \\] Vale la pena notar que en el caso de variables aleatorias continuas \\(P(X=x)=0\\) para toda \\(x\\) y pensar en \\(p_X(x)\\) como \\(P(X=x)\\) solo tiene sentido en el caso discreto. Familias Continuas importantes Distribución Uniforme \\(X\\) tiene una distribución \\(Uniforme(a,b)\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{b-a} &amp; x \\in [a,b]\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] donde \\(a &lt; b\\). La función de distribución acumualda es \\[ P_X(x) = \\left\\{ \\begin{array}{lr} 0 &amp; x &lt; a \\\\ \\frac{x-a}{b-a} &amp; x \\in [a,b]\\\\ 1 &amp; x&gt;b \\\\ \\end{array} \\right. \\] \\(E(X) = (a+b)/2, Var(X)= (b-a)^2/12\\) densidades &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = dunif, aes(colour = &quot;a=0; b=1&quot;), show.legend = FALSE) + stat_function(fun = dunif, args = list(min = -5, max = 5), aes(colour = &quot;a=-5; b=5&quot;), show.legend = FALSE) + stat_function(fun = dunif, args = list(min = 0, max = 2), aes(colour = &quot;a=0; b=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución uniforme&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = punif, aes(colour = &quot;a=0; b=1&quot;), show.legend = FALSE) + stat_function(fun = punif, args = list(min = -5, max = 5), aes(colour = &quot;a=-5; b=5&quot;), show.legend = FALSE) + stat_function(fun = punif, args = list(min = 0, max = 2), aes(colour = &quot;a=0; b=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;FDA&quot;) grid.arrange(densidades, dists, ncol = 3, newpage = FALSE) Distribución Normal \\(X\\) tiene una distribución normal con parámetros \\(\\mu\\) y \\(\\sigma\\), denotado \\(X\\sim N(\\mu, \\sigma^2)\\) si \\[p(x) = \\frac{1}{\\sigma\\sqrt{2\\pi}}exp\\bigg(-\\frac{1}{2\\sigma^2}(x-\\mu)^2\\bigg)\\] \\(E(X)=\\mu, Var(X)=\\sigma^2\\) donde \\(\\mu \\in \\mathbb{R}\\) y \\(\\sigma&gt;0\\). Decimos que \\(X\\) tiene una distribución Normal estándar si \\(\\mu=0\\) y \\(\\sigma=1\\). Una variable aleatoria Normal estándar se denota tradicionalmente por \\(Z\\), su función de densidad de probabilidad por \\(\\phi(z)\\) y la función de probabilidad acumulada por \\(\\Phi(z)\\). Algunas porpiedades importantes son: Si \\(X \\sim N(\\mu, \\sigma^2)\\), entonces \\(Z=(X-\\mu)/\\sigma \\sim N(0,1)\\). Si \\(Z \\sim N(0, 1)\\) entonces \\(X = \\mu + \\sigma Z \\sim N(\\mu, \\sigma^2)\\). Si \\(X_i \\sim N(\\mu_i, \\sigma_i^2)\\), \\(i=1,...,n\\) independientes, entonces: \\[\\sum_{i=1}^n X_i \\sim N(\\sum_{i=1}^n \\mu_i, \\sum_{i=1}^n \\sigma_i^2)\\] Se sigue de 1 que si \\(X\\sim N(\\mu, \\sigma^2)\\), entonces \\[P(a&lt;X&lt;b) = P\\big(\\frac{a-\\mu}{\\sigma} &lt; Z &lt; \\frac{b-\\mu}{\\sigma}\\big)= \\Phi\\big(\\frac{b-\\mu}{\\sigma}\\big) - \\Phi\\big(\\frac{a-\\mu}{\\sigma}\\big)\\] densidades &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = dnorm, aes(colour = &quot;m=0; s=1&quot;), show.legend = FALSE) + stat_function(fun = dnorm, args = list(mean = 1), aes(colour = &quot;m=1; s=1&quot;), show.legend = FALSE) + stat_function(fun = dnorm, args = list(sd = 2), aes(colour = &quot;m=1; s=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución Normal&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(-5 , 5)), aes(x)) + stat_function(fun = pnorm, aes(colour = &quot;m=0; s=1&quot;), show.legend = FALSE) + stat_function(fun = pnorm, args = list(mean = 1), aes(colour = &quot;m=1; s=1&quot;), show.legend = FALSE) + stat_function(fun = pnorm, args = list(sd = 2), aes(colour = &quot;m=1; s=2&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;FDA&quot;) cuantiles &lt;- ggplot(data_frame(x = c(0, 1)), aes(x)) + stat_function(fun = qnorm, aes(colour = &quot;m=0; s=1&quot;)) + stat_function(fun = qnorm, args = list(mean = 1), aes(colour = &quot;m=1; s=1&quot;)) + stat_function(fun = qnorm, args = list(sd = 2), aes(colour = &quot;m=1; s=2&quot;)) + labs(y = &quot;&quot;, title = &quot;Funciones de cuantiles&quot;, colour = &quot;&quot;) grid.arrange(densidades, dists, cuantiles, ncol = 3, newpage = FALSE) Distribución Exponencial Una variable aleatoria \\(X\\) tienen distribución Exponencial con parámetro \\(\\beta\\), \\(X \\sim Exp(\\beta)\\) si, \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{\\beta}e^{-x/\\beta} &amp; x &gt;0\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=\\beta, Var(X)=\\beta^2\\) donde \\(\\beta &gt; 0\\). La distribución exponencial se utiliza para modelar tiempos de espera hasta un evento, por ejemplo modelar el tiempo de vida de un componente electrónico o el tiempo de espera entre llamadas telefónicas. Distribución Gamma Comencemos definiendo la función Gamma: para \\(\\alpha&gt;0\\), \\(\\Gamma(\\alpha)=\\int_0^{\\infty}y^{\\alpha-1}e^{-y}dy\\), esta función es una extensión de la función factorial, tenemos que si \\(n\\) es un entero positivo, \\(\\Gamma(n)=(n-1)!\\). Ahora, \\(X\\) tienen una distribución Gamma con parámetros \\(\\alpha\\), \\(\\beta\\), denotado como \\(X \\sim Gamma(\\alpha, \\beta)\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{1}{\\beta^\\alpha \\Gamma(\\alpha)}x^{\\alpha-1}e^{-x/\\beta} &amp; x &gt;0\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=\\alpha \\beta, Var(X)=\\alpha \\beta^2\\) Vale la pena notar que una distribución exponencial es una \\(Gamma(1, \\beta)\\). Una propiedad adicional es que si \\(X_i \\sim Gamma(\\alpha_i, \\beta)\\) independientes, entonces \\(\\sum_{i=1}^n X_i \\sim Gamma(\\sum_{i=1}^n \\alpha_i, \\beta)\\). En la práctica la distribución Gamma se ha usado para modelar el tamaño de las reclamaciones de asegurados, en neurociencia se ha usado para describir la distribución de los intervalos entre los que ocurren picos. Finalmente, la distribución Gamma es muy usada en estadística bayesiana como a priori conjugada para el parámetro de precisión de una distribución Normal. densidades &lt;- ggplot(data_frame(x = c(0 , 12)), aes(x)) + stat_function(fun = dgamma, args = list(shape = 1), aes(colour = &quot;a=1;b=1&quot;), show.legend = FALSE) + stat_function(fun = dgamma, args = list(scale = 0.5, shape = 2), aes(colour = &quot;a=2;b=0.5&quot;), show.legend = FALSE) + stat_function(fun = dgamma, args = list(scale = 3, shape = 4), aes(colour = &quot;a=4,b=3&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución Gamma&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(0 , 12)), aes(x)) + stat_function(fun = dgamma, args = list(shape = 1), aes(colour = &quot;a=1;b=1&quot;)) + stat_function(fun = dgamma, args = list(scale = 0.5, shape = 2), aes(colour = &quot;a=2;b=0.5&quot;)) + stat_function(fun = dgamma, args = list(scale = 3, shape = 4), aes(colour = &quot;a=4,b=3&quot;)) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color=&quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) Distribución Beta \\(X\\) tiene una distrinución Beta con parámetros \\(\\alpha &gt; 0\\) y \\(\\beta &gt;0\\), \\(X \\sim Beta(\\alpha, \\beta)\\) si \\[ p(x) = \\left\\{ \\begin{array}{lr} \\frac{\\Gamma(\\alpha+\\beta)}{\\Gamma(\\alpha)\\Gamma(\\beta)}x^{\\alpha-1}(1-x)^{\\beta-1} &amp; 0 &lt; x &lt; 1\\\\ 0 &amp; e.o.c. \\\\ \\end{array} \\right. \\] \\(E(X)=\\alpha/(\\alpha+\\beta), Var(X)=\\alpha \\beta /[(\\alpha+\\beta)^2(\\alpha + \\beta + 1)]\\) La distribución Beta se ha utilizado para describir variables aleatorias limitadas a intervalos de longitud finita, por ejemplo, distribución del tiempo en sistemas de control o administración de proyectos, proporción de minerales en rocas, etc. densidades &lt;- ggplot(data_frame(x = c(0 , 1)), aes(x)) + stat_function(fun = dbeta, args = list(shape1 = 2, shape2 = 2), aes(colour = &quot;a=2; b=2&quot;), show.legend = FALSE) + stat_function(fun = dbeta, args = list(shape1 = 5, shape2 = 2), aes(colour = &quot;a=5; b=2&quot;), show.legend = FALSE) + stat_function(fun = dbeta, args = list(shape1 = .5, shape2 = .5), aes(colour = &quot;a=.5; b=.5&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, title = &quot;Distribución Beta&quot;, colour = &quot;&quot;) dists &lt;- ggplot(data_frame(x = c(0 , 1)), aes(x)) + stat_function(fun = pbeta, args = list(shape1 = 2, shape2 = 2), aes(colour = &quot;a=2; b=2&quot;)) + stat_function(fun = pbeta, args = list(shape1 = 5, shape2 = 2), aes(colour = &quot;a=5; b=2&quot;)) + stat_function(fun = pbeta, args = list(shape1 = .5, shape2 = .5), aes(colour = &quot;a=.5; b=.5&quot;)) + labs(y = &quot;&quot;, title = &quot;FDA&quot;, color=&quot;&quot;) grid.arrange(densidades, dists, ncol = 2, newpage = FALSE) "],
["simulación-de-variables-aleatorias.html", "7.3 Simulación de variables aleatorias", " 7.3 Simulación de variables aleatorias Veremos métodos generales para simular muestras de distribuciones univariadas, generales se refiere a que se pueden utilizar independientemente de la forma de la función de densidad. Para utilizar estos métodos debemos tener un generador de números aleatorios confiable, pues la mayoría de los métodos consisten en una transformación de números aleatorios. Variables aletaorias discretas Método de Inversión Supongamos que deseamos generar el valor de una variable aleatoria discreta \\(X\\) con función de probabilidad: \\[P(X=x_j) = p_j\\] con \\(j=0,1,2,..\\). Para lograr esto generamos un número aleatorio \\(U\\), esto es \\(U\\sim Uniforme(0,1)\\) y definimos \\[ X = \\left\\{ \\begin{array}{lr} x_0 &amp; U &lt; p_0\\\\ x_1 &amp; p_0 \\leq U &lt; p_0 + p_1\\\\ \\vdots &amp;\\\\ x_j &amp; \\sum_{i=0}^{j-1}p_i \\leq U &lt; \\sum_{i=0}^j p_i \\\\ \\vdots &amp; \\\\ \\end{array} \\right. \\] Como para \\(0&lt;a&lt;b&lt;1\\) tenemos que \\(P(a\\leq U &lt; b)=b-a\\), tenemos que \\[P(X=x_j)=P\\bigg\\{\\sum_{i=0}^{j-1}p_i \\leq U &lt; \\sum_{i=0}^{j}p_i \\bigg \\}=p_j\\] y por tanto \\(X\\) tiene la distribución deseada. Método de inversión Genera un número aleatorio \\(U\\), tal que \\(U \\in (0,1)\\). Si \\(U&lt;p_0\\) define \\(X=x_0\\) y para. Si \\(U&lt; p_0+p_1\\) define \\(X = x_1\\) y para. Si \\(U &lt; p_0 + p_1 + p_2\\) define \\(X=x_2\\) y para. \\(\\vdots\\) Si las \\(x_i\\), están ordenadas de tal manera que \\(x_0&lt;x_1&lt;x_2&lt;\\cdots\\) y si denotamos por \\(P\\) la función de distribución acumulada de \\(X\\), entonces \\(P(x_k)=\\sum_{i=0}^kp_i\\) y por tanto, \\(X\\) será igual a \\(x_j\\) si \\[P(x_{j-1}) \\leq U \\leq P(x_j)\\] En otras palabras, tras generar un número aleatorio \\(U\\) determinamos el valor de \\(X\\) encontrando el intervalo \\([P(x_{j-1}),P(x_j))\\) en el que cae \\(U\\), esto es equivalente a encontrar la inversa de \\(P(U)\\). El tiempo que uno tarda en generar una variable aleatoria discreta usando el método de arriba es proporcional al número de intervalos que uno debe buscar, es por esto que en ocasiones vale la pena considerar los posibles valores \\(x_j\\) en orden decreciente de \\(p_j\\). Utiliza la función runif de R y el método de inversión para generar 1000 simulaciones de una variable aleatoria \\(X\\) tal que \\(p_1=0.20, p_2= 0.15, p_3=0.25, p_4=0.40\\) donde \\(p_j=P(X=j)\\). Ejemplos Uniforme discreta. Supongamos que deseamos simular de una variable aleatoria uniforme discreta que toma valores \\(1,...,k\\), usando los resultados anteriores tenemos que: \\(X=j\\) si \\(\\frac{j-1}{n} \\leq U &lt; \\frac{j}{n}\\) Entonces \\(X=[kU] + 1\\), donde \\([x]\\) representa la parte entera de x. # uniforme discreta: donde n es el número de simulaciones y k el número de elementos runifD &lt;- function(n = 1, k) floor(k * runif(n)) + 1 # veamos un histograma de 1000 simulaciones de una distribución Uniforme # discreta con parámetro k = 20 x &lt;- runifD(n = 1000, k = 20) qplot(x, binwidth = 1) También podmeos usar la función sample de R: qplot(sample(1:20, size = 1000, replace= TRUE), binwidth = 1) Poisson: la clave para usar el método de la transformación inversa en este ejemplo es notar que: \\[p_{i+1}=\\frac{\\lambda}{i+1}p_i\\] donde \\(p_i=P(X=i) = e^{-\\lambda} \\cdot \\lambda^i/i!\\), con \\(i=0,1,...\\). Ahora, la cantidad \\(i\\) se refiere al valor que estamos considerando, \\(p=p_i\\) es la probabilidad de \\(X = i\\) y \\(P=P(i)\\) es la probabilidad de \\(X\\leq i\\). Entonces, para generar una observación sequimos los siguientes pasos: Generar un número aleatorio \\(U\\), tal que \\(U \\in (0,1)\\). Inicializar: \\(i=0\\), \\(p=e^{-\\lambda}\\), \\(F=p\\). Si \\(U&lt;F\\), definir \\(X=i\\) y parar. \\(p=\\lambda p/(i+1)\\), \\(F=F+p\\), \\(i=i+1\\). Volver a 3. # Poisson usando Inversión rpoisI &lt;- function(lambda = 1){ U &lt;- runif(1) i &lt;- 0 p &lt;- exp(-lambda) P &lt;- p while(U &gt;= P){ p &lt;- lambda * p / (i + 1) P &lt;- P + p i &lt;- i + 1 } i } sims_pois &lt;- rerun(1000, rpoisI()) %&gt;% flatten_dbl() qplot(sims_pois, binwidth = 1) El algoritmo que propusimos verifica de manera sucesiva si el valor es 0, 1, etc. por lo que el número de comparaciones necesarias será uno más que el valor de la variable. Ahora, el valor esperado de una variable aleatoria Poisson es \\(\\lambda\\) por lo que en promedio se harían \\(1+\\lambda\\) busquedas. Cuando \\(\\lambda\\) es grande se puede mejorar el algoritmo buscando primero en valores cercanos a \\(\\lambda\\). Escribe una función en R que genere simulaciones de una variable aleatoria Poisson de la siguiente manera: define \\(I=[\\lambda]\\), y usa que \\(p_{i+1}=\\lambda p_i /(i+1)\\) para determinar \\(F\\) de manera recursiva. Genera un número aleatorio \\(U\\), determina si \\(X \\leq I\\) comparando si \\(U \\leq F(I)\\). Si \\(X \\leq I\\) busca hacia abajo comenzando en \\(I\\), de lo contrario busca hacia arriba comenzando por \\(I+1\\). Compara el tiempo que tardan los dos algoritmos en 5000 simulaciones de una variable aleatoria Poisson con parámetro \\(\\lambda=10, 200, 500\\). Aceptación y rechazo Supongamos que tenemos un método eficiente para generar simulaciones de una variable aleatoria con función de probabilidad masa \\(\\{q_j, j\\geq 0\\}\\), podemos usarla como la base para simular de una distribución que tiene función de probabilidad masa \\(\\{p_j, j \\geq 0\\}\\), para hacer esto comenzamos simulando una variable aleatoria \\(Y\\) con función \\(\\{q_j\\}\\) y después aceptamos o rechazamos el valor simulado con una probabilidad proporcional a \\(p_Y/q_Y\\). En particular, sea \\(c\\) una constante tal que \\[\\frac{p_j}{q_j}\\leq c\\] para toda \\(j\\) con \\(p_j &gt; 0\\). Entonces el método de aceptación y rechazo para simular una variable aleatoria \\(X\\) con función masa de probabilidad \\(p_j=P(x=j)\\) es como sigue: Método de aceptación y rechazo Simula el valor de \\(Y\\), con función de probabilidad masa \\(q_j\\). Genera un número aleatorio \\(U\\), tal que \\(U \\in (0,1)\\). Si \\(U &lt; p_y/(cq_y)\\) definimos \\(X=Y\\) y paramos, en otro caso regresamos a 1. En promedio este algoritmo requiere \\(c\\) iteraciones para obtener un valor generado para \\(X\\). . Supongamos que queremos simular el valor de una variable aleatoria \\(X\\) que toma uno de los valores \\(1,2,3,4\\) con probabilidades \\(p_1=0.20, p_2= 0.15, p_3=0.25, p_4=0.40\\) donde \\(p_j=P(X=j)\\). Usemos el método de aceptación y rechazo con \\(q\\) la densidad uniforme en \\(1,...,4\\). ¿Cómo se compara en velocidad con la función que implementaste usando el método de la transformación inversa? Variables aleatorias continuas Método de inversión Sea \\(U\\) una variable aleatoria con ditribución \\(U(0,1)\\). Para cualquier función de distribución \\(F_X\\) (\\(F_X\\) es creciente y continua entonces existe \\(F_X^{-1}\\)) la variable aleatoria \\(X\\) definida como: \\[X = F_X^{-1}(U)\\] tiene distribución \\(F_X\\). La proposición anterior nos da un camino para simular variables aleatorias continuas: Generamos un número aleatorio \\(U\\). Definimos \\(X = F^{-1}(U)\\): ggplot(data_frame(x = c(-2 , 2)), aes(x)) + geom_hline(yintercept = 0, color = &quot;gray&quot;) + geom_vline(xintercept = 0, color = &quot;gray&quot;) + stat_function(fun = qnorm, aes(color = &quot;fq&quot;)) + stat_function(fun = dnorm, aes(color = &quot;fdp&quot;)) + stat_function(fun = pnorm, aes(color = &quot;fda&quot;)) + coord_fixed() + labs(color = &quot;&quot;, title = &quot;Método de transformación inversa caso Normal&quot;) Ejemplo: Exponencial Si \\(X\\) es una variable aleatoria exponencial con tasa 1, entonces \\[F(x)=1-e^{-x}\\] Si definimos \\(x=F^{-1}(u)\\), entonces \\[u=F(x)=1-e^{-x}\\] o \\[x = -log(1-u)\\] Vale la pena notar que si \\(U\\) tiene distribución \\(U(0,1)\\), \\(1-U\\) también se distribuye uniforme(0,1). simExp &lt;- function(){ u &lt;- runif(1) x &lt;- -log(u) } Notemos que para cualquier constante positiva \\(c\\), \\(cX\\) tiene distribución exponencial con media \\(c\\), por tanto una variable aleatoria exponencial con parámetro \\(\\beta\\) se puede generar de la siguiente manera: \\[X=-\\beta log(U)\\] sim_exp_beta &lt;- function(beta, n = 1000){ -beta * log(runif(n)) } sims_exp &lt;- sim_exp_beta(2) mean(sims_exp) #&gt; [1] 2.0629 ggplot() + geom_histogram(aes(x = sims_exp, y = ..density..), binwidth = 0.7) El algoritmo anterior también provee una manera de generar variables aleatorias Poisson. Para esto usamos la relación entre la Poisson y la exponencial: Un proceso Poisson con tasa \\(\\lambda\\) resulta cuando los tiempos de espera entre eventos sucesivos son exponenciales independientes con parámetro \\(\\beta = 1/\\lambda\\), Para este proceso, \\(N(1)\\), el número de eventos en el tiempo 1 se distribuye Poisson con media \\(1/\\beta\\). Si denotamos por \\(X_i\\) los tiempos entre eventos, el \\(n\\)-ésimo evento ocurrirá en el tiempo \\(\\sum_{i=1}^n X_i\\) y por tanto el número de eventos al tiempo 1 se puede expresar como: \\[N(1)=max\\bigg\\{n: \\sum_{i=1}^nX_i \\leq 1\\bigg\\}\\] Esto es, el número de eventos al tiempo 1 es igual a la \\(n\\) mayor para la cual el n-ésimo evento ocurrió al tiempo 1. Por ejemplo, si el cuarto evento ocurrió al tiempo uno pero el quinto no, habría 4 eventos al tiempo 1. Por tanto podemos generar una variable aleatoria Poisson con media \\(\\lambda = 1/\\beta\\) generando números aleatorios \\(U_1,...U_n,...\\) y definiendo \\[N=max\\bigg\\{n: \\sum_{i=1}^n -\\beta log(U_i) \\le 1\\bigg\\}\\] \\[=max\\bigg\\{n: \\sum_{i=1}^n -1/\\lambda log(U_i) \\le 1\\bigg\\}\\] \\[=max\\bigg\\{n:\\sum_{i=1}^n log(U_i)\\geq -\\lambda \\bigg\\}\\] \\[=max\\{n:log(U_1\\cdot\\cdot\\cdot U_n) \\geq -\\lambda\\}\\] \\[=max\\{n: U_1\\cdot \\cdot \\cdot U_n \\geq e^{-\\lambda}\\}\\] Es así, que una variable aleatoria Poisson con media \\(\\lambda\\) se puede generar a partir de una sucesión de números aleatorios, generando números hasta que el producto sea menor a \\(e^{-\\lambda}\\) y definiendo \\(X\\) como uno menos del número de números aleatorios requeridos. \\[N = min\\{n: U_1\\cdot\\cdot\\cdot U_n &lt; e^{-\\lambda}\\} - 1\\] poisson &lt;- function(lambda){ u &lt;- runif(1) N &lt;- 1 while (u &gt; exp(-lambda)) { u &lt;- u * runif(1) N &lt;- N + 1 } N - 1 } poisson(10) #&gt; [1] 10 mean(rdply(1000, poisson(10))$V1) #&gt; [1] 10.073 Ejemplo: Gamma Supongamos que deseamos generar el valor de una variable aleatoria \\(Gamma(n,\\beta)\\), la función de distribución es, \\[\\int_{0}^x \\frac{1}{\\beta^n \\Gamma(n)}x^{n-1}e^{-x/\\beta}dy\\] La inversa de la función de distribución acumulada anterior no se puede escribir de forma cerrada. Sin embargo, podeos usar que una \\(Gamma(n,\\beta)\\) se puede ver como la suma de \\(n\\) exponenciales independientes, cada una con parámetro \\(\\beta\\): \\[X=-\\beta log(U_1)-\\cdot\\cdot\\cdot - \\beta log(U_n)\\] \\[=-\\beta log(U_1\\cdot\\cdot\\cdot U_n)\\] donde la identidad \\(\\sum log(x_i) = log(x_1\\cdot\\cdot\\cdot x_n)\\) deriva en ganancias computacionales. gamma_nb &lt;- function(n, beta){ -beta * log(Reduce(`*`,runif(10))) } sims_gamma &lt;- rdply(1000, gamma_nb(n = 10, beta = 2)) mean(sims_gamma$V1) #&gt; [1] 19.77 var(sims_gamma$V1) #&gt; [1] 37.19 Aceptación y rechazo Supongamos que tenemos un método para generar variables aleatorias con función de densidad \\(g(x)\\), podemos usarla como base para generar observaciones de una variable aleatoria con densidad \\(f(x)\\) generando \\(Y\\) de \\(g\\) y después aceptando el valor generado con una probabilidad proporcional a \\(f(Y)/g(Y)\\). Sea \\(c\\) una constante tal que \\[\\frac{f(y)}{g(y)} \\leq c\\] para toda \\(c\\), entonces el método se puede escribir como sigue: Aceptación y rechazo Genera \\(Y\\) con densidad \\(g\\). Genera un número aleatorio \\(U\\). Si \\(U \\leq \\frac{f(Y)}{cg(Y)}\\) define \\(X=Y\\), de lo contrario regresa a 1. El método de aceptación y rechazo es análogo al correspondiente a variables aleatorias discretas. La variable aleatoria generada usando el método de aceptación y rechazo tiene densidad \\(f\\). El número de iteraciones del algoritmo que se necesitan es una variable aleatoria geométrica con media \\(c\\). Ejemplo: Beta Usemos el método de aceptación y rechazo para generar observaciones de una variable aleatoria \\(beta(2,4)\\): \\[f(x)=20x(1-x)^3\\] La variable aleatoria beta toma valores en el intervalo (0,1) por lo que consideremos \\(g(x)=1\\), para \\(0&lt;x&lt;1\\). Para determinar la menor \\(c\\) tal que \\(f(x)/g(x)\\leq c\\) podemos derivar y obtenemos \\(c = 135/64\\), \\[\\frac{f(x)}{g(x)} \\leq 20 \\cdot \\frac{1}{4} \\bigg(\\frac{3}{4}\\bigg)^3 = \\frac{135}{64}\\] y \\[\\frac{f(x)}{cg(x)}=\\frac{256}{27}x(1-x)^3\\] por lo que el procedimiento para simular sería el siguiente: beta24 &lt;- function(){ # 1. Generar dos números aleatorios U_1, U_2. u1 &lt;- runif(1) u2 &lt;- runif(1) # 2. Comparar con f(x)/cg(x) while (u2 &gt; 256 / 27 * u1 * (1 - u1) ^ 3) { u1 &lt;- runif(1) u2 &lt;- runif(1) } u1 } sims &lt;- rdply(1000, beta24) mean(sims$V1) #&gt; [1] 0.334 Ejemplo: Gamma(3/2, 1) Supongamos que deseamos generar simulaciones de una variable aleatoria con densidad gamma(3/2, 1): \\[f(x)=\\frac{1}{\\Gamma(3/2)}x^{1/2}e^{-x}\\] dado que la variable aleatoria de nuestro interés se concentra en los números positivos, y tiene media \\(3/2\\), es conveniente usar el método de aceptación y rechazo con la variable aleatoria exponencial de la misma media. \\[g(x)=\\frac{2}{3}e^{-2x/3}\\] Usa el método de aceptación y rechazo para generar 1000 observaciones de una variable aleatoria con distribución gamma(3/2,1). Ejemplo: Variable aleatoria normal Nuestro objetivo es primero, simular una variable aleatoria normal estándar Z, para ello comencemos notando que el valor absoluto de Z tiene función de densidad: \\[f(x)=\\frac{2}{\\sqrt{2\\pi}}e^{-x^2/2}\\] con soporte en los reales positivos. Generaremos observaciones de la densidad anterior usando el método de aceptación y rechazo con \\(g\\) una densidad exponencial con media 1: \\[g(x)= e^{-x}\\] Ahora, \\(\\frac{f(x)}{g(x)}=\\sqrt{2/\\pi}e^{x - x^2/2}\\) y por tanto el máximo valor de \\(f(x)/g(x)\\) ocurre en el valor \\(x\\) que maximiza \\(x - x^2/2\\), esto ocurre en \\(x=1\\), y podemos tomar \\(c=\\sqrt{2e/\\pi}\\), \\[\\frac{f(x)}{cg(x)}=exp\\bigg\\{x - \\frac{x^2}{2}-{1}{2}\\bigg\\}\\] \\[=exp\\bigg\\{\\frac{(x-1)^2}{2}\\bigg\\}\\] y por tanto podemos generar el valor absoluto de una variable aleatoria con distribución normal estándar de la siguiente manera: Genera \\(Y\\) una variable aleatoria exponencial con tasa 1. Genera un número aleatorio \\(U\\). Si \\(U \\leq exp\\{-(Y-1)^2/2\\}\\) define \\(X=Y\\), en otro caso vuelve a 1. Para generar una variable aleatoria con distribución normal estándar \\(Z\\) simplemente elegimos \\(X\\) o \\(-X\\) con igual probabilidad. Notemos además que en paso 3 \\(Y\\) es aceptado si \\(U \\leq exp(-(Y-1)^2/2)\\) esto es equivalente a \\(-log(U) \\geq (Y-1)^2/2\\) y recordemos que \\(-log(U)\\) es exponencial con parámetro 1, por lo que podems escribir los pasos como: Genera 2 exponenciales independientes con parámetro 1: \\(Y_1, Y_2\\). Si \\(Y_2 \\geq (Y_1 - 1)^2/2\\) define \\(X=Y\\), de lo contrario vuelve a 1. Supongamos ahora que aceptamos \\(Y_1\\), esto es equivalente a decir que \\(Y_2\\) es mayor a \\((Y_1 - 1)^2/2\\), y la diferencia \\(Y_2 - (Y_1 - 1)^2/2\\) se distribuye exponencial con parámetro 1. Esto es, cuando aceptamos en el segundo paso no sólo obtenemos \\(X\\) sino que calculando \\(Y_2 - (Y_1 - 1)^2/2\\) podemos generar una variable aleatoria exponencial con parámetro 1 independiente de \\(X\\). Esto es relevante pues si estamos generando una sucesión de variables aleatorias normales obtendríamos un algoritmo más eficiente. "],
["simulación-de-modelos.html", "Sección 8 Simulación de modelos", " Sección 8 Simulación de modelos ¿Para qué simular de un modelo? Alternativa para presentar inferencias: en lugar de presentar un estimador puntual y/o intervalo de confianza podemos analizar simulaciones del modelo. Inferencia predictiva: es fácil usar simulación para calcular errores estándar, o intervalos de confianza, resulta particularmente útil cuando estamos estimando cantidades que no son coeficientes de un modelo o transformaciones lineales de coeficientes. Simulación para revisar el ajuste de un modelo. Podemos simular datos del modelo ajustado y compararlos con los datos verdaderos. Simulación para calcular tamaños de muestra. "],
["distribuciones-multivariadas.html", "8.1 Distribuciones multivariadas", " 8.1 Distribuciones multivariadas Hasta ahora hemos estudiado distribuciones univariadas y como simular de ellas, sin embargo, es común que un modelo probabilístico involucre más de una variable aleatoria por lo que estudiaremos el concepto de distribuciones de probabilidad multivariadas. La distribución conjunta sobre un conjunto de variables aleatorias \\(\\{X_1,...,X_n\\}\\), que denotamos \\(p(x_1,...,x_n)\\), asigna probabilidades a todos los eventos determinados por el conjunto de variables aleatorias. En el caso discreto bivariado, dado las variables aleatorias discretas \\(X\\) y \\(Y\\), definimos la función de densidad conjunta como \\(f(x,y)=P(X=x, Y=y)\\). Ejemplo. Consideremos una distribución sobre la población de departamentos en renta de Hong Kong, el espacio de resultados es el conjunto de todos los departamentos en la población. En muchas ocasiones buscamos resolver preguntas que involucran más de una variable aleatoria, en este ejemplo nos interesan: Renta mensual: toma los valores baja (≤1k), media ((1k,5k]), media alta ((5k,12k]) y alta (&gt;12k). Tipo de departamento: toma 3 valores, público, privado u otros. La distribución conjunta de variables aleatorias discretas se puede representar por medio de tablas. Renta/Tipo público privado otros baja 0.17 0.01 0.02 media 0.44 0.03 0.01 media alta 0.09 0.07 0.01 alta 0 0.14 0.10 En el caso continuo bivariado, decimos que la función \\(p(x,y)\\) es una función de densidad de probabilidad para las variables aleatorias \\((X,Y)\\) si: 1. \\(p(x,y) \\geq 0\\) para toda \\((x,y)\\). \\(\\int_{-\\infty}^{\\infty}p(x,y)dxdy=1\\). Para cualquier conjunto \\(A \\subset \\mathbb{R} \\times \\mathbb{R}\\), \\(P((X,Y) \\in A) = \\int\\int_A p(x,y)dxdy\\). Ejemplo. Sean \\((X,Y)\\) uniformes en el cuadrado unitario, entonces \\[ p(x,y) = \\left\\{ \\begin{array}{lr} 1, &amp; 0\\leq x \\leq 1,0\\leq y \\leq 1\\\\ 0, &amp; e.o.c. \\end{array} \\right. \\] Para encontrar \\(P(X &lt; \\frac{1}{2}, Y&lt;\\frac{1}{2})\\), esto es la probailidad del evento \\(A=\\{X&lt;1/2, Y&lt;1/2\\}\\). La integral de \\(p\\) sobre este subconjunto corresponde, en este caso, a calcular el área del conjunto \\(A\\) que es igual a \\(\\frac{1}{4}\\). De la distribución conjunta \\(p(x_1,...,x_n)\\) podemos obtener la distribución de únciamente una variable aleatoria \\(X_j\\), donde \\(X_j \\in \\{X_1,...,X_n\\}\\), la llamamos la distribución marginal de \\(X_j\\). Sea \\(\\{X_1,...,X_n\\}\\) un conjunto de variables aleatorias con distribución conjunta \\(p(x_1,...,x_n)\\), la distribución marginal de \\(X_j\\) (\\(j \\in \\{1,...,n\\}\\)) se define como, \\[p_{X_j}(x_j) = \\sum_{x_1,...,x_{j-1},x_{j+1},...,x_n}p(x_1,...,x_n)\\mbox{ en el caso discreto,}\\] \\[p_{X_j}(x_j) = \\int_{x_1,...,x_{j-1},x_{j+1},...,x_n}p(x_1,...,x_n)dx_1,...,dx_n\\mbox{ en el caso continuo}\\] Ejemplo. Retomando el problema de los departamentos, ¿Cuál es la probabilidad de que un departamento elegido al azar tenga renta baja? Probabilidad condicional Sean \\(A\\), \\(B\\) dos eventos, con \\(P(B)&gt;0\\), la probabilidad condicional de \\(A\\) dado \\(B\\) es \\[P(A|B)=\\frac{P(AB)}{P(B)}\\] Ejemplo. ¿Cuál es la probabilidad de que un departamento privado tenga renta baja? ¿Cómo se compara con la probabilidad de que la renta sea baja (desconozco el tipo de departamento)? La noción de probabilidad condicional se extiende a distribuciones condicionales: Sean \\(X\\), \\(Y\\) dos variables aleatorias con función de densidad conjunta \\(p(x,y)\\), entonces la función de densidad condicional de \\(X\\) dado \\(Y=y\\), para toda \\(y\\) tal que \\(p_Y(y) &gt; 0\\), se define como \\[p_{X\\vert Y}(x\\vert y) = \\frac{p(x, y)}{p_Y(y).}\\] Ejemplo. ¿Cuál es la distribución condicional de renta dado tipo privado? Para obtener toda la distribución condicional calculamos los dos casos restantes (renta media, media alta y alta). Vale la pena destacar que una distribución condicional es una distribución de probabilidad. En el ejemplo anterior, notemos que cada renglón de la tabla probabilidades suman uno, son no negativas y menores que uno. Probabilidad Total Sean \\(E\\), \\(F\\) dos eventos entonces, \\[P(E) = P(E\\vert F)P(F) + P(E\\vert F^c)P(F^c).\\] De manera más general, sean \\(F_i\\) \\(i = 1,...,n\\) eventos mutuamente excluyentes cuya unión es el espacio muestral, entonces \\[P(E) = \\sum_{i=1}^n P(E\\vert F_i)P(F_i).\\] Ejemplo. Supongamos que una aseguradora clasifica a la gente en tres grupos de acuerdo a su nivel de riesgo: bajo, medio y alto. De acuerdo a los registros, las probabilidades de incurrir en un accidente en un laspo de un año son \\(0.05\\), \\(0.15\\) y \\(0.30\\) respectivamente. Si el \\(20\\%\\) de la población se clasifica en riesgo bajo, \\(50\\%\\) en medio y \\(30\\%\\) en alto, ¿qué proporción de la población tiene un accidente en un año dado? Para variables aleatorias tenemos: Sean \\(X\\), \\(Y\\) dos variables aleatorias, podemos expresar la distribución marginal de \\(X\\) como: \\[p_X(x) = \\sum_{y} p_{X \\vert Y}(x\\vert y)p_Y(y).\\] Supongamos que ruedo un dado, si observo un número par lanzo una moneda justa (la probabilidad de observar águila es la misma que la de observar sol), si el dado muestra un número impar lanzo una moneda sesgada en la que la probabilidad de observar águila es \\(0.9\\). Si observo sol, ¿Cuál es la probabilidad de que haya lanzado la moneda sesgada? El ejercicio anterior introduce la noción de probabilidad inversa: inicialmente conozco la probabilidad de observar sol condicional a que la moneda es sesgada pero ahora me interesa conocer la probabilidad de que haya lanzado una moneda sesgada una vez que observé un sol en el volado. Regla de Bayes La regla de Bayes es una consecuencia de la definición de probabilidad condicional. Sean \\(F_i\\) y \\(i = 1,...,n\\) eventos mutuamente excluyentes cuya unión es el espacio muestral, entonces \\[P(F_j\\vert E) = \\frac{P(E\\vert F_j)P(F_j)}{\\sum_{i=1}^n P(E\\vert F_i)P(F_i)}\\] esta identidad se conoce como la regla de Bayes. Ejemplo. En el contexto del ejemplo de los seguros ahora nos hacemos la siguiente pregunta: si un asegurado tuvo accidentes en 2013, ¿cuál es la probabilidad de que clasifique en riesgo bajo? La intuición es engañosa: En estudios en Alemania y EUA, investigadores le pidieron a médicos que estimaran la probabilidad de que una mujer asintomática entre los \\(40\\) y \\(50\\) años tuviera cáncer de mama si su mamograma era positivo. Se les explicó que el \\(7\\%\\) de los mamogramas indican cáncer cuando no lo hay (falsos positivos). Adicional mente, se le explicó a los médicos que la incidencia de cáncer de mama en ese grupo de edad es \\(0.8\\%\\) y la tasa de falsos negativos de \\(10\\%\\). En Alemania, un tercio de los médicos determinaron que la probabilidad era cercana al \\(90\\%\\) y la mediana de las estimaciones fue \\(70\\%\\). En EUA \\(95\\) de \\(100\\) médicos estimaron que la probabilidad rondaba el \\(75\\%\\). ¿Cómo determinas la probabilidad de que una mujer con mamograma positivo tenga cáncer? Al igual que con probabilidad condicional, la Regla de Bayes tiene una definición análoga para variables aleatorias. Sean \\(X\\), \\(Y\\) dos variables aleatorias, \\[p_{X\\vert Y}(x\\vert y) = \\frac{p_{Y\\vert X}(y\\vert x)p_X(x)}{p_Y(y)}.\\] Supongamos ahora que una compañía de seguros divide a la gente en dos clases: propensos a accidente (30% de las personas) y no propensos a accidente. En un año dado aquellos propensos a accidentes sufren un accidente con probabilidad 0.4, mientras que los del otro grupo sufren un accidente con probabilidad 0.2. ¿Cuál es la probabilidad de que un asegurado tenga un accidente en su segundo año condicional a que sufrió un accidente en el primer año? Una consecuencia de la regla de Bayes es que cualquier distribución multivariada sobre \\(n\\) variables \\(X_1,X_2,...X_n\\) se puede expresar como: \\[p(x_1,x_2,...x_n) = p_{X_1}(x_1)p_{X_2\\vert X_1}(x_2\\vert x_1)p_{X_3\\vert X_1X_2}(x_3\\vert x_1x_2)···p_{X_n\\vert X_1...X_{n-1}}(x_n\\vert x_1...x_{n-1})\\] esta igualdad se conoce como regla de la cadena. Nótese que esta regla funciona para cualquier ordenamiento de las variables aleatorias. Independencia Los eventos \\(E\\), \\(F\\) son independientes sí y solo sí \\[P(EF) = P(E)P(F)\\] De la definición de independencia se sigue que \\(P(E\\vert F) = P(E)\\). Esto es, los eventos \\(E\\) y \\(F\\) son independientes si saber que uno de ellos ocurrió no afecta la probabilidad del otro. Utilizaremos la notación \\(E\\perp F\\) que se lee “\\(E\\) es independiente de \\(F\\)”. Dos variables aleatorias \\(X\\), \\(Y\\), son independientes sí y sólo sí \\[p(x,y) = p_X(x)p_Y(y)\\] Más aún, \\(X\\) y \\(Y\\) son independientes sí y sólo sí \\(p(x,y) \\propto g(x)h(y)\\), por lo que para demostrar independecia podemos omitir las constantes en la factorización de las densidades Similar a la independencia en eventos, la independencia de variables aleatorias implica que \\(p_{X\\vert Y}(x\\vert y) = p_X(x)\\), esto es, \\(Y = y\\) no provee información sobre \\(X\\). Ejemplo. Consideremos la función de densidad conjunta \\(p(x,y) = \\frac{1}{384} x^2y^4e^{-y-(x/2)}\\), \\(x&gt;0\\), \\(y&gt;0\\), ¿\\(X\\) y \\(Y\\) son independientes? Podemos definir \\[ g(x) = \\left\\{ \\begin{array}{lr} x^2e^{-x/2} &amp; : x &gt; 0\\\\ 0 &amp; : x \\le 0 \\end{array} \\right. \\] y \\[ h(y) = \\left\\{ \\begin{array}{lr} y^4e^{-y} &amp; : y &gt; 0\\\\ 0 &amp; : y \\le 0 \\end{array} \\right. \\] entonces \\(p(x,y) \\propto g(x)h(y)\\), para toda \\(x\\), \\(y\\) \\(\\in \\mathbb{R}\\) y concluímos que \\(X\\) y \\(Y\\) son independientes. **Ejemplo.*. Si la densidad conjunta de \\(X\\) y \\(Y\\) está dada por: \\[ p(x, y) = \\left\\{ \\begin{array}{lr} 2 &amp; : 0 &lt; x &lt; y, 0 &lt; y &lt; 1\\\\ 0 &amp; : e.o.c. \\end{array} \\right. \\] ¿\\(X\\) y \\(Y\\) son independientes? Ejercicio. Recordando el ejemplo de departamentos en Hong Kong, veamos si Renta y Tipo son independientes, para esto comparemos \\(p(renta|tipo)\\) y \\(p(renta)\\). 8.1.0.1 Independencia condicional La independencia de eventos o variables aleatorias es poco común en la práctica, más frecuente es el caso en que dos eventos son independientes dado un tercer evento. Ejemplo. En una competencia de velocidad, cada atleta se somete a dos pruebas de dopaje que buscan detectar si el deportista ingirió una substania prohibida. La prueba A consiste en un examen de sangre y la prueba B en un exámen de orina, cada prueba se realiza en un laboratorio distinto y no hay intercambio de información entre los laboratorios. Es razonable pensar que los resultados de los dos exámenes no son independientes. Ahora, supongamos que sabemos que el atleta consumió la substancia prohibida, en este caso podemos argumentar que conocer el resultado de la prueba A no cambia la probabilidad de que el atleta salga positivo en la prueba B. Decimos que el resultado de la prueba B es condicionalmente independiente del resultado de la prueba A dado que el atleta consumió la substancia. Sean \\(A\\), \\(B\\) y \\(C\\), tres eventos decimos que \\(A\\) es independiente de \\(B\\) condicional a \\(C\\) (\\(A \\perp B \\vert C\\)) si, \\[ P(A,B\\vert C) = P(A\\vert C)P(B\\vert C)\\] Similar al caso de independencia, \\(A\\) y \\(B\\) son condicionalmente independientes dado \\(C\\) sí y solo sí \\(P(A \\vert B,C) = P(A \\vert C)\\), esto es, una vez que conocemos el valor de \\(C\\), \\(B\\) no proporciona información adicional sobre \\(A\\). Ejemplo. Retomemos el ejercicio de asegurados. En la solución de este ejercicio utilizamos que \\(P(A_2|AA_1) = 0.4\\) y que \\(P(A_2|A^cA_1) = 0.2\\), al establecer esa igualdad estamos asumiendo que \\(A_2\\) (el asegurado tiene un accidente en el año 2) y \\(A_1\\) (el asegurado tiene un accidente en el año 1) son eventos condicionalmente independientes dado \\(A\\) (el asegurado es propenso a accidentes): \\(P(A_2|AA_1) = P(A_2|A) = 0.4\\) y \\(P(A_2|A^cA_1) = P(A_2|A^c) = 0.2\\). En el caso de variables aleatorias definimos independencia condicional como sigue. Sean \\(X\\), \\(Y\\) y \\(Z\\), tres variables aleatorias decimos que \\(X\\) es independiente de \\(Y\\) condicional a \\(Z\\) (\\(X \\perp Y \\vert Z\\)) si y sólo sí, \\[p(x,y\\vert z) = p_{X\\vert Z}(x\\vert z)p_{Y\\vert Z}(y\\vert z).\\] Y tenemos que \\(X\\) es independiente de \\(Y\\) condicional a \\(Z\\) sí y sólo sí, \\(p(x,y,z) \\propto g(x,z)h(y,z)\\). Ejemplo. Supongamos que ruedo un dado, si observo un número par realizo dos lanzamientos de una moneda justa (la probabilidad de observar águila es la misma que la de observar sol), si el dado muestra un número impar realizo dos lanzamientos de una moneda sesgada en la que la probabilidad de observar águila es 0.9. Denotemos por \\(Z\\) la variable aleatoria asociada a la selección de la moneda, \\(X_1\\) la correspondiente al primer lanzamiento y \\(X_2\\) la correspondiente al segundo. Entonces, \\(X_1\\) y \\(X_2\\) no son independientes, sin embargo, son condicionalmente independientes (\\(X_1 \\perp X_2 \\vert Z\\)), puesto que una vez que se que moneda voy a lanzar el resultado del primer lanzamiento no aporta información adicional para el segundo lanzamiento. Calcularemos la distribución conjunta y la distribución condicional de \\(X_2\\) dado \\(X_1\\). La distribución conjunta esta determinada por la siguiente tabla: Z X1 X2 P(Z,X1,X2) justa a a 0.125 justa a s 0.125 justa s a 0.125 justa s s 0.125 ses a a 0.405 ses a s 0.045 ses s a 0.045 ses s s 0.005 La distribución condicional \\(p(X_2|X_1)\\) es, X1/X2 a s . a 0.757 0.243 1 s 0.567 0.433 1 y la distribución condicional \\(p(X_2|X_1,Z)=p(X_2|Z)\\) es, X1/X2 Z a s . a par 0.5 0.5 1 s par 0.5 0.5 1 a impar 0.9 0.1 1 s impar 0.9 0.1 1 En este punto es claro que \\(X \\perp Y \\vert Z\\) no implica \\(X \\perp Y\\), pues como vimos en el ejemplo de las monedas \\(X_1 \\perp X_2 \\vert Z\\) pero \\(X_1 \\not \\perp X_2\\). Más aún, \\(X \\perp Y\\) tampoco implica \\(X \\perp Y \\vert Z\\). La independencia condicional tiene importantes consecuencias, por ejemplo, si \\(X\\) es independiente de \\(Y\\) dado \\(Z\\) entonces, \\[p(x,y,z) = p_Z(z)p_{X\\vert Z}(x\\vert z)p_{Y\\vert Z}(y\\vert z).\\] Esta expresión de la densidad conjunta es similar a la que obtendríamos usando la regla de la cadena; sin embargo, el número de parámetros necesarios bajo esta representación es menor lo que facilita la estimación. "],
["modelos-gráficos-y-simulación-predictiva.html", "8.2 Modelos gráficos y simulación predictiva", " 8.2 Modelos gráficos y simulación predictiva El objetivo de esta sección es la simulación de modelos, una manera conveniente de simular de un modelo probabilístico es a partir del modelo gráfico asociado. Un modelo gráfico representa todas las cantidades involucradas en el modelo mediante nodos de una gráfica dirigida, el modelo representa el supuesto que dados los nodos padres \\(padres(v)\\) cada nodo es independiente del resto de los nodos a excepción de sus descendientes. Los nodos en las gráficas se clasifican en 3 tipos: Constantes fijas por el diseño del estudio, siempre son nodos sin padres. Estocásticos son variables a los que se les asigna una distribución. Determinísticos son funciones lógicas de otros nodos. Los supuestos de independencia condicional que representa la gráfica implican que la distribución conjunta de todas las cantidades V tiene una factorización en términos de la distribución condicional \\(p(v|padres(v))\\) de tal manera que: \\[p(V) = \\prod p(v|padres(v))\\] Veamos como usar las gráficas para simular de modelos probabilísticos. Los siguientes ejemplos están escritos con base en Gelman and Hill (2007). Ejemplo de simulación discreta predictiva La probabilidad de que un bebé sea niña o niño es \\(48.8\\%\\) y \\(51.2\\%\\) respectivamente. Supongamos que hay 400 nacimientos en un hospital en un año dado. ¿Cuántas niñas nacerán? Comencemos viendo el modelo gráfico asociado. La gráfica superior muestra todas las variables relevantes en el problema, y las dependencias entre ellas. En este caso \\(n\\) es una constante que representa el número de nacimientos, (\\(n=400\\)), \\(p=48.8\\) es la probabilidad de que un nacimiento resulte en niña y \\(k \\sim Binomial(p, n)\\). Debido a que el número de éxitos (nacimientos que resultan en niña) depende de la tasa \\(p\\) y el número de experimentos \\(n\\), los nodos que representan a éstas dos últimas variables están dirigidos al nodo que representa \\(k\\). Una vez que tenemos la gráfica es fácil simular del modelo: library(ggplot2) library(dplyr) library(arm) library(tidyr) set.seed(918739837) n_ninas &lt;- rbinom(1, 400, 0.488) esto nos muestra algo que podría ocurrir en \\(400\\) nacimientos. Ahora, para tener una noción de la distribución simulamos el proceso \\(1000\\) veces: sims_ninas &lt;- rerun(1000, rbinom(1, 400, 0.488)) %&gt;% flatten_dbl() mean(sims_ninas) #&gt; [1] 195.773 sd(sims_ninas) #&gt; [1] 10.11582 ggplot() + geom_histogram(aes(x = sims_ninas), binwidth = 3, alpha = 0.7) El histograma de arriba representa la distribución de probabilidad para el número de niñas y refleja la incertidumbre en los nacimientos. Podemos agregar complejidad al modelo, por ejemplo con probabilidad \\(1/125\\) un nacimiento resulta en gemelos fraternales, y para cada uno de los bebés hay una posibilidad de aproximadamente \\(49.5\\%\\) de ser niña. Además la probabilidad de gemelos idénticos es de \\(1/300\\) y estos a su vez resultan en niñas en aproximadamente \\(40.5\\%\\) de los casos. Podemos simular 400 nacimientos bajo este modelo como sigue: tipo_nacimiento &lt;- sample(c(&quot;unico&quot;, &quot;fraternal&quot;, &quot;identicos&quot;), size = 400, replace = TRUE, prob = c(1 - 1 / 125 - 1 / 300, 1 / 125, 1 / 300)) n_unico &lt;- sum(tipo_nacimiento == &quot;unico&quot;) # número de nacimientos únicos n_fraternal &lt;- sum(tipo_nacimiento == &quot;fraternal&quot;) n_identicos &lt;- 400 - n_unico - n_fraternal n_ninas &lt;- rbinom(1, n_unico, 0.488) + rbinom(1, 2 * n_fraternal, 0.495) + # en cada nacimiento hay 2 bebés 2 * rbinom(1, n_identicos, 0.405) n_ninas #&gt; [1] 183 Repetimos la simulación 1000 veces para aproximar la distribución de número de niñas en 400 nacimientos. modelo2 &lt;- function(){ tipo_nacimiento &lt;- sample(c(&quot;unico&quot;, &quot;fraternal&quot;, &quot;identicos&quot;), size = 400, replace = TRUE, prob = c(1 - 1 / 125 - 1 / 300, 1 / 125, 1 / 300)) # número de nacimientos de cada tipo n_unico &lt;- sum(tipo_nacimiento == &quot;unico&quot;) # número de nacimientos únicos n_fraternal &lt;- sum(tipo_nacimiento == &quot;fraternal&quot;) n_identicos &lt;- 400 - n_unico - n_fraternal # simulamos para cada tipo de nacimiento n_ninas &lt;- rbinom(1, n_unico, 0.488) + rbinom(1, 2 * n_fraternal, 0.495) + # en cada nacimiento hay 2 bebés 2 * rbinom(1, n_identicos, 0.405) n_ninas } sims_ninas_2 &lt;- rerun(1000, modelo2()) %&gt;% flatten_dbl() mean(sims_ninas_2) #&gt; [1] 197.511 sd(sims_ninas_2) #&gt; [1] 10.2152 ggplot() + geom_histogram(aes(x = sims_ninas_2), binwidth = 4, alpha = 0.7) 8.2.0.1 Ejemplo de simulación continua predictiva El 52% de los adultos en EUA son mujeres y el 48% hombres, las estaturas de los hombres se distribuyen aproximadamente normal con media 175 cm y desviación estándar de 7.37 cm, en el caso de las mujeres la distribución es aproximadamente normal con media 161.80 cm y desviación estándar de 6.86 cm. Supongamos que seleccionamos 10 adultos al azar, ¿cuál es el modelo gráfico asociado? ¿qué podemos decir del promedio de estatura? sexo &lt;- rbinom(10, 1, 0.52) altura &lt;- rnorm(sexo, mean = 161.8 * (sexo == 1) + 175 * (sexo == 0), sd = 6.86 * (sexo == 1) + 7.37 * (sexo == 0)) mean(altura) #&gt; [1] 172.2232 Simulamos la distribución de la altura promedio: mediaAltura &lt;- function(){ sexo &lt;- rbinom(10, 1, 0.52) altura &lt;- rnorm(sexo, mean = 161.8 * (sexo == 1) + 175 * (sexo == 0), sd = 6.86 * (sexo == 1) + 7.37 * (sexo == 0)) } sims_alturas &lt;- rerun(1000, mediaAltura()) media_alturas &lt;- sims_alturas %&gt;% map_dbl(mean) mean(media_alturas) #&gt; [1] 168.0653 sd(media_alturas) #&gt; [1] 3.045816 ggplot() + geom_histogram(aes(x = media_alturas), binwidth = 1.2, alpha = 0.7) ¿Y que podemos decir de la altura máxima? alt_max &lt;- sims_alturas %&gt;% map_dbl(max) qplot(alt_max, geom = &quot;histogram&quot;, binwidth = 1.5, alpha = 0.7) Supongamos que una compañía cambia la tecnología usada para producir una cámara, un estudio estima que el ahorro en la producción es de $5 por unidad con un error estándar de $4. Más aún, una proyección estima que el tamaño del mercado (esto es, el número de cámaras que se venderá) es de 40,000 con un error estándar de 10,000. Suponiendo que las dos fuentes de incertidumbre son independientes, usa simulación de variables aleatorias normales para estimar el total de dinero que ahorrará la compañía, calcula un intervalo de confianza. Ejemplo de simulación de un modelo de regresión En regresión podemos utilizar simulación para capturar tanto la incertidumbre en la predicción (término de error en el modelo) como la incertidumbre en la inferencia (errores estándar de los coeficientes e incertidumbre del error residual). Comenzamos con un ejemplo en el que simulamos únicamente incertidumbre en la predicción. Supongamos que el puntaje de un niño de tres años en una prueba cognitiva esta relacionado con las características de la madre, el siguiente modelo resume la diferencia en los puntajes promedio de los niños cuyas madres se graduaron de preparatoria y los que no. \\[y_i= \\beta_0 + \\beta_1 X_{i1} + \\epsilon_i\\] donde \\(y_i\\) es el puntaje del \\(i\\)-ésimo niño, \\(X_{i1}\\) es una variable binaria que indica si la madre se graduó de preparatoria (codificado como \\(1\\)) o no (codificado como \\(0\\)), y \\(\\epsilon_i\\) son los error aleatorios, estos son independientes con distribución normal \\(\\epsilon_i \\sim N(0, \\sigma^2)\\). Ahora consideremos el problema de simular el puntaje de \\(50\\) niños \\(30\\) con madres que terminaron la preparatoria y \\(20\\) cuyas madres no terminaron. Los coeficientes que usaremos son: \\[\\beta_0 = 78\\] \\[\\beta_1 = 12\\] \\[\\sigma = 20\\] El modelo gráfico asociado sería como sigue: vector_mu &lt;- c(rep(78 + 12, 30), rep(78, 20)) # beta_0 + beta_1 X y &lt;- rnorm(50, vector_mu, 20) sims_y &lt;- rerun(2000, rnorm(50, vector_mu, 20)) Podemos calcular la media y su intervalo de confianza: medias &lt;- sims_y %&gt;% map_dbl(mean) quantile(medias, c(0.025, 0.975)) #&gt; 2.5% 97.5% #&gt; 79.66242 90.82172 qplot(medias, geom = &quot;histogram&quot;, binwidth = 1.5, alpha = 0.7) Supongamos ahora que nos interesa incorporar incertidumbre en los coeficientes de regresión, y expresamos nuestra incertidumbre a través de distribuciones de probabilidad, ¿cómo sería el modelo gráfico asociado? Primero, suponemos que \\(\\sigma^2\\) tiene una distribución centrada en \\(20^2\\), proporcional a una distribución \\(\\chi^2\\) con \\(432\\) grados de libertad. \\[ \\begin{eqnarray*} \\begin{pmatrix}\\beta_{0}\\\\ \\beta_{1} \\end{pmatrix} &amp; \\sim &amp; N\\left[\\left(\\begin{array}{c} 78\\\\ 12 \\end{array}\\right), \\sigma^2 \\left(\\begin{array}{cc} 0.01 &amp; -0.01\\\\ -0.01 &amp; 0.01 \\end{array}\\right)\\right] \\end{eqnarray*} \\] Ahora, simulamos del modelo incorporando tanto la incertidumbre correpondiente a la predicción como la incertidumbre en los coeficientes de regresión, para los coeficientes: Simula \\(\\sigma=20\\sqrt{(432)/X}\\) donde \\(X\\) es una generación de una distribución \\(\\chi^2\\) con \\(432\\) grados de libertad. Dado \\(\\sigma\\) (obtenido del paso anterior), simula \\(\\beta\\) de una distribución normal multivariada con media \\((77,12)\\) y matriz de covarianzas \\(\\sigma^2 V\\). Simula \\(y\\) el vector de observaciones usando los parámetros de \\(1\\) y \\(2\\). simula_parametros &lt;- function(){ # empezamos simulando sigma sigma &lt;- 20 * sqrt((432) / rchisq(1, 432)) # la usamos para simular betas beta &lt;- MASS::mvrnorm(1, mu = c(78, 12), Sigma = sigma ^ 2 * matrix(c(0.011, -0.011, -0.011, 0.013), nrow = 2)) # Simulamos parámetros list(sigma = sigma, beta = beta) } sims_parametros &lt;- rerun(10000, simula_parametros()) # simulamos los puntajes simula_puntajes &lt;- function(beta, sigma, n_hs = 30, n_nhs = 20){ vector_mu &lt;- c(rep(beta[1] + beta[2], n_hs), rep(beta[1], n_nhs)) # beta_0 + beta_1 X obs = rnorm(50, vector_mu, sigma) } sims_puntajes &lt;- map(sims_parametros, ~simula_puntajes(beta = .[[&quot;beta&quot;]], sigma = .[[&quot;sigma&quot;]])) medias_incert &lt;- sims_puntajes %&gt;% map_dbl(mean) quantile(medias_incert, c(0.025, 0.975)) #&gt; 2.5% 97.5% #&gt; 79.22699 90.89105 qplot(medias_incert, geom = &quot;histogram&quot;, binwidth = 1, alpha = 0.7) Si nos interesara la mediana de los puntajes, ¿qué cambio tendríamos que hacer en el código? Los parametros se obtuvieron de ajustar el modelo de regresión lineal a un conjunto de 434 observaciones de puntajes de niños. library(usethis) use_directory(&quot;data&quot;) # crea carpeta en caso de que no exista ya usethis::use_zip(&quot;https://github.com/tereom/estcomp/raw/master/data-raw/data_sim.zip&quot;, &quot;data&quot;) # descargar y descomprimir zip library(foreign) kids_iq &lt;- read.dta(&quot;data/data_sim/kidiq.dta&quot;) lm_kid &lt;- lm(kid_score ~ mom_hs, kids_iq) summary(lm_kid) #&gt; #&gt; Call: #&gt; lm(formula = kid_score ~ mom_hs, data = kids_iq) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -57.55 -13.32 2.68 14.68 58.45 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 77.548 2.059 37.670 &lt; 2e-16 *** #&gt; mom_hs 11.771 2.322 5.069 5.96e-07 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 19.85 on 432 degrees of freedom #&gt; Multiple R-squared: 0.05613, Adjusted R-squared: 0.05394 #&gt; F-statistic: 25.69 on 1 and 432 DF, p-value: 5.957e-07 summary(lm_kid)$cov.unscaled #&gt; (Intercept) mom_hs #&gt; (Intercept) 0.01075269 -0.01075269 #&gt; mom_hs -0.01075269 0.01368524 summary(lm_kid)$sigma #&gt; [1] 19.85253 Podemos usar simulación para calcular intervalos de confianza para \\(\\beta_0\\) y \\(\\beta_1\\), sims_parametros %&gt;% map_dbl(~(.$beta[1])) %&gt;% sd() #&gt; [1] 2.114737 sims_parametros %&gt;% map_dbl(~(.$beta[2])) %&gt;% sd() #&gt; [1] 2.303523 No parece que valga la pena el esfuerzo cuando podemos calcular los intervalos analíticamente, sin embargo con simulación podemos responder fácilmente otras preguntas, por ejemplo, la pregunta inicial: ¿cuál es la media esperada para un conjunto de \\(50\\) niños, \\(30\\) con madres que hicieron preparatoria y \\(20\\) que no? es fácil de responder con simulación. Podríamos usar predict() para calcular el estimador puntual de la media en el examen para los niños: pred_mi_pob &lt;- predict(lm_kid, newdata = data.frame(mom_hs = c(rep(1, 30), rep(0, 20))), se.fit = TRUE) mean(pred_mi_pob$fit) #&gt; [1] 84.61114 ¿Cómo calculas el error estándar? En este caso se puede resolver pues es una combinación lineal de los coeficientes del modelo, pero en muchos casos nuestro objetivo es más complicado que coeficientes o combinaciones lineales de estos. 8.2.0.2 Simulación de predicciones no lineales Veamos un ejemplo de las elecciones en el congreso de EUA. Tenemos un modelo que usaremos para predecir la elección de \\(1990\\) basados en la de \\(1988\\). Explicación del problema. EUA está dividido en \\(435\\) distritos congresionales, definimos la variable de interés \\(y_i\\) con \\(i=1,...,n\\), como la participación del partido Demócrata en el distrito \\(i\\) en \\(1988\\). La participación se calcula como el porcentaje de los votos correspondientes a los demócratas del total de votos que recibieron los demócratas y republicanos, esto es, se excluyen los votos a otros partidos. El modelo del que simularemos se construyó usando datos de \\(1986\\) y \\(1988\\). # Los datos están almacenados en 3 archivos correspondientes al año paths &lt;- fs::dir_ls(here::here(&quot;data&quot;, &quot;data_sim&quot;, &quot;congress&quot;)) paths &lt;- set_names(paths, basename(paths)) # Leemos los datos y recodificamos variables congress &lt;- map_dfr(paths, read.table, quote = &quot;\\&quot;&quot;, stringsAsFactors = FALSE, .id = &quot;year&quot;) %&gt;% mutate(id = rep(1:435, 3), year = parse_number(year), # año de la elección incumbency = ifelse(V3 == -9, NA, V3), # codificar NAs dem_share = V4 / (V4 + V5)) %&gt;% # participación demócrata dplyr::select(id, year, incumbency, dem_share) glimpse(congress) #&gt; Observations: 1,305 #&gt; Variables: 4 #&gt; $ id &lt;int&gt; 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17,… #&gt; $ year &lt;dbl&gt; 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986, 1986… #&gt; $ incumbency &lt;int&gt; 1, 1, 1, -1, 1, -1, 0, -1, -1, 1, 1, 1, 1, 1, 1, 0, 1, 1, … #&gt; $ dem_share &lt;dbl&gt; 0.7450362, 0.6738455, 0.6964566, 0.4645901, 0.3910945, 0.3… # datos en forma horizontal congress_w &lt;- pivot_wider(congress, names_from = year, values_from = c(incumbency, dem_share)) # quitamos NAs congress_w &lt;- congress_w %&gt;% drop_na() ggplot(congress_w, aes(x = dem_share_1986, y = dem_share_1988, color = factor(incumbency_1986))) + geom_abline(color = &quot;darkgray&quot;) + geom_point() + labs(color = &quot;&quot;) # quitamos las elecciones que no se compitieron congress_88 &lt;- filter(congress_w, dem_share_1988 != 1 &amp; dem_share_1988 != 0, incumbency_1988 != 0) %&gt;% mutate( dem_share_1986 = case_when(dem_share_1986 == 0 ~ 0.25, dem_share_1986 == 1 ~ 0.75, TRUE ~ dem_share_1986)) fit_88 &lt;- lm(dem_share_1988 ~ dem_share_1986 + incumbency_1988, data = congress_88) summary(fit_88) #&gt; #&gt; Call: #&gt; lm(formula = dem_share_1988 ~ dem_share_1986 + incumbency_1988, #&gt; data = congress_88) #&gt; #&gt; Residuals: #&gt; Min 1Q Median 3Q Max #&gt; -0.170533 -0.035497 -0.000485 0.038853 0.215279 #&gt; #&gt; Coefficients: #&gt; Estimate Std. Error t value Pr(&gt;|t|) #&gt; (Intercept) 0.16729 0.02016 8.298 2.8e-15 *** #&gt; dem_share_1986 0.65174 0.03846 16.947 &lt; 2e-16 *** #&gt; incumbency_1988 0.06809 0.00754 9.031 &lt; 2e-16 *** #&gt; --- #&gt; Signif. codes: 0 &#39;***&#39; 0.001 &#39;**&#39; 0.01 &#39;*&#39; 0.05 &#39;.&#39; 0.1 &#39; &#39; 1 #&gt; #&gt; Residual standard error: 0.06749 on 328 degrees of freedom #&gt; Multiple R-squared: 0.8865, Adjusted R-squared: 0.8858 #&gt; F-statistic: 1281 on 2 and 328 DF, p-value: &lt; 2.2e-16 Simulemos del modelo: congress_90 &lt;- filter(congress_w, dem_share_1990 != 1 &amp; dem_share_1990 != 0, incumbency_1990 != 0) %&gt;% mutate( dem_share_1990 = case_when(dem_share_1988 == 0 ~ 0.25, dem_share_1988 == 1 ~ 0.75, TRUE ~ dem_share_1988)) # Matriz X X &lt;- cbind(1, congress_w$dem_share_1988, congress_w$incumbency_1990) df &lt;- df.residual(fit_88) s &lt;- summary(fit_88)$sigma V &lt;- summary(fit_88)$cov.unscaled coef_mod &lt;- summary(fit_88)$coefficients[, 1] simula_modelo &lt;- function(){ sigma &lt;- s * sqrt(df / rchisq(1, df)) beta &lt;- MASS::mvrnorm(1, mu = coef_mod, Sigma = sigma ^ 2 * V) mu &lt;- X %*% beta tibble(id = 1:length(mu), dem_share = rnorm(length(mu), mu, sigma)) } sims_congress &lt;- rerun(1000, simula_modelo()) sims_congress_df &lt;- sims_congress %&gt;% bind_rows(.id = &quot;sim&quot;) ggplot(sims_congress_df, aes(x = reorder(id, dem_share), y = dem_share)) + geom_boxplot() + geom_hline(color = &quot;red&quot;, yintercept = 0.5) Podemos preguntarnos cuántas elecciones ganaron los demócratas en \\(1990\\): \\(\\sum I(\\tilde{y} &gt; 0.5)\\) sims_congress_df %&gt;% group_by(sim) %&gt;% mutate(wins = sum(dem_share &gt; 0.5)) %&gt;% ungroup() %&gt;% summarise( mean_wins = mean(wins), median_wins = median(wins), sd_wins = sd(wins) ) #&gt; # A tibble: 1 x 3 #&gt; mean_wins median_wins sd_wins #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 257. 257 2.97 sims_congress_df %&gt;% group_by(sim) %&gt;% mutate(wins = sum(dem_share &gt; 0.5)) %&gt;% pull(wins) %&gt;% qplot(binwidth = 1, alpha = 0.7) Veamos lo que ocurrió realmente sum(congress_w$dem_share_1990 &gt; 0.5) #&gt; [1] 265 La función sim() del paquete arm permite simular de modelos lineales y lineales generalizados. sim_fit_88 &lt;- arm::sim(fit_88, n.sims = 1000) sim_fit_88@coef[1:10, ] #&gt; (Intercept) dem_share_1986 incumbency_1988 #&gt; [1,] 0.1790449 0.6435951 0.06845862 #&gt; [2,] 0.1591542 0.6682493 0.07115940 #&gt; [3,] 0.1215345 0.7285898 0.05552796 #&gt; [4,] 0.2003955 0.5966258 0.07670371 #&gt; [5,] 0.1670941 0.6609347 0.06537666 #&gt; [6,] 0.1669521 0.6451482 0.06956311 #&gt; [7,] 0.1328750 0.7088040 0.06084818 #&gt; [8,] 0.1570452 0.6686808 0.06762329 #&gt; [9,] 0.1735274 0.6488581 0.06723601 #&gt; [10,] 0.1976371 0.6030459 0.08171117 sim_fit_88@sigma[1:10] #&gt; [1] 0.06857743 0.06694072 0.06671788 0.06556664 0.06812829 0.06757501 #&gt; [7] 0.06298298 0.06899185 0.06584510 0.07016688 Referencias "],
["inferencia-visual.html", "8.3 Inferencia visual", " 8.3 Inferencia visual Las gráficas nos ayudan a descubrir patrones, a diferencia de los modelos las gráficas nos pueden sorprender y podemos entender relaciones en las variables que de otra manera sería difícil. La siguiente gráfica muestra las tasas de mortalidad por diabetes para mayores de 75 años. Usamos las siguientes fuentes: INEGI - México estadísticas vitales, defunciones generales y fetales 2015. CONAPO - Estimaciones y proyecciones de la población, datos descargados de CONAPO. Apofenia El ímpetu por concluir (rage-to-conclude bias, Tufte) nos hace ver patrones en datos donde no existen dichos patrones. Esto puede conllevar a inferencias prematuras, el análisis estadístico busca alinear la realidad en la evidencia con la inferencia que se realiza a partir de dicha evidencia. La inferencia visual nos permite descubrir nuevas relaciones, controlando la apofenia. ¿Qué mapa muestra las tasas de mortalidad reales? Si podemos distinguir los datos hay evidencia estadística rigurosa de un patrón espacial en las tasas de mortalidad que se puede detectar en la gráfica. Inferencia Recordemos que la inferencia ocurre cuando usamos muestras de los datos para concluir acerca de la población completa. Típicamente se usan estadísticas (funciones de la muestra) como medias, desviaciones estándar, medianas, etc. y nuestros conocimientos de como se comportan las estadísticas a lo largo de las posibles muestras (ej. error estándar de la media). En el caso de inferencia visual las estadísticas son las gráficas. Las referencias de esta sección son los artículos Hofmann et al. (2010), Cook et al. (2012), Buja et al. (2009), la discusión en el sitio de Tufte, E. Making better inferences from statistical graphics y la presentación de Cook, D. To the tidyverse and beyond: Challenges to the future of data science. Protocolos de inferencia visual Rorschach. Antes de observar los datos, grafica una serie de datos nulos, para obtener una idea de como se vería nuestra gráfica cuando no hay relación entre las variables. Lineup. Escondemos la gráfica de los datos en un conjunto de datos nulos y pregunta a un tercero si puede identificar los datos reales. Si eligen la gráfica con los datos verdaderos tenemos evidencia de que los datos tienen estructura que es significativamente diferente a lo que esperaríamos por azar. Datos nulos Para generar datos nulos podemos usar un método no paramétrico o uno paramétrico. Permutación: Seleccionamos una de las columnas de los datos de interés y permutamos los valores. Simulación: Suponemos que una variable sigue una distribución y simulamos de esta. Calibración con Rorschach La siguiente imagen proviene de un escrito de Edmond Murphy que en 1964 escribió sobre la dudosa inferencia de mecanismos causales a partir de la observación de una distribución bimodal (Tufte (1986), p. 169): Ejemplo: Estaturas Estaturas de hombres y mujeres. Supongamos que nos interesa describir de manera simple los datos, independientemente de si se trata de un hombre o una mujer. glimpse(singer_g) #&gt; Observations: 235 #&gt; Variables: 2 #&gt; $ gender &lt;chr&gt; &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F&quot;, &quot;F… #&gt; $ height &lt;dbl&gt; 162.56, 157.48, 167.64, 165.10, 152.40, 154.94, 165.10, 167.64… Suponemos que la estatura es una medición que se distribuye aproximadamente normal con media 171 cm y desviación estándar 10 cm. ¿Es razonable esta descripción?* Una manera de probar que tan buena es esta descripción es considerando qué es lo que veríamos si el modelo es el que acabamos de mencionar, para esto hacemos 19 simulaciones bajo el modelo \\(N(\\mu, \\sigma^2)\\) y las comparamos con los datos observados. ¿Captura este modelo las características observadas? library(nullabor) sing_null &lt;- lineup(null_dist(&#39;height&#39;, dist = &#39;normal&#39;, params = list(mean = 171, sd = 10)), n = 20, singer_g) #&gt; decrypt(&quot;NH19 joko 7w QWh7k7Ww eA&quot;) ggplot(sing_null, aes(x = gender, y = height)) + facet_wrap(~ .sample) + geom_jitter(position = position_jitter(width = 0.1, height = 1), size = 0.8, alpha = 0.5) Veremos como estas pruebas visuales se comparan con las pruebas de hipótesis típicas de estadística. Pruebas de hipótesis típicas Antes de proseguir recordemos los conceptos de prueba de hipótesis: Hipótesis nula (\\(H_0\\)): hipótesis que se desea contrastar, describe la conducta default del fenómeno de interés. Hipótesis alternativa (\\(H_1\\)). Estadística de prueba: es una estadística con base en la cuál tomamos la decisión de rechazar o no rechazar. Se calcula considerando la hipótesis nula como verdadera. Valor-p: Nivel de significancia alcanzado, probabilidad de que la estadística de prueba sea al menos tan extrema como la observada con los datos si la hipótesis nula es verdadera. Escenario \\(H_0\\) verdadera \\(H_0\\) Falsa Rechazar \\(H_0\\) Error Tipo 1 (\\(\\alpha\\)) Decisión correcta No rechazar \\(H_0\\) Decisión correcta Error tipo 2 (\\(\\beta\\)) Ejemplo: estaturas Usamos datos de estaturas, la hipótesis nula es que la media de las estaturas es la misma para hombres y mujeres. Hipótesis: \\[H_0:\\mu_m = \\mu_h\\] Estadistica de prueba es: \\[Z=\\frac{\\bar{X_m}-\\bar{X_h}}{\\hat{\\sigma}\\sqrt{1/n_1+1/n_2}}\\] la prueba se basa en una distribución \\(t\\) con \\(n_1 + n_2 - 2\\) grados de libertad. heights_f &lt;- singer_g$height[singer_g$gender == &quot;F&quot;] heights_m &lt;- singer_g$height[singer_g$gender == &quot;M&quot;] n_f &lt;- length(heights_f) n_m &lt;- length(heights_m) t &lt;- (mean(heights_m) - mean(heights_f)) / (sd(singer_g$height) * sqrt(1 / n_f + 1 / n_m)) t #&gt; [1] 11.23764 Y rechazamos si \\(t\\) es menor/mayor al valor crítico \\(t^*\\). t_star &lt;- qt(0.025, n_f + n_m - 2) t_star #&gt; [1] -1.970198 Datos nulos: ¿Cómo se ven los inocentes? nulos &lt;- data.frame(t = rt(10000, 233)) ggplot(nulos, aes(x = t)) + geom_histogram(color = &quot;darkgray&quot;, fill = &quot;darkgray&quot;) + geom_vline(xintercept = c(t_star, - t_star), color = &quot;red&quot;, alpha = 0.5) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Notamos que el valor obtenido con nuestros datos está en las colas de la distribución, es decir, es muy poco plausible observar un valor tan bajo como el que obtenemos bajo la hipótesis nula. ¿Cómo calculamos el valor-p con simulación? Inferencia visual Los principios de pruebas de hipótesis son los mismos para pruebas visuales, a excepción de dos aspectos: la estadística de prueba y el mecanismo para medir similitud. La estadística de prueba ahora es una gráfica de los datos, y en lugar de una diferencia matemática usamos el ojo humano. Prueba visual Genera n-1 datos nulos (datos que siguen la hipótesis nula) Grafica los nulos + los datos reales, donde los datos están posicionados de manera aleatoria. Muestra la gráfica a un observador imparcial. ¿Pueden distinguir los datos? Si es el caso, hay evidencia de diferencia verdadera (valor p = 1/n). Regresando a estaturas Volvamos al ejemplo de las estaturas, proponemos el siguiente modelo: la estatura es aproximadamente normal con media 179 para hombres y 164 para mujeres, la desviación estándar es de 6.5 en ambos casos. singer_g %&gt;% group_by(gender) %&gt;% summarise(mean(height), sd(height)) #&gt; # A tibble: 2 x 3 #&gt; gender `mean(height)` `sd(height)` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 F 164. 6.33 #&gt; 2 M 179. 6.92 library(nullabor) singer_c &lt;- singer_g %&gt;% group_by(gender) %&gt;% mutate(height_c = height - mean(height)) set.seed(26832) sing_null_c &lt;- lineup(null_dist(&#39;height_c&#39;, dist = &#39;normal&#39;, params = list(mean = 0, sd = sd(singer_c$height_c))), n = 20, singer_c) #&gt; decrypt(&quot;NH19 joko 7w QWh7k7Ww bU&quot;) head(sing_null_c) #&gt; gender height height_c .sample #&gt; 1 F 162.56 -2.5792606 1 #&gt; 2 F 157.48 1.1509316 1 #&gt; 3 F 167.64 3.5892689 1 #&gt; 4 F 165.10 0.4958803 1 #&gt; 5 F 152.40 8.7156540 1 #&gt; 6 F 154.94 -16.6183436 1 Prueba: ggplot(sing_null_c, aes(x = gender, y = height_c)) + facet_wrap(~ .sample) + geom_jitter(position = position_jitter(width = 0.1, height = 1), size = 0.8, alpha = 0.5) En esta segunda prueba gráfica no rechazamos la hipótesis nula. ¿Porqué pruebas visuales? En el ejemplo de estaturas se pueden utilizar pruebas estadísticas tradicionales, sin embargo, estas pruebas no cubren todas las complejidades que pueden surgir en una exploración de datos. El siguiente es un lineup de nubes de palabras tomado de Graphical Inference for Infovis. Además, las pruebas visuales nos pueden enseñar no solo si rechazamos la hipótesis nula, sino que en ocasiones revelan el por qué rechazar. El paquete nullabor El paquete nullabor tiene funciones para implementar los protocolos Rorschach y Lineup, en cualquiera de los dos protocolos el primer paso es poder generar datos nulos, y para ello nullabor incluye las siguientes opciones: null_permute: Utiliza permutación, la variable es independiente de las otras. null_dist: Simula dada una distribución particular: Beta, Cauchy, Exponencial, Poisson,… null_lm: Simula cuando la variable es una combinación lineal de predictores. Hay ocasiones que queremos simular datos nulos más allá de las funciones incluídas en nullabor, más adelante veremos un ejemplo de Gelman and Hill (2007) donde utilizamos lo aprendido en la sección de simulación de modelos para implementar nuestra prueba visual. Adicionalmente, el material suplemental de Buja et al. (2009) explica algunas consideraciones para generar datos nulos. Ejemplo: diagramas de dispersión Un diagrama de dispersión muestra la relación entre dos variables continuas y responde a la pregunta: ¿existe una relación entre \\(x\\) y \\(y\\)? Una posible hipótesis nula es que no hay relación entre las variables. Supongamos que queremos usar pruebas visuales para esta hipótesis, la función null_permute del paquete nullabor recibe el nombre de una variable de los datos y la salida de la funcción consiste en la variable permutada para obtener los datos nulos. En este ejercicio usarás los datos diamonds, toma una muestra de tamaño 5000 (sin reemplazo) y procede como se indica: Usa la función null_permute para crear una nueva base de datos con la variable depth permutada en los datos nulos. Usa la función line_up (como se usó en el caso de las estaturas) y genera una gráfica con 20 páneles, donde grafiques depth en el eje horizontal y carat en el eje vertical tienes evidencia para afirmar que existe una relación entre depth y carat. Explora ahora precio y carat. Más allá que permutación En muchos casos el supuesto de independencia es demasiado fuerte, es claro que las variables están relacionadas y queremos estudiar una relación particular. Por ejemplo, podemos pensar que los intentos de encestar a tres puntos en el basquetbol siguen una distribución cuadrática en el espacio: mientras el ángulo entre el jugador y la canasta aumenta el jugador se acerca más para asegurar el éxito. La siguiente figura prueba esta hipótesis usando datos de los tres punteros de los Lakers en la temporada 2008/2009, los datos se obtuvieron de Basketball Geek. paths &lt;- dir(&quot;data/data_sim/2008-2009&quot;, pattern = &quot;LAL&quot;, full.names = TRUE) basket &lt;- purrr::set_names(paths, paths) %&gt;% map_df(read_csv) basket_LA &lt;- basket %&gt;% filter(team == &quot;LAL&quot;, type == &quot;3pt&quot;, !is.na(x), !is.na(y)) %&gt;% # datos Lakers mutate( x = x + runif(length(x), -0.5, 0.5), y = y + runif(length(y), -0.5, 0.5), r = sqrt((x - 25) ^ 2 + y ^ 2), # distancia a canasta angle = atan2(y, x - 25)) %&gt;% # ángulo filter(r &gt; 20 &amp; r &lt; 39) %&gt;% # lanzamientos en el rango típico dplyr::select(x, y, r, angle) # guardar datos write.table(basket_LA, file = &quot;data/data_sim/basket_LA.csv&quot;, sep = &quot;,&quot;) glimpse(basket_LA) #&gt; Observations: 1,411 #&gt; Variables: 4 #&gt; $ x &lt;dbl&gt; 0.6348924, 41.4774656, 46.1710232, 36.7522275, 42.8014014, 7.99… #&gt; $ y &lt;dbl&gt; 6.240224, 26.148988, 17.276561, 29.819819, 23.728493, 28.393449… #&gt; $ r &lt;dbl&gt; 25.15152, 30.90755, 27.32566, 32.05209, 29.66364, 33.09658, 30.… #&gt; $ angle &lt;dbl&gt; 2.8908688, 1.0085109, 0.6844497, 1.1953796, 0.9271595, 2.110430… basket_null &lt;- lineup(null_lm(r ~ poly(angle, 2)), basket_LA, n = 10) #&gt; decrypt(&quot;NH19 joko 7w QWh7k7Ww ee&quot;) ggplot(basket_null, aes(x = angle * 180 / pi, y = r)) + geom_point(alpha = 0.5, size = 0.8) + scale_x_continuous(&quot;Angle (degrees)&quot;, breaks = c(0, 45, 90, 135, 180), limits = c(0, 180)) + facet_wrap(~ .sample, nrow = 2) Los datos reales están escondidos entre un conjunto de datos nulos que siguen la hipótesis de una relación cuadrática, los conjuntos nulos se crean ajustando el modelo, produciendo predicciones y residuales, y sumando los residuales rotados a las predicciones. Otras consideraciones Potencia La potencia en una prueba de hipótesis es la probabilidad de rechazar la hipótesis nula cuando esta es falsa. En el caso de pruebas visuales la potencia depende de la calidad de la gráfica. Se ha estudiado la potencia de las pruebas visuales, Majumder, Hofmann, and Cook (2013) y se ha visto con simulación que las pruebas visuales pueden tener potencia comparable a las pruebas de hipótesis tradicionales, a veces incluso superándolas. El paquete nullabor incluye la función visual_power() para calcular el poder de una prueba simulada. Valor p Si usamos un jurado compuesto por \\(K\\) jueces en lugar de un juez y \\(k\\) de ellos entonces el valor p combinado es \\(P(X \\le k)\\) donde \\(X\\) tiene distribución \\(Binomial(K, 1/20)\\). Si todos los jueces identifican los datos el valor p sería \\(0.05^K\\) El paquete nullabor tiene una función para calcular el valor p de una prueba dada pvisual(). Las pruebas de hipótesis visuales, no son la única herramienta que se debe usar en el análisis exploratorio o para evaluar un modelo. Sin embargo, las pruebas visuales nos ayudan a explorar relaciones observadas en gráficas controlando por la apofenia, y en general graficar modelos ajustados nos puede ayudar a comprender las implicaciones de un modelo y las fallas del mismo. Ejemplo: modelo Poisson con sobreabundancia de ceros Las ideas detrás de inferencia visual para diagnósticos de modelos son comunes en estadística bayesiana, y se pueden extender a la estimación frecuentista usando lo que aprendimos de simulación de modelos probabilísticos. Cuando simulamos datos usando el modelo se conoce como simulación de datos falsos (fake data) o datos replicados y lo que buscamos es comparar datos simulados bajo el modelo ajustado con los datos observados. El siguiente ejemplo se tomó de Gelman and Hill (2007). Problema: se busca estudiar el efecto de pesticidas en el control de cucarachas en departamentos urbanos. Se realiza un experimento donde se dividen los departamentos en: grupo de tratamiento (\\(160\\) deptos.) y grupo de control (\\(104\\) deptos.). En cada departamento se mide el número de cucarachas atrapadas \\(y_i\\) en un conjunto de trampas. Distintos departamentos tuvieron trampas un número distinto de días, y denotamos por \\(u_i\\) el número de días-trampa en el i-ésimo departamento. Se propone el siguiente modelo: \\[y_i \\sim Poisson(u_iexp(X\\beta))\\] donde X reprersenta variables explicativas (predictores), en este caso, consisten en el nivel de cucarachas antes del tratamiento (roach1), una variable binaria indicando si se aplica insecticida en el departamento (treatment) y una variable binaria indicando si el edificio es de personas mayor (senior). En R el modelo se ajusta como sigue: library(arm) roachdata &lt;- read.csv(&quot;data/data_sim/roachdata.csv&quot;, stringsAsFactors = FALSE) glm_1 &lt;- glm(y ~ roach1 + treatment + senior, family = poisson, offset = log(exposure2), data = roachdata) display(glm_1) #&gt; glm(formula = y ~ roach1 + treatment + senior, family = poisson, #&gt; data = roachdata, offset = log(exposure2)) #&gt; coef.est coef.se #&gt; (Intercept) 3.09 0.02 #&gt; roach1 0.01 0.00 #&gt; treatment -0.52 0.02 #&gt; senior -0.38 0.03 #&gt; --- #&gt; n = 262, k = 4 #&gt; residual deviance = 11429.5, null deviance = 16953.7 (difference = 5524.2) ¿Qué tan bien se ajusta el modelo a los datos? Para responder esta pregunta simularemos del modelo. X &lt;- model.matrix(~ roach1 + treatment + senior, family = poisson, data = roachdata) simula_modelo &lt;- function(n_sims = 19, ajuste){ n &lt;- nrow(X) # simulamos los coeficientes del modelo betas &lt;- coef(sim(ajuste, n_sims)) # calculamos ui*exp(Xb) y_hat &lt;- roachdata$exposure2 * exp(X %*% t(betas)) # creamos una lista con las y_hat de cada simulación y_hat_list &lt;- split(y_hat, rep(1:ncol(y_hat), each = nrow(y_hat))) # simulamos observaciones y_sims &lt;- map_df(y_hat_list, ~rpois(n, .)) y_sims_df &lt;- bind_cols(X = 1:n, y_sims) %&gt;% gather(sim, y, -X) # código para esconder los datos codigo &lt;- sample(1:(n_sims + 1), n_sims + 1) sims_datos &lt;- y_sims_df %&gt;% bind_rows(dplyr::select(roachdata, X, y)) %&gt;% mutate(sample = rep(codigo, each = n)) list(sims_datos = sims_datos, y_sims_df = y_sims_df, codigo = codigo[n_sims + 1]) } sim_1 &lt;- simula_modelo(n_sims = 9, glm_1) ggplot(sim_1$sims_datos, aes(x = y)) + geom_histogram(binwidth = 3) + xlim(0, 40) + facet_wrap(~ sample, nrow = 2) #&gt; Warning: Removed 306 rows containing non-finite values (stat_bin). #&gt; Warning: Removed 20 rows containing missing values (geom_bar). # los datos están en sim_1$codigo ¿En que se diferencían los datos observados de los simulados? Comparemos el número de ceros de los datos observados y el primer conjunto de datos simulados: mean(roachdata$y == 0) #&gt; [1] 0.3587786 mean(sim_1$sims_datos$y == 0) #&gt; [1] 0.03664122 Vemos que el \\(36\\%\\) de los datos observados hay ceros mientras que en los datos replicados el porcentaje de ceros es cercano a cero. Además de la prueba visual podemos pensar en la proporción de ceros como una estadística de prueba, simulamos \\(1000\\) conjuntos de datos y calculamos la proporción de ceros: sim_2 &lt;- simula_modelo(1000, glm_1) # calculamos el porcentaje de ceros en cada conjunto simulado sims_p_ceros &lt;- sim_2$y_sims_df %&gt;% group_by(sim) %&gt;% summarise(p_ceros = mean(y == 0)) min(sims_p_ceros$p_ceros) #&gt; [1] 0 max(sims_p_ceros$p_ceros) #&gt; [1] 0.007633588 Vemos que en el porcentaje de ceros varía entre \\(0\\) y \\(0.008\\), todos ellos mucho menores a la estadística de prueba \\(0.36\\). Ahora veamos que ocurre si ajustamos un modelo Poisson con sobredispersión, este modelo busca acomodar la sobreabundancia de ceros observada en los datos y que no es congruente con el modelo Poisson que ajustamos. glm_2 &lt;- glm(y ~ roach1 + treatment + senior, family = quasipoisson, offset = log(exposure2), data = roachdata) display(glm_2) #&gt; glm(formula = y ~ roach1 + treatment + senior, family = quasipoisson, #&gt; data = roachdata, offset = log(exposure2)) #&gt; coef.est coef.se #&gt; (Intercept) 3.09 0.17 #&gt; roach1 0.01 0.00 #&gt; treatment -0.52 0.20 #&gt; senior -0.38 0.27 #&gt; --- #&gt; n = 262, k = 4 #&gt; residual deviance = 11429.5, null deviance = 16953.7 (difference = 5524.2) #&gt; overdispersion parameter = 65.4 simula_modelo &lt;- function(n_sims = 19, ajuste){ # simulamos los coeficientes del modelo betas &lt;- coef(sim(ajuste, n_sims)) # calculamos ui*exp(Xb) y_hat &lt;- roachdata$exposure2 * exp(X %*% t(betas)) n &lt;- nrow(X) # creamos una lista con las y_hat de cada simulación y_hat_list &lt;- split(y_hat, rep(1:ncol(y_hat), each = nrow(y_hat))) # simulamos observaciones y_sims &lt;- map_df(y_hat_list, ~rnegbin(n, ., . / (65.4 - 1))) y_sims_df &lt;- bind_cols(X = 1:n, y_sims) %&gt;% gather(sim, y, -X) # código para esconder los datos codigo &lt;- sample(1:(n_sims + 1), n_sims + 1) sims_datos &lt;- y_sims_df %&gt;% bind_rows(dplyr::select(roachdata, X, y)) %&gt;% mutate(sample = rep(codigo, each = n)) list(sims_datos = sims_datos, y_sims_df = y_sims_df, codigo = codigo[n_sims + 1]) } sim_2 &lt;- simula_modelo(n_sims = 9, glm_2) ggplot(sim_2$sims_datos, aes(x = y)) + geom_histogram(binwidth = 4) + xlim(0, 100) + facet_wrap(~ sample, nrow = 2) El panel que corresponde a los datos es: sim_2$codigo #&gt; [1] 3 Podemos comparar el número de ceros. sim_1000 &lt;- simula_modelo(1000, glm_2) # calculamos el porcentaje de ceros en cada conjunto simulado sims_p_ceros &lt;- sim_1000$y_sims_df %&gt;% group_by(sim) %&gt;% summarise(p_ceros = mean(y == 0)) mean(sims_p_ceros$p_ceros &gt;= 0.36) #&gt; [1] 0.189 En este caso el \\(19\\%\\) de los datos muestran una proporción de ceros al menos tan alta como la observada. La simulación de datos falsos no debe ser la única herramienta para evaluar el ajuste de un modelo; sin embargo, es una herramienta útil que nos puede ayudar a detectar desajustes y en caso de revelarlos nos da indicios de porque falla el modelo. Referencias "],
["tamaño-de-muestracalculos-de-potencia.html", "8.4 Tamaño de muestra/calculos de potencia", " 8.4 Tamaño de muestra/calculos de potencia Cuando se esta diseñando un estudio se determina la precisión en las inferencias que se desea, y esto (junto con algunos supuestos de la población) determina el tamaño de muestra que se tomará. Usualmente se fija uno de los siguientes dos objetivos: Se determina el error estándar de un parámetro o cantidad de interés (o de manera equivalente se fija la longitud máxima aceptable del intervalo de confianza que resultará). Por ejemplo, en encuestas electorales es típico reportar los resultados de esta encuesta más menos \\(3\\) puntos porcentuales tienen un nivel del \\(95\\%\\) de confianza, ¿cúantas personas se debe entrevistar para lograr esto? Se determina la probabilidad de que un estadístico determinado sea estadísticamente significativo, esto se conoce como la potencia de un experimento. Por ejemplo, cuando se hacen ensayos clínicos se determina un tamaño de muestra para que con probabilidad de \\(x\\)% se detecte una diferencia clinicamente relevante con el nuevo tratamiento (si es que este es efectivo). En muchos casos existen fórmulas para calcular tamaños de muestra de tal manera que se cumplan los objetivos planteados, sin embargo, conforme se agrega complejidad al levantamiento de los datos (faltantes, levantamientos en varias etapas, …) o si nos alejamos de las estadísticas típicas, las fórmulas dejan de aplicar o se vuelven muy complejas, de manera que suele ser conveniente recurrir a simulación. Veremos dos ejemplos que se tomaron de Gelman and Hill (2007). Tamaño de muestra para un error estándar determinado Supongamos que queremos estimar el porcentaje de la población que desaprueba la legalización del aborto en México (ante una pregunta particular). Sospechamos que la proporción es \\(60\\%\\), imaginemos que queremos una precisión (error estándar) de a lo más \\(0.05\\), o \\(5\\) puntos porcentuales. Bajo muestreo aleatorio simple, para una muestra de tamaño \\(n\\), el error estándar de la proporción \\(p\\) es \\[\\sqrt{p(1-p)/n}\\] Sustituyendo nuestra expectativa \\(p = 0.60\\) llegamos a que el error estándar sería \\(0.49/\\sqrt{n}\\), de tal manera que si queremos \\(se(p) \\le 0.05\\) necesitamos \\(n&gt;96\\), en el caso de proporciones es fácil determinar el tamaño de muestra de manera conservadora pues basta con suponer \\(p = 0.5\\). se_fun_n &lt;- function(n, p) sqrt(p * (1 - p) / n) xy &lt;- tibble(x = 20:220, y = seq(0, 1, 0.005)) ggplot(xy, aes(x = x, y = y)) + stat_function(fun = se_fun_n, args = list(p = 0.7), aes(color = &quot;p=0.7&quot;)) + stat_function(fun = se_fun_n, args = list(p = 0.9), aes(color = &quot;p=0.9&quot;)) + stat_function(fun = se_fun_n, args = list(p = 0.5), aes(color = &quot;p=0.5&quot;)) + labs(x = &quot;n&quot;, y = &quot;se&quot;, color = &quot;&quot;) + geom_segment(x = 20, xend = 100, y = 0.05, yend = 0.05, color = &quot;red&quot;, alpha = 0.3, linetype = &quot;longdash&quot;) + geom_segment(x = 100, xend = 100, y = 0.05, yend = 0, color = &quot;red&quot;, alpha = 0.3, linetype = &quot;longdash&quot;) Cómo calcularíamos el tamaño de muestra simulando? En este caso es trivial calcular de manera analítica, pero nos sirve para comparar los resultados que obtendríamos con simulación. sim_p_hat &lt;- function(n, p, n_sims = 1000){ sim_muestra &lt;- rbinom(n_sims, size = n, prob = p) se_p_hat &lt;- sd(sim_muestra / n) tibble(n = n, se_p_hat = se_p_hat, p = p) } sims_.7 &lt;- map_df(seq(20, 220, 5), sim_p_hat, p = 0.7) sims_.5 &lt;- map_df(seq(20, 220, 5), sim_p_hat, p = 0.5) sims_.9 &lt;- map_df(seq(20, 220, 5), sim_p_hat, p = 0.9) sims &lt;- bind_rows(sims_.7, sims_.5, sims_.9) ggplot(sims, aes(x = n, y = se_p_hat, color = factor(p), group = p)) + geom_smooth(se = FALSE, size = 0.5) + labs(x = &quot;n&quot;, y = &quot;se&quot;, color = &quot;&quot;) + geom_segment(x = 20, xend = 100, y = 0.05, yend = 0.05, color = &quot;red&quot;, alpha = 0.3, linetype = &quot;longdash&quot;) + geom_segment(x = 100, xend = 100, y = 0.05, yend = 0, color = &quot;red&quot;, alpha = 0.3, linetype = &quot;longdash&quot;) #&gt; `geom_smooth()` using method = &#39;loess&#39; and formula &#39;y ~ x&#39; Tamaño de muestra determinado para obtener significancia estadística con una probabilidad determinada Supongamos que nuestro objetivo es demostrar que más de la mitad de la población desaprueba la legalización del anorto, esto es \\(p&gt;0.5\\), nuevamente tenemos la hipótesis que el verdadero valor es \\(p=0.6\\). Una prueba de potencia típica tiene un poder de \\(80\\)%, es decir nos gustaría seleccionar \\(n\\) tal que el \\(80\\%\\) de los intervalos construidos con \\(95\\%\\) de confianza no incluyan \\(0.5\\). Para encontrar la \\(n\\) tal que el \\(80\\%\\) de las estimaciones estén al menos, \\(1.96\\) errores estándar por encima de \\(0.5\\) necesitamos que: \\[0.5 + 1.96 se \\leq 0.6 - 0.84 se\\] Sustituyendo \\(se = 0.5/\\sqrt{n}\\) obtenemos \\(n=196\\) Y simulando sería sim_potencia &lt;- function(n, p, n_sims = 1000){ sim_muestra &lt;- rbinom(n_sims, size = n, prob = p) p_hat &lt;- sim_muestra / n se_p_hat &lt;- sqrt(p_hat * (1 - p_hat) / n) acepta &lt;- (p_hat - 1.96 * se_p_hat) &gt; 0.5 tibble(n = n, p = p, potencia = mean(acepta)) } casos_sim &lt;- expand.grid(n = c(5, 10, 50, 80, 100, 150, 200, 300, 500), p = c(0.6, 0.7, 0.8)) sims &lt;- map2_df(casos_sim$n, casos_sim$p, ~sim_potencia(n = .x, p = .y)) ggplot(sims) + geom_line(aes(x = n, y = potencia, color = factor(p), group = p)) + geom_hline(yintercept = 0.8, color = &quot;red&quot;, size = 0.7, linetype = &quot;longdash&quot;) Respuestas continuas En el caso de variables respuesta continuas la dificultad adicional está en que debemos fijar además del tamaño hipotético del efecto la desviación estándar poblacional. Como ejemplo, supongamos que se implementará un experimento en el que se añadirá un suplemento de zinc en la dieta de niños VIH positivos en el Sudáfrica, esto porque en otras poblaciones se ha visto que agregar zinc y otros micronutrientes reduce la ocurrencia de diarrea que a su vez está asociado a problemas del sistema inmune y que alenta el avance del VIH. Comenzamos con el problema de una muestra, ¿qué tamaño de muestra necesitamos para alacanzar una precisión dada? Calculo de poder en modelo multinivel Veamos un ejemplo más interesante, en donde usamos simulación de un modelo probabilístico. Tenemos medidas del sistema inmune (porcentaje de CD4 transformado con raíz cuadrada) de niños VIH positivos a lo largo de un periodo de \\(2\\) años. Las series de tiempo se ajustan de manera razonable con un modelo de intercepto y pendiente variable: \\[y_i \\sim N(\\alpha_{j[i]} + \\beta_{j[i]}t_i, \\sigma^2_y)\\] donde \\(i\\) indexa las mediciones tomadas al tiempo \\(i\\) en el individuo \\(j[i]\\). # preparación de los datos library(lme4) allvar &lt;- read_csv(&quot;data/data_sim/allvar.csv&quot;) #&gt; Parsed with column specification: #&gt; cols( #&gt; VISIT = col_double(), #&gt; newpid = col_double(), #&gt; VDATE = col_character(), #&gt; CD4PCT = col_double(), #&gt; arv = col_double(), #&gt; visage = col_double(), #&gt; treatmnt = col_double(), #&gt; CD4CNT = col_double(), #&gt; baseage = col_double() #&gt; ) cd4 &lt;- allvar %&gt;% filter(treatmnt == 1, !is.na(CD4PCT), baseage &gt; 1, baseage &lt; 5) %&gt;% mutate( y = sqrt(CD4PCT), person = newpid, time = visage - baseage ) La siguiente gráfica muestra las mediciones para cada individuo, podemos ver que las series de tiempo son ruidosas. ggplot(cd4, aes(x = time, y = y, group = person)) + geom_line(alpha = 0.5) Veamos un ajuste usando la función lmer() del paquete lme4. fit_cd4 &lt;- lmer(formula = y ~ time + (1 + time | person), cd4) summary(fit_cd4) #&gt; Linear mixed model fit by REML [&#39;lmerMod&#39;] #&gt; Formula: y ~ time + (1 + time | person) #&gt; Data: cd4 #&gt; #&gt; REML criterion at convergence: 1096.1 #&gt; #&gt; Scaled residuals: #&gt; Min 1Q Median 3Q Max #&gt; -4.4621 -0.4354 0.0504 0.3597 3.4620 #&gt; #&gt; Random effects: #&gt; Groups Name Variance Std.Dev. Corr #&gt; person (Intercept) 1.7659 1.3289 #&gt; time 0.4619 0.6796 0.15 #&gt; Residual 0.5595 0.7480 #&gt; Number of obs: 369, groups: person, 83 #&gt; #&gt; Fixed effects: #&gt; Estimate Std. Error t value #&gt; (Intercept) 4.8460 0.1597 30.349 #&gt; time -0.4683 0.1276 -3.671 #&gt; #&gt; Correlation of Fixed Effects: #&gt; (Intr) #&gt; time -0.146 Notamos que las tendencias sobre el tiempo \\(\\beta\\) tienen un promedio estimado en \\(-0.5\\) con desviación estándar de \\(0.7\\), es decir, estimamos que la mayoría de los niños tienen niveles de CD4 decrecientes, pero no todos. Usaremos estos resultados para hacer calculos de potencia para una nueva prueba que busca medir el efecto del consumo de zinc en la dieta. Quisiéramos que el estudio fuera suficientemente grande para que con probabilidad de al menos \\(80\\)% la media del efecto del tratamiento sea significativo con un nivel de confianza del \\(95\\%\\). Necesitamos hacer supuestos del efecto del tratamiento y del resto de los parámetros que caracterizan el estudio. El análisis de arriba muestra que en los niños VIH positivos que no recibieron zinc los niveles de CD4 caían en promedio \\(0.5\\) al año. Suponemos que con el zinc reduciremos la caída a cero. \\[y_i \\sim N(\\alpha_{j[i]} + \\beta_{j[i]}t_i, \\sigma^2_y)\\] \\[ \\begin{eqnarray*} \\begin{pmatrix}\\alpha_{j}\\\\ \\beta_{j} \\end{pmatrix} &amp; \\sim &amp; N\\left[\\left(\\begin{array}{c} \\gamma_0^{\\alpha}\\\\ \\gamma_0^{\\beta}+\\gamma_1^{\\beta}z_j \\end{array}\\right), \\left(\\begin{array}{cc} \\sigma^2_{\\alpha} &amp; \\rho \\sigma_{\\alpha}\\sigma_{\\beta}\\\\ \\rho \\sigma_{\\alpha}\\sigma_{\\beta} &amp; \\sigma^2_{\\beta} \\end{array}\\right)\\right] \\end{eqnarray*} \\] donde \\[ z_j = \\left\\{ \\begin{array}{lr} 1 &amp; \\text{si el }j \\text{-ésimo niño recibió trataiento}\\\\ 0 &amp; e.o.c \\end{array} \\right. \\] El tratamiento \\(z_j\\) afecta la pendiente \\(\\beta_j\\) más no el intercepto \\(\\alpha_j\\) pues el tratamiento no puede afectar en el tiempo cero. Usando los datos del ajuste de arriba tenemos que para el grupo control la pendiente será: \\(\\gamma_0^{\\beta} = -0.5\\) y el efecto del tratamiento \\(\\gamma_1^{\\beta} = 0.5\\), el resto de los parámetros los especificamos de acuerdo al ajuste de arriba. Por simplicidad fijaremos la correlación \\(\\rho\\) en cero. El siguiente paso es determinar el diseño del modelo, suponemos que dividiremos a \\(J\\) niños VIH positivos en dos grupos del mismo tamaño, \\(J/2\\) de ellos recibirán el cuidado usual y \\(J/2\\) recibirán suplementos de zinc. Más aún suponemos que se medirá el porcentaje de CD4 cada \\(2\\) meses durante un año. Usaremos simulación para determinar el tamaño de muestra \\(J\\) que se requiere para tener una potencia de \\(80\\%\\) si el verdadero efecto es \\(0.5\\), ¿cuál es el modelo gráfico asociado? # cd4_sim simula del modelo con los supuestos que fijamos arriba # podemos variar los valores de los parámetros para cambiar el escenario cd4_sim &lt;- function(J, K, mu.a.true = 4.8, g.0.true = -0.5, g.1.true = 0.5, sigma.y.true = 0.7, sigma.a.true = 1.3, sigma.b.true = 0.7){ time &lt;- rep(seq(0, 1, length = K), J) # K mediciones en el año person &lt;- rep(1:J, each = K) # ids treatment &lt;- sample(rep(0:1, J/2)) treatment1 &lt;- treatment[person] # parámetros a nivel persona a.true &lt;- rnorm(J, mu.a.true, sigma.a.true) b.true &lt;- rnorm(J, g.0.true + g.1.true * treatment, sigma.b.true) y &lt;- rnorm(J * K, a.true[person] + b.true[person] * time, sigma.y.true) data.frame(y, time, person, treatment1) } # calcular si el parámetro es significativo para una generación de simulación cd4_signif &lt;- function(J, K){ fake &lt;- cd4_sim(J, K) lme_power &lt;- lmer(y ~ time + time:treatment1 + (1 + time | person), data = fake) theta_hat &lt;- fixef(lme_power)[&quot;time:treatment1&quot;] theta_se &lt;- summary(lme_power)$coefficients[&quot;time:treatment1&quot;, &quot;Std. Error&quot;] theta_hat - 1.96 * theta_se &gt; 0 } # repetir la simulación de cd4 n_sims veces y calcular el porcentaje de las # muestras en que es significativo el parámetro (el poder) cd4_power &lt;- function(n_sims, J, K){ rerun(n_sims, cd4_signif(J, K = 7)) %&gt;% flatten_dbl() %&gt;% mean() } # calculamos el poder para distintos tamaños de muestra, con 7 mediciones al año potencias &lt;- map_df(c(8, 16, 60, 100, 150, 200, 225, 250, 300, 400), ~data_frame(n = ., p = cd4_power(n_sims = 500, J = ., K = 7))) ggplot(potencias, aes(x = n, y = p)) + geom_hline(yintercept = 0.8, color = &quot;red&quot;, alpha = 0.5) + geom_line() + ylim(0, 1) Notemos que la función cd4_rep() regresa la proporción de las simulaciones en las que el resultado es estadísticamente significativo, esto es, la potencia calculada con simulación, para un estudio con \\(J\\) niños medidos en \\(K\\) intervalos igualmente espaciados. Notemos también que en el límite, cuando \\(J \\to 0\\) el poder es \\(0.025\\), esto es, con una muestra suficientemente chica el efecto del estimador es básicamente aleatorio y por tanto en \\(2.5\\%\\) de los casos el estimador está \\(2\\) desviaciones por encima de cero. Una ventaja de usar simulación para calcular potencia es que nos permite flexibilidad, por ejemplo, es fácil calcular para más escenarios: ¿qué ocurriría si solo puedo medir \\(3\\) veces al año? Se sabe que es común que algunos participantes abandonen el estudio, o no asistan a todas las mediciones, con simulación es fácil incorporar faltantes. Referencias "],
["inferencia-paramétrica.html", "Sección 9 Inferencia paramétrica", " Sección 9 Inferencia paramétrica En esta sección revisaremos algunos conceptos de inferencia paramétrica y estudiaremos bootstrap paramétrico. Sean \\(X_1,...,X_n \\sim p(x| \\theta)\\). Queremos estimar \\(\\theta=(\\theta_1,...,\\theta_k)\\). Recordemos que un estimador \\[\\hat{\\theta} = w(X_1,...,X_n)\\] es una función de los datos. Recordaremos la estimación de \\(\\theta\\) por máxima verosimilitud y algunas de sus propiedades, después introduciremos las ideas de bootstrap paramétrico, y veremos como se relacionacon máxima verosimilitud y bootstrap no paramétrico. "],
["máxima-verosimilitud.html", "9.1 Máxima verosimilitud", " 9.1 Máxima verosimilitud El método más común para estimar parámetros es el método de máxima verosimilitud. Sea \\(X_1,...,X_n\\) independientes e idénticamente distribuidas con función de densidad de probabilidad \\(p(x;\\theta)\\) entonces: La función de verosimilitud se define como: \\[\\mathcal{L}(\\theta) = \\prod_{í=1}^n p(x_i;\\theta).\\] y la log-verosimilitud se define como \\[\\mathcal{l}(\\theta)=\\log \\mathcal{L}(\\theta)=\\sum_{i=1}^n \\log p(x_i; \\theta)\\] La función de verosimilitud no es mas que la densidad conjunta de los datos, con la diferencia de que la tratamos como función del parámetro \\(\\theta\\). Por tanto \\(\\mathcal{L}:\\Theta \\to [0, \\infty)\\), en general \\(\\mathcal{L}(\\theta)\\) no integra uno respecto a \\(\\theta\\). El estimador de máxima verosimilitud es el valor de \\(\\theta\\) que maximiza \\(\\mathcal{L}(\\theta)\\). El máximo de \\(\\mathcal{l}(\\theta)\\) se alcanza en el mismo lugar que el máximo de \\(\\mathcal{L}(\\theta)\\), por lo que maximizar la log-verosimilitud es equivalente a maximizar la verosimilitud. Ejemplo: Bernoulli. Supongamos \\(X_1,...X_n \\sim Bernoulli(\\theta)\\). La función de densidad correspondiente es \\(p(x;\\theta)=\\theta^x(1-\\theta)^{1-x}\\), por lo que: \\[\\mathcal{L}(p)=\\prod_{i=1}^n p(x_i;\\theta)=\\prod_{i=1}^n \\theta^{x_i}(1-\\theta)^{1-x_i}=\\theta^{\\sum x_i}(1-\\theta)^{n-\\sum x_i}\\] denotemos \\(S=\\sum x_i\\), entonces \\[\\mathcal{l}(\\theta)=S \\log \\theta + (n-S) \\log (1-\\theta)\\] . Si \\(n=20\\) y \\(S=12\\) tenemos la función: library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine # Verosimilitud X_1,...,X_n ~ Bernoulli(theta) L_bernoulli &lt;- function(n, S){ function(theta){ theta ^ S * (1 - theta) ^ (n - S) } } # log-verosimilitud l_bernoulli &lt;- function(n, S){ function(theta){ S * log(theta) + (n - S) * log(1 - theta) } } xy &lt;- data.frame(x = 0:1, y = 0:1) verosimilitud &lt;- ggplot(xy, aes(x = x, y = y)) + stat_function(fun = L_bernoulli(n = 20, S = 12)) + xlab(expression(theta)) + ylab(expression(L(theta))) + ggtitle(&quot;Verosimilitud (n=20, S = 12)&quot;) log_verosimilitud &lt;- ggplot(xy, aes(x = x, y = y)) + stat_function(fun = l_bernoulli(n = 20, S = 12)) + xlab(expression(theta)) + ylab(expression(l(theta))) + ggtitle(&quot;log-verosimilitud (n=20, S = 12)&quot;) grid.arrange(verosimilitud, log_verosimilitud, nrow = 1) En ocasiones podemos calcular el estimador de máxima verosimilitud analíticamente, esto es derivando respecto al vector de parámetros de interés, igualando a cero el sistema de ecuaciones resultante, y revisando la segunda derivada para asegurar que se encontró un máximo. En el ejemplo este proceso lleva a \\(\\hat{\\theta}=S/n\\), y con \\(S=12, n = 20\\) obtenemos \\(\\hat{\\theta}=0.6\\). Es muy común recurrir a métodos numéricos (por ejemplo Newton Raphson, BHHH, DFP) en el caso de R podemos usar las funciones optim u optimize. optimize(L_bernoulli(n = 20, S = 12), interval = c(0, 1), maximum = TRUE) #&gt; $maximum #&gt; [1] 0.6000004 #&gt; #&gt; $objective #&gt; [1] 1.426576e-06 optimize(l_bernoulli(n = 20, S = 12), interval = c(0, 1), maximum = TRUE) #&gt; $maximum #&gt; [1] 0.5999985 #&gt; #&gt; $objective #&gt; [1] -13.46023 Sean \\(X_1,...X_n \\sim N(\\mu, \\sigma^2)\\). Calcula el estimador de máxima verosimilitud para \\(\\theta = (\\mu, \\sigma^2)\\). Supongamos que observamos una muestra de tamaño \\(100\\) tal que: \\(\\sum X_i = 40\\) y \\(\\sum X_i^2 = 20\\). Calcula \\(\\hat{\\theta}\\) usando el método de máxima verosimilitud. ¿Cómo graficarías la verosimilitud o log-verosimilitud? Propiedades de los estimadores de máxima verosimilitud Bajo ciertas condiciones del modelo, el estimador de máxima verosimilitud \\(\\hat{\\theta}\\) tiene propiedades deseables, las principales son: Consistencia: \\(\\hat{\\theta} \\xrightarrow{P} \\theta\\) (converge en probabilidad), donde \\(\\theta\\) es el verdadero valor del parámetro. Equivariante: Si \\(\\hat{\\theta}\\) es el estimador de máxima verosimilitud de \\(\\theta\\), entonces \\(g(\\hat{\\theta})\\) es el estimador de máxima verosimilitud de \\(g(\\theta)\\). Supongamos \\(g\\) invertible, entonces \\(\\hat{\\theta} = g^{-1}(\\hat{\\eta})\\). Para cualquier \\(\\eta\\), \\[\\mathcal{L}(\\eta)=\\prod_{i=1}^n p(x_i;g^{-1}(\\eta)) = \\prod_{i=1}^n p(x_i;\\theta)=\\mathcal{L}(\\theta)\\] Por lo tanto, para cualquier \\(\\eta\\), \\[\\mathcal{L}(\\eta)=\\mathcal{L}(\\theta) \\leq \\mathcal{L}(\\hat{\\theta})=\\mathcal{L}(\\hat{\\eta})\\] y concluimos que \\(\\hat{\\eta}=g(\\hat{\\theta})\\) maximiza \\(\\mathcal{L}(\\eta)\\). Ejemplo: Binomial. El estimador de máxima verosimilitud es \\(\\hat{p}=\\bar{X}\\). Si \\(\\eta=log(p/(1-p)\\), entonces el estimador de máxima verosimilitud es \\(\\hat{\\eta}=log(\\hat{p}/(1-\\hat{p}))\\) Asintóticamente normal: \\(\\hat{\\theta} \\leadsto N(\\theta, I(\\theta)^{-1})\\), veremos a que nos referimos con \\(I(\\theta)^{-1}\\) en la siguiente sección. sim_sigma_hat &lt;- function(n = 50, mu_sim = 0, sigma_sim = 1){ x &lt;- rnorm(n, mu_sim, sigma_sim) sigma_hat &lt;- sqrt(sum((x - mean(x)) ^ 2) / n) } sigma_hats &lt;- rerun(1000, sim_sigma_hat(n = 5, mu_sim = 10, sigma_sim = 5)) %&gt;% flatten_dbl() # aprox normal con media theta y error estándar mean(sigma_hats) #&gt; [1] 4.237391 sd(sigma_hats) #&gt; [1] 1.525728 ggplot(data_frame(sigma_hats), aes(sample = sigma_hats)) + stat_qq() + stat_qq_line() #&gt; Warning: `data_frame()` is deprecated, use `tibble()`. #&gt; This warning is displayed once per session. Asintóticamente eficiente: A grandes razgos, esto quiere decir que del conjunto de estimadores con comportamiento estable, el estimador de máxima verosimilitud tiene la menor varianza al menos para muestras grandes (alcanza la cota de Cramer-Rao). Ahora que tenemos estimadores de maxima verosimilitud resta calcular errores estándar. Matriz de información y errores estándar La varianza de un estimador de máxima verosimilitud se calcula mediante la inversa de la matriz de información: \\[var(\\theta)=[I_n(\\theta)]^{-1}\\] La matriz de información es el negativo del valor esperado de la matriz Hessiana: \\[[I_n(\\theta)] = - E[H(\\theta)]\\] Y la Hessiana es la matriz de segundas derivadas de la log-verosimilitud respecto a los parámetros: \\[H(\\theta)=\\frac{d^2 \\mathcal{l}(\\theta)}{d\\theta d\\theta^´}\\] Entonces, la matriz de varianzas y covarianzas de \\(\\hat{\\theta}\\) es: \\[var(\\theta) = [I_n(\\theta)]^{-1} = \\big(-E[H(\\theta)\\big)^{-1}=\\bigg(-E\\bigg[\\frac{d^2 \\mathcal{l}(\\theta)}{d\\theta d\\theta^´}\\bigg]\\bigg)^{-1}\\] En el caso Bernoulli obtenemos \\(I_n(\\theta) = \\frac{n}{\\theta(1-\\theta)}\\). ¿Porqué se calculan de esta manera los errores estándar? Idea intuitiva: Podemos pensar que la curvatura de la función de verosimilitud nos dice que tanta certeza tenemos de la estimación de nuestros parámetros. Entre más curva es la función de verosimilitud mayor es la certeza de que hemos estimado el parámetro adecuado. La segunda derivada de la verosimilitud es una medida de la curvatura local de la misma, es por esto que se utiliza para estimar la incertidumbre con la que hemos estimado los parámetros. l_b1 &lt;- ggplot(xy, aes(x = x, y = y)) + stat_function(fun = L_bernoulli(n = 20, S = 10)) + xlab(expression(theta)) + ylab(expression(L(theta))) + labs(title = &quot;Verosimilitud&quot;, subtitle = &quot;n=20, S = 10&quot;) l_b2 &lt;- ggplot(xy, aes(x = x, y = y)) + stat_function(fun = L_bernoulli(n = 20, S = 14)) + xlab(expression(theta)) + ylab(expression(L(theta))) + labs(title = &quot;Verosimilitud&quot;, subtitle = &quot;n=20, S = 14&quot;) l_b3 &lt;- ggplot(xy, aes(x = x, y = y)) + stat_function(fun = L_bernoulli(n = 20, S = 19)) + xlab(expression(theta)) + ylab(expression(L(theta))) + labs(title = &quot;Verosimilitud&quot;, subtitle = &quot;n=20, S = 19&quot;) grid.arrange(l_b1, l_b2, l_b3, nrow = 1) Adicionalmente, resulta que el estimador de máxima verosimilitud \\(\\hat{\\theta}\\) es aproximadamente Normal por lo que obtenemos el siguiente resultado Bajo condiciones de regularidad apropiadas, se cumple: Definimos \\(se=\\sqrt{1/I_n(\\theta)}\\), entonces \\[\\frac{(\\hat{\\theta} - \\theta)}{se} \\leadsto N(0, 1)\\] Definimos \\(\\hat{se}=\\sqrt{1/I_n(\\hat{\\theta})}\\), entonces \\[\\frac{(\\hat{\\theta} - \\theta)}{\\hat{se}} \\leadsto N(0, 1)\\] El primer enunciado dice que \\(\\hat{\\theta} \\approx N(\\theta,se)\\), donde el error estándar de \\(\\hat{\\theta}\\) es \\(se=\\sqrt{1/I_n(\\theta)}\\). Por su parte el segundo enunciado dice que esto es cierto incluso si reemplazamos el error estándar por su aproximación \\(\\hat{se}=\\sqrt{1/I_n(\\hat{\\theta})}\\). Y podemos usar esto para construir intervalos de confianza. Ejemplo: Bernoulli. Supongamos \\(X_1,...X_n \\sim Bernoulli(\\theta)\\). El estimador de máxima verosimilitud es \\(\\hat{\\theta}=\\sum X_i/n\\) y un intervalo de aproximadamante \\(95\\%\\) de confianza es: \\[\\hat{\\theta} \\pm 1.96 \\bigg\\{\\frac{\\hat{\\theta}(1- \\hat{\\theta})}{n} \\bigg\\}^{1/2}\\] Método delta. Si \\(\\tau=g(\\theta)\\) donde \\(\\theta\\) consta de únicamente un parámetro, \\(g\\) es diferenciable y \\(g´(\\theta)\\neq 0\\) entonces \\[\\frac{\\sqrt{n}(\\hat{\\tau}-\\tau)}{\\hat{se}(\\hat{\\tau})}\\leadsto N(0, 1)\\] donde \\(\\hat{\\tau}=g(\\theta)\\) y \\[\\hat{se}(\\hat{\\tau})=|g´(\\hat{\\theta})|\\hat{se}(\\hat{\\theta})\\] Por tanto, el método delta nos da una método para aproximar el error estándar y crear intervalos de confianza aproximados. Existe también una extensión del método delta para el caso en que \\(\\theta\\) es un vector de dimensión mayor a uno, es decir cuando el modelo tiene más de un parámetro. Notemos que los errores estándar de máxima verosimilutud son asintóticos, en el caso de tener muestras chicas podemos utilizar bootstrap para calcular errores estándar. Incluso con muestras grandes puede ser más conveniente usar Bootstrap pues nos permite calcular errores estándar cuando no hay fórmulas analíticas. "],
["bootstrap-paramétrico.html", "9.2 Bootstrap paramétrico", " 9.2 Bootstrap paramétrico El método bootstrap se puede utilizar para el cálculo de errores estándar y de intervalos de confianza en un modelo paramétrico. Recordemos que en bootstrap no paramétrico obteníamos muestras \\(X_1^*,...,X_n^*\\) de la distribución empírica \\(P_n\\). En el caso de bootstrap paramétrico las muestras se obtienen de \\(p(x,\\hat{\\theta})\\) donde \\(\\hat{\\theta}\\) es una estimación de \\({\\theta}\\) (esta se puede obtener por máxima verosimilitud). Es así, que la diferencia entre la versión no paramétrica y la paramétrica es como construimos la distribución de la que vamos a seleccionar muestras. Ejemplo. Sea \\(X_1,...,X_n\\) i.i.d. con \\(X_i \\sim N(\\mu, \\sigma^2)\\). Sea \\(\\theta = g(\\mu,\\sigma)=\\sigma/\\mu\\), esta cantidad se conoce como el coeficiente de variación. Estima \\(\\theta\\) y su error estándar. Calculamos \\(\\hat{\\mu}=\\frac{1}{n} \\sum{X_i}\\) y \\(\\hat{\\sigma}=\\frac{1}{n} \\sum(X_i-\\hat{\\mu})^2\\). Repetimos \\(2\\) y \\(3\\) B veces: Simulamos \\(X_1^*,...,X_n^*\\) con \\(X_i^*\\sim N(\\hat{\\mu},\\hat{\\sigma}^2)\\). Calculamos \\(\\hat{\\mu}^*=\\frac{1}{n} \\sum{X_i^*}\\) y \\(\\hat{\\sigma}^2=\\frac{1}{n} \\sum(X_i^*-\\hat{\\mu}^*)^2\\) y \\(\\hat{\\theta}=\\hat{\\sigma}^*/\\hat{\\mu}^*\\). Estimamos el error estándar como: \\[\\hat{se}_B=\\sqrt{\\frac{1}{B-1}\\sum_{b=1}^B \\big(\\hat{\\theta}^*(b) - \\bar{\\theta}\\big)^2}\\] Veamos un ejemplo donde tenemos \\(200\\) observaciones con una distribución \\(Normal(10, 5^2)\\) y nos interesa estimar \\(\\theta=\\sigma/\\mu\\). n &lt;- 200 x &lt;- rnorm(n, mean = 10, sd = 5) # observaciones normales # Paso 1: calcular mu_hat y sigma_hat mu_hat &lt;- mean(x) sigma_hat &lt;- sqrt(1 / n * sum((x - mu_hat) ^ 2)) # Pasos 2 y 3 thetaBoot &lt;- function(){ # Simular X_1*,...X_N* con distribución N(mu_hat, sigma_hat^2) x_boot &lt;- rnorm(n, mean = mu_hat, sd = sigma_hat) # Calcular mu*, sigma* y theta* mu_boot &lt;- mean(x_boot) sigma_boot &lt;- sqrt(1 / n * sum((x_boot - mu_boot) ^ 2)) sigma_boot / mu_boot # theta* } # Paso 4: Repetimos B = 2000 veces y estimamos el error estándar sims_boot &lt;- rerun(3000, thetaBoot()) %&gt;% flatten_dbl() sqrt(1 / 2999 * sum((sims_boot - sigma_hat/mu_hat) ^ 2)) #&gt; [1] 0.03135791 Comparamos con el método delta: \\[\\hat{se}=\\frac{1}{\\sqrt{n}}\\bigg(\\frac{1}{\\hat{\\mu}^4} + \\frac{\\hat{\\sigma}^2}{2\\hat{\\mu}^2}\\bigg)^{1/2}\\] 1 / sqrt(n) * (1 / mu_hat ^ 4 + sigma_hat ^ 2 / (2 * mu_hat ^ 2)) ^ (1 / 2) #&gt; [1] 0.02514667 Supongamos que observamos \\(70\\) realizaciones de una Bernoulli, de tal manera que observamos \\(20\\) éxitos, calcula un intervalo de confianza usando bootstrap y comparalo con el correspondiente usando la información de Fisher. Ejemplo Bsplines: Bootstrap no paramétrico, bootstrap paramétrico y máxima verosimilitud Ilustraremos los métodos usando un ejemplo de suavizamiento tomado de Hastie, Tibshirani, and Friedman (2001) para esto comenzamos creando una base de datos artificial: set.seed(90984) # simple harmonic motion shm &lt;- function(t, A = 1.5, omega = 4){ # Esta es una función sinusoidal t * A * sin(omega * t) } n &lt;- 90 x &lt;- sample(seq(0, 3, 0.02), n) # creamos una base con n observaciones y &lt;- shm(x) + rnorm(length(x), sd = 1) outliers &lt;- sample(1:length(y), 4) # elijo 4 puntos al azar a los que agrego ruido y[outliers] &lt;- y[outliers] + rnorm(4, sd = 2) toy &lt;- data.frame(x, y) ggplot(toy, aes(x, y)) + geom_point() En nuestro ejemplo los datos consisten en pares \\(z_i=(x_i, y_i)\\) donde \\(y_i\\) se entiende como la respuesta o la salida correspondiente a \\(x_i\\). De la gráfica de los datos es claro que la relación entre \\(x\\) y \\(y\\) no es lineal, por lo que usaremos un método de expansiones de base que permite mayor flexibilidad. La idea básica detrás de expansión de bases es aumentar la dimensión del espacio de covariables (o predictores) creando variables adicionales que consisten en transformaciones de las variables originales \\(X\\), para luego usar modelos lineales en el espacio aumentado. Si denotamos por \\(h_m(X)\\) la \\(m\\)-ésima transformación de \\(X\\), con \\(m = 1,...,M\\), podemos modelar: \\[f(X)=\\sum_{i=1}^M \\beta_m h_m(X)\\] Lo conveniente de este enfoque es que una vez que determinamos las funciones base \\(h_m\\) los modelos son lineales en estas nuevas variables y podemos explotar las ventajas de usar modelos lineales. En lo que sigue supondremos que \\(X\\) es unidimensional (como en el ejemplo). Dentro de los métodos de expansión de bases estudiaremos los splines. Una función spline está fromada por polinomios de grado \\(k\\), cada uno definido sobre un intervalo, y se unen entre sí en los límites de cada intervalo. Los lugares donde se unen se conocen como nudos (knots). Antes de proceder entendamos los polinomios por pedazos: un polinomio por pedazos se obtiene dividiendo el dominio de \\(X\\) en intervalos contiguos y representando a \\(f\\) por medio de un polinomio en cada intervalo. Por ejemplo una constante por pedazos: library(Hmisc) #&gt; Loading required package: lattice #&gt; Loading required package: survival #&gt; Loading required package: Formula #&gt; #&gt; Attaching package: &#39;Hmisc&#39; #&gt; The following objects are masked from &#39;package:dplyr&#39;: #&gt; #&gt; src, summarize #&gt; The following objects are masked from &#39;package:base&#39;: #&gt; #&gt; format.pval, units toy_k &lt;- toy toy_k &lt;- toy %&gt;% mutate(int = cut2(x, g = 4)) %&gt;% group_by(int) %&gt;% mutate(media = mean(y)) ggplot(toy_k, aes(x, y)) + geom_point() + geom_line(aes(x, y = media, group = int), color = &quot;red&quot;) Debido a que dividimos el dominio en regiones disjuntas, el estimador de mínimos cuadrados para el modelo \\(f(X)=\\sum \\beta_m h_m(X)\\) es \\(\\hat{\\beta_m} = \\bar{Y}\\_m\\) la media de \\(Y\\) en cada región con \\(m=1,2,3,4\\). Ahora ajustamos un polinomio lineal por pedazos: ggplot(toy_k, aes(x, y)) + geom_point() + geom_smooth(method = &quot;lm&quot;, aes(x, y = y, group = int), color = &quot;red&quot;, se = FALSE) Normalmente preferimos que la función sea continua en los nudos, esto conlleva a restricciones es los parámetros o al uso de bases que incorporen las restricciones. Más aún, es conveniente restringir no solo a continuidad de la función sino a continuidad de las derivadas. Supongamos que decidimos ajustar splines cúbicos a los datos, con \\(3\\) nudos ubicados en los cuartiles de \\(X\\). Esto corresponde a un espacio lineal de funciones, la dimensión del espacio es \\(7\\) (\\(4\\) regiones \\(\\times\\) \\(4\\) parámetros por región - \\(3\\) nodos por \\(3\\) restricciones por nodo). library(fda) # paquete con funciones útiles de splines #&gt; Loading required package: splines #&gt; Loading required package: Matrix #&gt; #&gt; Attaching package: &#39;Matrix&#39; #&gt; The following objects are masked from &#39;package:tidyr&#39;: #&gt; #&gt; expand, pack, unpack #&gt; #&gt; Attaching package: &#39;fda&#39; #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; matplot knots &lt;- quantile(x) # usamos la función create.bspline.basis para crear la base base &lt;- create.bspline.basis( norder = 4, # polinomios cúbicos breaks = knots # nodos en los cuartiles de x ) plot(base, lty = &quot;solid&quot;) Podemos representar el espacio por medio de una expansión lineal en las funciones base: \\[\\mu(x) = \\sum_{j=1}^7 \\beta_j h_j(x)\\] donde \\(h_j(x)\\) son las \\(7\\) funciones que graficamos en la figura superior. Podemos pensar en \\(\\mu(x)\\) como una representación de la media condicional \\(E(Y|X=x)\\). H &lt;- eval.basis(x, base) head(H) #&gt; bspl4.1 bspl4.2 bspl4.3 bspl4.4 bspl4.5 bspl4.6 #&gt; [1,] 0 0.00000000 0.000000e+00 4.171308e-02 0.352541747 0.5536436 #&gt; [2,] 0 0.00000000 0.000000e+00 1.898638e-05 0.003282264 0.1344214 #&gt; [3,] 0 0.00000000 9.151512e-06 1.886607e-01 0.572668932 0.2386612 #&gt; [4,] 0 0.00000000 2.470908e-04 2.301142e-01 0.578033931 0.1916048 #&gt; [5,] 0 0.11986971 6.146891e-01 2.639151e-01 0.001526094 0.0000000 #&gt; [6,] 0 0.02203905 4.885123e-01 4.678374e-01 0.021611259 0.0000000 #&gt; bspl4.7 #&gt; [1,] 0.05210157 #&gt; [2,] 0.86227739 #&gt; [3,] 0.00000000 #&gt; [4,] 0.00000000 #&gt; [5,] 0.00000000 #&gt; [6,] 0.00000000 Sea \\(H\\) la matriz de \\(n \\times 7\\), donde el elemento \\(ij\\) corresponde a \\(h_j(x_i)\\). Entonces, el estimador usual de \\(\\beta\\) (obtenido minimizando el error cuadrático) esta dado por: \\[\\hat{\\beta} = (H^TH)^{-1}H^Ty\\] y con esto obtenemos: \\(\\hat{\\mu}(x) = \\sum_{j=1}^7 \\hat{\\beta_j} h_j(x)\\) beta_hat &lt;- as.vector(solve(t(H) %*% H) %*% t(H) %*% toy$y) beta_hat #&gt; [1] -0.8580413 1.1694904 1.2203256 -4.7069161 9.1974688 -8.5999809 -1.5685756 # creamos una función que calcula mu(x) mu &lt;- function(x, betas){ as.numeric(betas %*% t(eval.basis(x, base))) } ggplot(toy, aes(x = x, y = y)) + geom_point(alpha = 0.8) + stat_function(fun = mu, args = list(betas = beta_hat), color = &quot;blue&quot;) + labs(title = &quot;B-splines&quot;) Bootstrap no paramétrico. Usemos bootstrap para calcular errores estándar, para esto tomamos muestras con reemplazo de los pares \\(z_i = (x_i,y_i)\\), para cada muestra bootstrap \\(Z^*\\) ajustamos un polinomio cúbico \\(\\hat{\\mu}^*(x)\\) y construimos bandas de confianza usando los intervalos de cada punto. splinesBoot &lt;- function(){ toy_boot &lt;- sample_n(toy, size = n, replace = TRUE) H &lt;- eval.basis(toy_boot$x, base) as.vector(solve(t(H) %*% H) %*% t(H) %*% toy_boot$y) } betas &lt;- rerun(4000, splinesBoot()) %&gt;% reduce(rbind) splines_boot &lt;- ggplot(toy, aes(x = x, y = y)) for (i in 1:100) { splines_boot &lt;- splines_boot + stat_function(fun = mu, args = list(betas = betas[i, ]), alpha = 0.1) } splines_boot + geom_point(color = &quot;red&quot;, alpha = 0.5) La gráfica superior muestra \\(100\\) replicaciones bootstrap del suavizamiento. Construyamos los intervalos bootstrap, en cada \\(x\\) encontramos el \\(2.5\\%\\) más chico y más grande. # construimos los intervalos x_grid &lt;- seq(knots[1], knots[5], 0.02) # creamos un grid para evaluar mu(x) H &lt;- eval.basis(x_grid, base) # Evalúo la base en el rango betas_list &lt;- split(betas, seq(nrow(betas))) y &lt;- purrr::map_df(betas_list, ~ data_frame(x = x_grid, mu = as.vector(. %*% t(H)))) limites &lt;- y %&gt;% group_by(x) %&gt;% summarise( limite_inf = quantile(mu, probs = 0.025), limite_sup = quantile(mu, probs = 0.975) ) ggplot(limites) + geom_line(aes(x = x, y = limite_inf), color = &quot;darkgray&quot;) + geom_line(aes(x = x, y = limite_sup), color = &quot;darkgray&quot;) + geom_point(data = toy, aes(x = x, y = y), color = &quot;red&quot;, alpha = 0.5) + stat_function(fun = mu, args = list(betas = beta_hat), color = &quot;blue&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;) Supongamos ahora que los errores se distribuyen normal: \\[y = \\mu(x) + \\epsilon; \\epsilon \\sim N(0, \\sigma^2)\\] \\[\\mu(x) = \\sum_{j=1}^7 \\beta_j h_j(x)\\] utilicemos bootstrap paramétrico, simularemos \\[y_i^* = \\hat{\\mu}(x_i) + \\epsilon_i^*; \\epsilon_i^* \\sim N(0,\\hat{\\sigma}^2)\\] Para implementar bootstrap paramétrico comencemos calculando los estimadores de máxima verosimilitud: \\[\\hat{\\beta} = (H^TH)^{-1}H^Ty\\] y \\[\\hat{\\sigma}^2=1/n \\sum_{i=1}^n(y_i-\\hat{\\mu}(x_i))^2\\] mu_hat &lt;- mu(toy$x, beta_hat) sigma_hat &lt;- sqrt(1 / n * sum((toy$y - mu_hat) ^ 2)) # creamos las muestras bootstrap (paramétrico) splinesBootP &lt;- function(){ toy_boot &lt;- data.frame(x = toy$x, y = mu_hat + rnorm(n, 0, sigma_hat)) H &lt;- eval.basis(toy_boot$x, base) as.vector(solve(t(H) %*% H) %*% t(H) %*% toy_boot$y) } betas_p &lt;- rerun(4000, splinesBootP()) %&gt;% reduce(rbind) splines_boot_p &lt;- ggplot(toy, aes(x = x, y = y)) for (i in 1:100) { splines_boot_p &lt;- splines_boot_p + stat_function(fun = mu, args = list(betas = betas_p[i, ]), alpha = 0.1) } splines_boot + geom_point(color = &quot;red&quot;, alpha = 0.5) y construímos intervalos # construimos los intervalos x_grid &lt;- seq(knots[1], knots[5], 0.02) # creamos un grid para evaluar mu(x) H &lt;- eval.basis(x_grid, base) # Evalúo la base en el rango y &lt;- betas_p %*% t(H) # calculo mu(x*) betas_list &lt;- split(betas_p, seq(nrow(betas))) y &lt;- purrr::map_df(betas_list, ~ data_frame(x = x_grid, mu = as.vector(. %*% t(H)))) limites &lt;- y %&gt;% group_by(x) %&gt;% summarise( limite_inf = quantile(mu, probs = 0.025), limite_sup = quantile(mu, probs = 0.975) ) ggplot(limites) + geom_line(aes(x = x, y = limite_inf), color = &quot;darkgray&quot;) + geom_line(aes(x = x, y = limite_sup), color = &quot;darkgray&quot;) + geom_point(data = toy, aes(x = x, y = y), color = &quot;red&quot;, alpha = 0.5) + stat_function(fun = mu, args = list(betas = beta_hat), color = &quot;blue&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;) Máxima verosimilitud: \\[\\hat{Var}(\\hat{\\beta})=(H^T H) ^{-1}\\hat{\\sigma}^2\\] donde \\[\\hat{\\sigma}^2=1/n \\sum_{i=1}^n(y_i-\\hat{\\mu}(x_i))^2\\], ahora, la matriz de información de \\(\\theta=(\\beta,\\sigma^2)\\) es una diagonal con bloques y el bloque correspondiente a \\(\\beta\\) es: \\[I(\\beta)=(H^TH)/\\sigma^2\\] de tal manera que la varianza estimada es \\(I(\\beta)=(H^TH)/\\hat{\\sigma}^2\\). Podemos usar esto para construir las bandas en de errores estándar \\(\\hat{\\mu}(x) = h(x)^T\\hat{\\beta}\\) es: \\[\\hat{se}=[h(x)^T(H^TH)^{-1}h(x)]^{1/2}\\hat{\\sigma}\\] seMu &lt;- function(x){ # calculo h(x) h &lt;- eval.basis(x, base) # calcilo se_hat(x) se_hat &lt;- as.numeric((h %*% solve(t(H) %*% H) %*% t(h)) ^ (1 / 2) * sigma_hat) se_hat } max_ver_errores &lt;- data.frame(x = x_grid, y_min = mu(x_grid, beta_hat) - 2 * sapply(x_grid, seMu), y_max = mu(x_grid, beta_hat) + 2 * sapply(x_grid, seMu)) %&gt;% gather(cuantil, valor, y_min, y_max) ggplot(toy) + geom_point(color = &quot;red&quot;, aes(x = x, y = y), alpha = 0.5) + stat_function(fun = mu, args = list(betas = beta_hat), color = &quot;blue&quot;) + geom_line(data = max_ver_errores, aes(x = x, y = valor, group = cuantil), color = &quot;darkgray&quot;) + stat_function(fun = mu, args = list(betas = beta_hat), color = &quot;blue&quot;) + labs(x = &quot;&quot;, y = &quot;&quot;) En general el bootstrap paramétrico coinicide con máxima verosimilitud, la ventaja de bootstrap sobre máxima verosimilitud es que permite calcular estimaciones de máxima verosimilitud de errores estándar en escenarios donde no hay fórmulas disponibles. Por ejemplo, podríamos seleccionar el número y la ubicación de los nudos de manera adaptativa, usando validación cruzada. En este caso no hay fórmulas para el cálculo de errores estándar pero bootstrap sigue funcionando. Sean \\(X_1,...,X_n \\sim N(\\mu, 1)\\). Sea \\(\\theta = e^{\\mu}\\), crea una tabla de datos usando \\(\\mu=5\\) que consista de \\(n=100\\) observaciones. Usa el método delta para estimar \\(\\hat{se}\\) y crea un intervalo del \\(95\\%\\) de confianza. Usa boostrap paramétrico para crear un intervalo del \\(95\\%\\). Usa bootstrap no paramétrico para crear un intervalo del 95%. Compara tus respuestas. Realiza un histograma de replicaciones bootstrap para cada método, estas son estimaciones de la distribución de \\(\\hat{\\theta}\\). El método delta también nos da una aproximación a esta distribución: \\(Normal(\\hat{\\theta},\\hat{se}^2)\\). Comparalos con la verdadera distribución de \\(\\hat{\\theta}\\) (que puedes obtener vía simulación). ¿Cuál es la aproximación más cercana a la verdadera distribución? Pista: \\(se(\\hat{\\mu}) = 1/\\sqrt{n}\\) Referencias "],
["análisis-bayesiano.html", "Sección 10 Análisis bayesiano", " Sección 10 Análisis bayesiano Para esta sección seguiremos principalmente Kruschke (2015), sin embargo, para el desarrollo de las notas también se utilizó Gelman and Hill (2007), Gelman et al. (2013) y Bolstad (2010). Hasta ahora hemos estudiado métodos estadísticos frecuentistas (o clásicos), el punto de vista frecuentista se basa en los siguientes puntos (Wasserman (2010)): La probabilidad se refiere a un límite de frecuencias relativas, las probabilidades son propiedades objetivas en el mundo real. En un modelo, los parámetros son constantes fijas (desconocidas). Como consecuencia, no se pueden realizar afirmaciones probabilísticas útiles en relación a éstos. Los procedimientos estadísticos deben diseñarse con el objetivo de tener propiedades frecuentistas bien definidas. Por ejemplo, un intervalo de confianza del \\(95\\%\\) debe contener el verdadero valor del parámetro con frecuencia límite de al menos el \\(95\\%\\). Por su parte, el paradigma Bayesiano se basa en los siguientes postulados: La probabilidad describe grados de creencia, no frecuencias limite. Como tal uno puede hacer afirmaciones probabilísticas acerca de muchas cosas y no solo datos sujetos a variabilidad aleatoria. Por ejemplo, puedo decir: “La probabilidad de que Einstein tomara una copa de te el \\(1^0\\) de agosto de \\(1948\\)” es \\(0.35\\), esto no hace referencia a ninguna frecuencia relativa sino que refleja la certeza que yo tengo de que la proposición sea verdadera. Podemos hacer afirmaciones probabilísticas de parámetros. Podemos hacer inferencia de un parámetro \\(\\theta\\) por medio de distribuciones de probabilidad. Las infernecias como estimaciones puntuales y estimaciones de intervalos se pueden extraer de dicha distribución. Kruschke describe los puntos de arriba como dos ideas fundamentales del análisis bayesiano: La inferencia bayesiana es la reubicación de creencias a lo largo de posbilidades. How often have I said to you that when you have eliminated the impossible, whatever remains, however improbable, must be the truth? (Doyle, 1890, chap. 6). Las posibilidades son valores de los parámetros en modelos descriptivos. Referencias "],
["probabilidad-subjetiva.html", "10.1 Probabilidad subjetiva", " 10.1 Probabilidad subjetiva ¿Qué tanta certeza tienes de que una moneda acuñada por la casa de moneda mexicana es justa? Si, en cambio, consideramos una moneda antigua y asimétrica, ¿creemos que es justa? En estos escenarios no estamos considerando la verdadera probabilidad, inherente a la moneda, lo que queremos medir es el grado en que creemos que cada probabilidad puede ocurrir. Para especificar nuestras creencias debemos medir que tan verosímil pensamos que es cada posible resultado. Describir con presición nuestras creencias puede ser una tarea difícil, por lo que exploraremos como calibrar las creencias subjetivas. Calibración Considera una pregunta sencilla que puede afectar a un viajero: ¿Qué tanto crees que habrá una tormenta que ocasionará el cierre de la autopista México-Acapulco en el puente del \\(20\\) de noviembre? Como respuesta debes dar un número entre \\(0\\) y \\(1\\) que refleje tus creencias. Una manera de seleccionar dicho número es calibrar las creencias en relación a otros eventos cuyas probabilidades son claras. Como evento de comparación considera una experimento donde hay canicas en una urna: \\(5\\) rojas y \\(5\\) blancas. Seleccionamos una canica al azar. Usaremos esta urna como comparación para considerar la tormenta en la autopista. Ahora, considera el siguiente par de apuestas de las cuales puedes elegir una: A. Obtienes \\(\\$1000\\) si hay una tormenta que ocasiona el cierre de la autopista el próximo \\(20\\) de noviembre. B. Obtienes \\(\\$1000\\) si seleccionas una canica roja de la urna que contiene \\(5\\) canicas rojas y \\(5\\) blancas. Si prefieres la apuesta B, quiere decir que consideras que la probabilidad de tormenta es menor a \\(0.5\\), por lo que al menos sabes que tu creencia subjetiva de una la probabilidad de tormenta es menor a \\(0.5\\). Podemos continuar con el proceso para tener una mejor estimación de la creencia subjetiva. A. Obtienes \\(\\$1000\\) si hay una tormenta que ocasiona el cierre de la autopista el próximo \\(20\\) de noviembre. C. Obtienes \\(\\$1000\\) si seleccionas una canica roja de la urna que contiene \\(1\\) canica roja y \\(9\\) blancas. Si ahora seleccionas la apuesta \\(A\\), esto querría decir que consideras que la probabilidad de que ocurra una tormenta es mayor a \\(0.10\\). Si consideramos ambas comparaciones tenemos que tu probabilidad subjetiva se ubica entre \\(0.1\\) y \\(0.5\\). ¿Cuántos analfabetas dirías que había en México en \\(2015\\)? Da un intervalo del \\(90\\%\\) de confianza para esta cantidad. Más de calibración: Prueba de calibración de Messy Matters. Más pruebas en An Educated Guess. Descripción matemática de creencias subjetivas Cuando hay muchos posibles resultados de un evento es practicamente imposible calibrar las creencias subjetivas para cada resultado, en su lugar, podemos usar una función matemática. Por ejemplo, puedes pensar que una mujer mexicana promedio mide 156 cm pero estar abierto a la posibilidad de que el promedio sea un poco mayor o menor. Es así que puedes describir tus creencias a través de una curva con forma de campana y centrada en 156. No olvidemos que estamos describiendo probabilidades, subjetivas o no deben cumplir los axiomas de probabilidad. Es por esto que la curva debe conformar una distribuión de probabilidad. Ahora, si \\(p(\\theta)\\) representa el grado de nuestra creencia en los valores de \\(\\theta\\), entonces la media de \\(p(\\theta)\\) se puede pensar como un valor de \\(\\theta\\) que representa nuestra creencia típica o central. Por su parte, la varianza de \\(\\theta\\), que mide que tan dispersa esta la distribución, se puede pensar como la incertidumbre entre los posibles valores. "],
["regla-de-bayes-e-inferencia-bayesiana.html", "10.2 Regla de Bayes e inferencia bayesiana", " 10.2 Regla de Bayes e inferencia bayesiana Thomas Bayes (\\(1702-1761\\)) fue un matemático y ministro de la iglesia presbiteriana, en \\(1764\\) se publicó su famoso teorema. Una aplicación crucial de la regla de Bayes es determinar la probabilidad de un modelo dado un conjunto de datos. Lo que el modelo determina es la probabilidad de los datos condicional a valores particulares de los parámetros y a la estructura del modelo. Por su parte usamos la regla de Bayes para ir de la probabilidad de los datos, dado el modelo, a la probabilidad del modelo, dados los datos. Ejemplo: Lanzamientos de monedas Comencemos recordando la regla de Bayes usando dos variables aleatorias discretas. Lanzamos una moneda \\(3\\) veces, sea \\(X\\) la variable aleatoria correspondiente al número de Águilas observadas y \\(Y\\) registra el número de cambios entre águilas y soles. Escribe la distribución conjunta de las variables, y las distribuciones marginales. Considera la probabilidad de observar un cambio condicional a que observamos un águila y compara con la probabilidad de observar un águila condicional a que observamos un cambio. Para entender probabilidad condicional podemos pensar en restringir nuestra atención a una única fila o columna de la tabla. Ejemplo. Supongamos que alguien lanza una moneda \\(3\\) veces y nos informa que la secuencia contiene exactamente un cambio. Dada esta información podemos restringir nuestra atención a la fila correspondiente a un solo cambio. Sabemos que ocurrió uno de los eventos de esa fila. Las probabilidades relativas de los eventos de esa fila no han cambiado pero sabemos que la probabilidad total debe sumar uno, por lo que simplemente normalizamos dividiendo entre \\(p(C=1)\\). En este ejemplo, vemos que cuando no sabemos nada acerca del número de cambios, todo lo que sabemos de número de águilas está contenido en la distribución marginal de \\(X\\), por otro lado, si sabemos que hubo un cambio entonces sabemos que estamos en los escenarios de la fila correspondiente a un cambio, y calculamos estas probabilidades condicionales. Es así que nos movemos de creencias iniciales (marginal) acerca de \\(X\\) a creecnias posteriores (condicional). Regla de Bayes en modelos y datos Una de las aplicaciones más importantes de la regla de Bayes es cuando las variables fila y columna son datos y parámetros del modelo respectivamente. Un modelo especifica la probabilidad de valores particulares dado la estructura del modelo y valores de los parámetros. Por ejemplo en un modelo de lanzamientos de monedas tenemos \\[p(x = A|\\theta)=\\theta\\], \\[p(x = S|\\theta)= 1- \\theta\\] De manera general, el modelo especifica: \\[p(\\text{datos}|\\text{valores de parámetros y estructura del modelo})\\] y usamos la regla de Bayes para convertir la expresión anterior a lo que nos interesa de verdad, que es, que tanta certidumbre tenemos del modelo condicional a los datos: \\[p(\\text{valores de parámetros y estructura del modelo} | \\text{datos})\\] Una vez que observamos los datos, usamos la regla de Bayes para determinar o actualizar nuestras creencias de posibles parámetros y modelos. Entonces: Cuantificamos la información (o incertidumbre) acerca del parámetro desconocido \\(\\theta\\) mediante distribuciones de probabilidad. Antes de observar datos \\(x\\), cuantificamos la información de \\(\\theta\\) externa a \\(x\\) en una distribución a priori: \\[p(\\theta),\\] esto es, la distribución a priori resume nuestras creencias acerca del parámetro ajenas a los datos. Por otra parte, cuantificamos la información de \\(\\theta\\) asociada a \\(x\\) mediante la distribución de verosimilitud \\[p(x|\\theta)\\] Combinamos la información a priori y la información que provee \\(x\\) mediante el teorema de Bayes obteniendo así la distribución posterior \\[p(\\theta|x) \\propto p(x|\\theta)p(\\theta).\\] Las inferencias se obtienen de resúmenes de la distribución posterior. Ejemplo: Ingesta calórica en estudiantes Supongamos que nos interesa aprender los hábitos alimenticios de los estudiantes universitarios en México, y escuchamos que de acuerdo a investigaciones se recomienda que un adulto promedio ingiera \\(2500\\) kcal. Es así que buscamos conocer que proporción de los estudiantes siguen esta recomendación, para ello tomaremos una muestra aleatoria de estudiantes del ITAM. Denotemos por \\(\\theta\\) la proporción de estudiantes que ingieren en un día \\(2500\\) kcal o más. El valor de \\(\\theta\\) es desconocido, y desde el punto de vista bayesiano cuando tenemos incertidumbre de algo (puede ser un parámetro o una predicción) lo vemos como una variable aleatoria y por tanto tiene asociada una distribución de probabilidad que actualizaremos conforme obtenemos información (observamos datos). Recordemos que la distribución \\(p(\\theta)\\) se conoce como la distribución a priori y representa nuestras creencias de los posibles valores que puede tomar el parámetro. Supongamos que tras leer artículos y entrevistar especialistas consideramos los posibles valores de \\(\\theta\\) y les asigmanos pesos: library(tidyverse) theta &lt;- seq(0.05, 0.95, 0.1) pesos.prior &lt;- c(1, 5.2, 8, 7.2, 4.6, 2.1, 0.7, 0.1, 0, 0) prior &lt;- pesos.prior/sum(pesos.prior) prior_df &lt;- tibble(theta, prior = round(prior, 3)) prior_df #&gt; # A tibble: 10 x 2 #&gt; theta prior #&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.05 0.035 #&gt; 2 0.15 0.18 #&gt; 3 0.25 0.277 #&gt; 4 0.35 0.249 #&gt; 5 0.45 0.159 #&gt; 6 0.55 0.073 #&gt; 7 0.65 0.024 #&gt; 8 0.75 0.003 #&gt; 9 0.85 0 #&gt; 10 0.95 0 Una vez que cuantificamos nuestro conocimiento (o la falta de este) sobre los posibles valores que puede tomar \\(\\theta\\) especificamos la verosimilitud y la distribución conjunta \\(p(x, \\theta)\\), donde \\(x = (x_1,...,x_N)\\) veamos la distribución de un estudiante en particular: \\[p(x_i|\\theta) \\sim Bernoulli(\\theta),\\] para \\(i=1,...,N\\), es decir, condicional a \\(\\theta\\) la probabilidad de que un estudiante ingiera más de \\(2500\\) calorías es \\(\\theta\\) y la función de verosimilitud \\(p(x_1,...,x_N|\\theta) = \\mathcal{L}(\\theta)\\): \\[p(x_1,...,x_N|\\theta) = \\prod_{n=1}^N p(x_n|\\theta)\\] \\[= \\theta^z(1 - \\theta)^{N-z}\\] donde \\(z\\) denota el número de estudiantes que ingirió al menos \\(2500\\) kcal y \\(N-z\\) el número de estudiantes que ingirió menos de \\(2500\\) kcal. Ahora calculamos la distribución posterior de \\(\\theta\\) usando la regla de Bayes: \\[p(\\theta|x) = \\frac{p(x_1,...,x_N,\\theta)}{p(x)}\\] \\[\\propto p(\\theta)\\mathcal{L}(\\theta)\\] Vemos que la distribución posterior es proporcional al producto de la verosimilitud y la distribución inicial, el denominador \\(p(x)\\) no depende de \\(\\theta\\) por lo que es constante (como función de \\(\\theta\\)) y esta ahí para normalizar la distribución posterior asegurando que tengamos una distribución de probabilidad. Inicial discreta Volviendo a nuestro ejemplo, usamos la inicial discreta que discutimos (tabla de pesos normalizados) y supongamos que tomamos una muestra de \\(30\\) alumnos de los cuales \\(z=11\\) ingieren al menos \\(2500\\) kcal, calculemos la distribución posterior de \\(\\theta\\), usando que \\[\\mathcal{L}(\\theta) = \\theta^{z}(1-\\theta)^{N-z}\\] con \\(0&lt;\\theta&lt;1\\) library(LearnBayes) N &lt;- 30 # estudiantes z &lt;- 11 # éxitos # Verosimilitud Like &lt;- theta ^ z * (1 - theta) ^ (N - z) product &lt;- Like * prior # Distribución posterior (normalizamos) post &lt;- product / sum(product) dists &lt;- bind_cols(prior_df, post = post) round(dists, 3) #&gt; # A tibble: 10 x 3 #&gt; theta prior post #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.05 0.035 0 #&gt; 2 0.15 0.18 0.006 #&gt; 3 0.25 0.277 0.22 #&gt; 4 0.35 0.249 0.529 #&gt; 5 0.45 0.159 0.224 #&gt; 6 0.55 0.073 0.021 #&gt; 7 0.65 0.024 0 #&gt; 8 0.75 0.003 0 #&gt; 9 0.85 0 0 #&gt; 10 0.95 0 0 # También podemos usar la función pdisc pdisc(p = theta, prior = prior, data = c(z, N - z)) %&gt;% round(3) #&gt; [1] 0.000 0.006 0.220 0.529 0.224 0.021 0.000 0.000 0.000 0.000 # Alargamos los datos para graficar dists_l &lt;- dists %&gt;% gather(dist, val, prior:post) %&gt;% mutate(dist = factor(dist, levels = c(&quot;prior&quot;, &quot;post&quot;))) ggplot(dists_l, aes(x = theta, y = val)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgray&quot;) + facet_wrap(~ dist) + labs(x = expression(theta), y = expression(p(theta))) ¿Cómo se ve la distribución posterior si tomamos una muestra de tamaño \\(90\\) donde observamos la misma proporción de éxitos. Realiza los cálculos y graficala como un panel adicional de la gráfica anterior. ¿Cómo definirías la distribución inicial si no tuvieras conocimiento de los artículos y expertos? Evidencia Ahora, en el teorema de Bayes también encontramos el término \\(p(x)\\) que denominamos la evidencia, también se conoce como verosimilitud marginal. La evidencia es la probabilidad de los datos de acuerdo al modelo y se calcula sumando a lo largo de todos los posibles valores de los parámetros y ponderando por nuestra certidumbre en esos valores de los parámetros. Es importante notar que hablamos de valores de los parámetros \\(\\theta\\) únicamente en el contexto de un modelo particular pues este el que da sentido a los parámetros. Podemos hacer evidente el modelo en la notación, \\[p(\\theta|x,M)=\\frac{p(x|\\theta,M)p(\\theta|M)}{p(x|M)}\\] en este contexto la evidencia se define como: \\[p(x|M)=\\int p(x|\\theta,M)p(\\theta|M)d\\theta\\] La notación anterior es conveniente sobre todo cuando estamos considerando más de un modelo y queremos usar los datos para determinar la certeza que tenemos en cada modelo. Supongamos que tenemos dos modelos \\(M_1\\) y \\(M_2\\), entonces podemos calcular el cociente de \\(p(M_1|x)\\) y \\(p(M_2|x)\\) obteniendo: \\[\\frac{p(M_1|x)}{p(M_2|x)} = \\frac{p(x|M_1) \\cdot p(M_1)}{p(x|M_2)\\cdot p(M_2)}\\] El cociente de evidencia \\(\\frac{p(x|M_1)}{p(x|M_2)}\\) se conoce como factor de Bayes, y \\(p(M_i)\\) describe nuestras creencias iniciales en cada modelo. La evidencia y el factor de Bayes no son muy usados en la práctica pero los vemos por su valor conceptual. Invarianza en el orden de los datos Vimos que la regla de Bayes nos permite pasar del conocimiento inicial \\(p(\\theta)\\) al posterior \\(p(\\theta|x)\\) conforme recopilamos datos. Supongamos ahora que observamos más datos, los denotamos \\(x&#39;\\), podemos volver a actualizar nuestras creencias pasando de \\(p(\\theta|x)\\) a \\(p(\\theta|x,x&#39;)\\). Entonces podemos preguntarnos si nuestro conocimiento posterior cambia si actualizamos de acuerdo a \\(x\\) primero y después \\(x&#39;\\) o vice-versa. La respuesta es que si \\(p(x|\\theta)\\) y \\(p(x&#39;|\\theta)\\) son iid entonces el orden en que actualizamos nuestro conocimiento no afecta la distribución posterior. La invarianza al orden tiene sentido intuitivamente: Si la función de verosimilitud no depende del tiempo o del ordenamineto de los datos, entonces la posterior tampoco tiene porque depender del ordenamiento de los datos. Like &lt;- theta ^ 1 * (1 - theta) ^ (1 - 1) product &lt;- Like * prior$p post &lt;- product / sum(product) post #&gt; [1] 0.125 0.500 0.375 Objetivos de la inferencia Los tres objetivos de la inferencia son: estimación de parámetros, predicción de valores y comparación de modelos. La estimación de parámetros implica determinar hasta que punto creemos en cada posible valor del parámetro. En estadística bayesiana la estimación se realiza con la distribución posterior sobre los valores de los parámetros \\(\\theta\\). La siguiente gráfica ejemplifica un experimento Bernoulli, con dos posibles iniciales, los datos observados son \\(N=20\\) lanzamientos de moneda que resultan en \\(12\\) éxitos o águilas. Predicción de valores. Usando nuestro conocimiento actual nos interesa predecir la probabilidad de datos futuros. La probabilidad predictiva de un dato \\(\\tilde{y}\\) (no observado) se determina promediando las probabilidades predictivas de los datos a lo largo de todos los posibles valores de los parámetros y ponderados por la creencia en los valores de los parámetros. Cuando solo contamos con nuestro conocimiento incial tendríamos: \\[p(\\tilde{y}) =\\int p(y|\\theta)p(\\theta)d\\theta\\] Notese que la ecuación anterior coincide con la correspondiente a la evidencia, con la diferencia de que la evidencia se refiere a un valor observado y en esta ecuación estamos calculando la probabilidad de cualquier valor \\(y\\). Una vez que observamos datos tenemos la distribución predictiva posterior: \\[p(\\tilde{y}|x) =\\int p(y|\\theta)p(\\theta|x)d\\theta\\] Por ejemplo podemos usar las creencias iniciales del modelo \\(1\\), que propusimos arriba para calcular la probabilidad predictiva de observar águila: \\[p(y=S) = \\sum_{\\theta}p(y=A|\\theta)p(\\theta) = 0.5\\] Vale la pena destacar que las prediciones son probabilidades de cada posible valor condicional a nuestro modelo de creencias actuales. Si nos interesa predecir un valor particular en lugar de una distribución a lo largo de todos los posibles valores podemos usar la media de la distribución predictiva. Por tanto el valor a predecir sería: \\[p(y)=\\int y p(y) dy\\] La integral anterior únicamente tiene sentido si \\(y\\) es una variable continua. Si \\(y\\) es nominal, como el resultado de un volado, entonces podemos usar el valor más probable. Comparación de modelos, cuando comparamos modelos usando el factor de Bayes, una caracterítica conveniente en estadística bayesiana es que la complejidad del modelo se toma en cuenta de manera automática. Recordemos los dos modelos discretos, en el primero supusimos que el parámetro \\(\\theta\\) únicamente puede tomar uno de \\(3\\) valores \\((0.25, 0.5, 0.75)\\), esta restricción dió lugar a un modelo simple. Por su parte, el modelo \\(2\\) es más complejo y permite muchos más valores de \\(\\theta\\) (\\(51\\)). La forma de la distribución inicial es triangular en ambos casos y el valor de mayor probabilidad inicial es \\(\\theta = 0.50\\) y reflejamos que creemos que es menos factible que el valor se encuentre en los extremos. Podemos calcular el factor de Bayes para distintos datos observados: # Modelo 1, 3 posibles valores p_M1 &lt;- tibble(theta = c(0.25, 0.5, 0.75), prior = c(0.25, 0.5, 0.25), modelo = &quot;M1&quot;) # Modelo 2, Creamos una inicial que puede tomar más valores p &lt;- seq(0, 24, 1) p2 &lt;- c(p, 24, sort(p, decreasing = TRUE)) p_M2 &lt;- tibble(theta = seq(0, 1, 0.02), prior = p2 / sum(p2), modelo = &quot;M2&quot;) N &lt;- 20 # estudiantes z &lt;- 12 # éxitos dists_h &lt;- bind_rows(p_M1, p_M2) %&gt;% # base de datos horizontal group_by(modelo) %&gt;% mutate( Like = theta ^ z * (1 - theta) ^ (N - z), # verosimilitud posterior = (Like * prior) / sum(Like * prior) ) dists &lt;- dists_h %&gt;% # base de datos larga gather(dist, valor, prior, Like, posterior) %&gt;% mutate(dist = factor(dist, levels = c(&quot;prior&quot;, &quot;Like&quot;, &quot;posterior&quot;))) factorBayes &lt;- function(N, z){ evidencia &lt;- bind_rows(p_M1, p_M2) %&gt;% # base de datos horizontal group_by(modelo) %&gt;% mutate( Like = theta ^ z * (1 - theta) ^ (N - z), # verosimilitud posterior = (Like * prior) / sum(Like * prior) ) %&gt;% summarise(evidencia = sum(prior * Like)) print(evidencia) return(evidencia[1, 2] / evidencia[2, 2]) } factorBayes(50, 25) #&gt; # A tibble: 2 x 2 #&gt; modelo evidencia #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 M1 4.44e-16 #&gt; 2 M2 2.75e-16 #&gt; evidencia #&gt; 1 1.6142 factorBayes(100, 75) #&gt; # A tibble: 2 x 2 #&gt; modelo evidencia #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 M1 9.46e-26 #&gt; 2 M2 4.17e-26 #&gt; evidencia #&gt; 1 2.269739 factorBayes(100, 10) #&gt; # A tibble: 2 x 2 #&gt; modelo evidencia #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 M1 1.36e-18 #&gt; 2 M2 2.47e-16 #&gt; evidencia #&gt; 1 0.005494554 factorBayes(40, 38) #&gt; # A tibble: 2 x 2 #&gt; modelo evidencia #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 M1 0.000000279 #&gt; 2 M2 0.00000895 #&gt; evidencia #&gt; 1 0.03120138 ¿Cómo explicarías los resultados anteriores? La evidencia de un modelo \\(p(x|M)\\) no dice mucho por si misma, es más relevante en el contexto del factor de Bayes (la evidencia relativa de dos modelos). Es importante recordar que la comparación de modelos nos habla únicamente de la evidencia relativa de un modelo; sin embargo, puede que ninguno de los modelos que estamos considerando sean adecuados para nuestros datos, por lo que más adelante estudiaremos otras maneras de evaluar un modelo. Cálculo de la distribución posterior En la inferencia Bayesiana se requiere calcular el denominador de la fórmula de Bayes \\(p(x)\\), es común que esto requiera que se calcule una integral complicada; sin embargo, hay algunas maneras de evitar esto, El camino tradicional consiste en usar funciones de verosimilitud con dsitribuciones iniciales conjugadas. Cuando una distribución inicial es conjugada de la verosimilitud resulta en una distribución posterior con la misma forma funcional que la distribución inicial. Otra alternativa es aproximar la integral numericamente. Cuando el espacio de parámetros es de dimensión chica, se puede cubrir con una cuadrícula de puntos y la integral se puede calcular sumando a través de dicha cuadrícula. Sin embargo cuando el espacio de parámetros aumenta de dimensión el número de puntos necesarios para la aproximación crece demasiado y hay que recurrir a otas técnicas. Se ha desarrollado una clase de métodos de simulación para poder calcular la distribución posterior, estos se conocen como cadenas de Markov via Monte Carlo (MCMC por sus siglas en inglés). El desarrollo de los métodos MCMC es lo que ha propiciado el desarrollo de la estadística bayesiana en años recientes. "],
["distribuciones-conjugadas.html", "10.3 Distribuciones conjugadas", " 10.3 Distribuciones conjugadas Ejemplo: Bernoulli Comenzaremos con el modelo Beta-Binomial. Recordemos que si \\(X\\) en un experimento con dos posibles resultados, \\(X\\) se distribuye Bernoulli y la función de probabilidad esta definida por: \\[p(x|\\theta)=\\theta^x(1-\\theta)^{1-x}\\] si lanzamos una moneda \\(N\\) veces tenemos un conjunto de datos \\(\\{x_1,...,x_N\\}\\), suponemos que los lanzamientos son independientes por lo que la probabilidad de observar el conjunto de \\(N\\) lanzamientos es el producto de las probabilidades para cada observación: \\[p(x_1,...,x_N|\\theta) = \\prod_{n=1}^N p(x_n|\\theta)\\] \\[= \\theta^z(1 - \\theta)^{N-z}\\] donde \\(z\\) denota el número de éxitos (águilas). Ahora, en principio para describir nuestras creencias iniciales podríamos usar cualquier función de densidad con soporte en \\([0, 1]\\), sin embargo, sería conveniente que el producto \\(p(x|\\theta)p(\\theta)\\) (el numerador de la fórmula de Bayes) resulte en una función con la misma forma que \\(p(\\theta)\\). Cuando este es el caso, las creencias inicial y posterior se describen con la misma distribución. Esto es conveninte pues si obtenemos nueva información podemos actualizar nuestro conocimiento de manera inmediata, conservando la forma de las distribuciones. Cuando las funciones \\(p(x|\\theta)\\) y \\(p(\\theta)\\) se combinan de tal manera que la distribución posterior pertenece a la misma familia (tiene la misma forma) que la distribución inicial, entonces decimos que \\(p(\\theta)\\) es conjugada para \\(p(x|\\theta)\\). Vale la pena notar que la inicial es conjugada únicamente respecto a una función de verosimilitud particular. Una distribución conjugada para \\(p(x|\\theta) = \\theta^z(1 - \\theta)^{N-z}\\) es una \\(Beta(a, b)\\) \\[p(\\theta) = \\frac {\\theta^{a-1}(1-\\theta)^{b-1}}{B(a,b)}\\] Para describir nuestro conocimiento inicial podemos explorar la media y desviación estándar de la distribución beta, la media es \\[\\bar{\\theta} = a/(a+b)\\] por lo que si \\(a=b\\) la media es \\(0.5\\) y conforme aumenta \\(a\\) en relación a \\(b\\) aumenta la media. La desviación estándar es \\[\\sqrt{\\bar{\\theta}(1-\\bar{\\theta})/(a+b+1)}\\] Una manera de seleccionar los parámetros \\(a\\) y \\(b\\) es pensar en la proporción media de águilas (\\(m\\)) y el tamaño de muestra (\\(n\\)). Ahora, \\(m=a/(a+b)\\) y \\(n = a+b\\), obteniendo. \\[a=mn, b=(1-m)n\\] Otra manera es comenzar con la media y la desviación estándar. Al usar este enfoque debemos recordar que la desviación estándar debe tener sentido en el contexto de la densidad beta. En particular la desviación estándar típicamente es menor a \\(0.289\\) que corresponde a la desviación estándar de una uniforme. Entonces, para una densidad beta con media \\(m\\) y desviación estándar \\(s\\), los parámetros son: \\[a=m\\bigg(\\frac{m(1-m)}{s^2}- 1\\bigg), b=(1-m)\\bigg(\\frac{m(1-m)}{s^2}- 1\\bigg)\\] Una vez que hemos determinado una inicial conveniente para la verosimilitud Bernoulli, veamos la posterior. Supongamos que observamos \\(N\\) lanzamientos de los cuales \\(z\\) son águilas, entonces podemos ver que la posterior es nuevamente una densidad Beta. \\[p(\\theta|z)\\propto \\theta^{a+z-1}(1 -\\theta)^{(N-z+b)-1}\\] Concluímos entonces que si la distribución inicial es \\(Beta(a,b),\\) la posterior es \\(Beta(z+a,N-z+b).\\) Vale la pena explorar la relación entre la distribución inicial y posterior en las medias. La media incial es \\[a/(a+b)\\] y la media posterior es \\[(z+a)/[(z+a) + (N-z+b)]=(z+a)/(N+a+b)\\] podemos hacer algunas manipulaciones algebráicas para escribirla como: \\[\\frac{z+a}{N+a+b}=\\frac{z}{N}\\frac{N}{N+a+b} + \\frac{a}{a+b}\\frac{a+b}{N+a+b}\\] es decir, podemos escribir la media posterior como un promedio ponderado entre la media inicial \\(a/(a+b)\\) y la proporción observada \\(z/N\\). Ahora podemos pasar a la inferencia, comencemos con estimación de la proporción \\(\\theta\\). La distribución posterior resume todo nuestro conocimiento del parámetro \\(\\theta\\), en este caso podemos graficar la distribución y extraer valores numéricos como la media. library(gridExtra) #&gt; #&gt; Attaching package: &#39;gridExtra&#39; #&gt; The following object is masked from &#39;package:dplyr&#39;: #&gt; #&gt; combine N = 14; z = 11; a = 1; b = 1 base &lt;- ggplot(tibble(x = c(0, 1)), aes(x)) p1 &lt;- base + stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), aes(colour = &quot;inicial&quot;), show.legend = FALSE) + stat_function(fun = dbeta, args = list(shape1 = z + 1, shape2 = N - z + 1), aes(colour = &quot;verosimilitud&quot;), show.legend = FALSE) + stat_function(fun = dbeta, args = list(shape1 = a + z, shape2 = N - z + b), aes(colour = &quot;posterior&quot;), show.legend = FALSE) + labs(y = &quot;&quot;, colour = &quot;&quot;, x = expression(theta)) N = 14; z = 11; a = 100; b = 100 p2 &lt;- base + stat_function(fun = dbeta, args = list(shape1 = a, shape2 = b), aes(colour = &quot;inicial&quot;)) + stat_function(fun = dbeta, args = list(shape1 = z + 1, shape2 = N - z + 1), aes(colour = &quot;verosimilitud&quot;)) + stat_function(fun = dbeta, args = list(shape1 = a + z, shape2 = N - z + b), aes(colour = &quot;posterior&quot;)) + labs(y = &quot;&quot;, colour = &quot;&quot;, x = expression(theta)) grid.arrange(p1, p2, nrow = 1, widths = c(0.38, 0.62)) knitr::include_app(&quot;https://tereom.shinyapps.io/app_bernoulli/&quot;, height = &quot;1000px&quot;) Una manera de resumir la distribución posterior es a través de intervalos de probabilidad, otro uso de los intervalos es establecer que valores del parámetro son creíbles. Calcula un intervalo del \\(95\\%\\) de probabilidad para cada una de las distribuciones posteriores del ejemplo. Ahora pasemos a predicción, calculamos la probabilidad de \\(y =1\\): \\[p(y = 1) = \\int p(y=1|\\theta)p(\\theta|z)d\\theta\\] \\[=\\int \\theta p(\\theta|z,N) d\\theta\\] \\[=(z+a)/(N+a+b)\\] Esto es, la probabilidad predictiva de águila es la media de la distribución posterior sobre \\(\\theta\\). Finalmente, comparemos modelos. Para esto calculamos la evidencia \\(p(x|M)\\) para cada modelo: \\[p(x|M)=\\int p(x|\\theta,M)p(\\theta|M)d\\theta\\] en este caso los datos están dados por \\(z\\) y \\(N\\), en el caso de la incial beta es fácil calcular la evidencia: \\[p(z)=B(z+a,N-z+b)/B(a,b)\\] En nuestro ejemplo, una inicial tuiene un pico en \\(0.5\\) mientras que la otra es uniforme. Por otra parte, la proporción de \\(1\\) observados en la muestra no es cercana a \\(0.5\\) por lo que la inicial picuda no captura los datos muy bien. # N = 14, z = 11, a = 1, b = 1 beta(12, 4) / beta(1, 1) #&gt; [1] 0.0001831502 # N = 14, z = 12, a = 100, b = 100 beta(126, 126) / beta(100, 100) #&gt; [1] 1.97762e-16 Supongamos que observamos una secuencia en la que la mitad de los volados resultan en águila: # N = 14, z = 7, a = 1, b = 1 beta(8, 8) / beta(1, 1) #&gt; [1] 1.942502e-05 # N = 14, z = 7, a = 100, b = 100 beta(107, 107) / beta(100, 100) #&gt; [1] 5.900009e-05 En general, preferimos un modelo con un valor mayor de \\(p(x|\\theta)\\) pero la preferencia no es absoluta, una diferencia chica no nos dice mucho. Debemos considerar que los datos no son mas que una muestra aleatoria. Supongamos que nos interesa analizar el IQ de una muestra de estudiantes del ITAM y suponemos que el IQ de un estudiante tiene una distribución normal \\(x \\sim N(\\theta, \\sigma^2)\\) con \\(\\sigma ^ 2\\) conocida. Considera que observamos el IQ de un estudiante \\(x\\). La verosimilitud del modelo es: \\[p(x|\\theta)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x-\\theta)^2\\right)\\] Realizaremos un análisis bayesiano por lo que hace falta establer una distribución inicial, elegimos \\(p(\\theta)\\) que se distribuya \\(N(\\mu, \\tau^2)\\) donde elegimos los parámetros \\(\\mu, \\tau\\) que mejor describan nuestras creencias iniciales. Calcula la distribución posterior \\(p(\\theta|x) \\propto p(x|\\theta)p(\\theta)\\), usando la inicial y verosimilitud que definimos arriba. Una vez que realices la multiplicación debes identificar el núcleo de una distribución Normal, ¿cuáles son sus parámetros (media y varianza)? "],
["aproximación-por-cuadrícula.html", "10.4 Aproximación por cuadrícula", " 10.4 Aproximación por cuadrícula Supongamos que la distribución beta no describe nuestras creencias de manera adecuada. Por ejemplo, mis creencias podrían estar mejor representadas por una distribución trimodal: la moneda esta fuertemente sesgada hacia sol, fuertemente sesgada hacia águila o es justa. No hay parámetros en una beta que puedan describir este patrón. Exploraremos entonces una técnica de aproximación numérica de la distribución posterior que consiste en definir la distribución inicial en una cuadrícula de valores de \\(\\theta\\). En este método no necesitamos describir nuestras creencias mediante una función matemática ni realizar integración analítica. Suponemos que existe únicamente un número finito de valores de \\(\\theta\\) que creemos que pueden ocurrir (el primer ejemplo que estudiamos usamos esta técnica). Es así que la regla de Bayes se escribe como: \\[p(x|\\theta)=\\frac{p(x|\\theta)p(\\theta)}{\\sum_{\\theta}p(x|\\theta)p(\\theta)}\\] Entonces, si podemos discretizar una distribución inicial continua mediante una cuadrícula de masas de probabilidad discreta podemos usar la versión discreta de la regla de Bayes. El proceso consiste en dividir el dominio en regiones, crear un rectángulo con la altura correspondiente al valor de la densidad en el punto medio. Aproximamos el área de cada región mediante la altura del rectángulo. # N = 14, z = 11, a = 1, b = 1 N = 14; z = 11 inicial &lt;- data.frame(theta = seq(0.05, 1, 0.05), inicial = rep(1/20, 20)) dists_h &lt;- inicial %&gt;% mutate( verosimilitud = theta ^ z * (1 - theta) ^ (N - z), # verosimilitud posterior = (verosimilitud * inicial) / sum(verosimilitud * inicial) ) dists &lt;- dists_h %&gt;% # base de datos larga gather(dist, valor, inicial, verosimilitud, posterior) %&gt;% mutate(dist = factor(dist, levels = c(&quot;inicial&quot;, &quot;verosimilitud&quot;, &quot;posterior&quot;))) ggplot(dists, aes(x = theta, y = valor)) + geom_point() + facet_wrap(~ dist, scales = &quot;free&quot;) + scale_x_continuous(expression(theta), breaks = seq(0, 1, 0.2)) + labs(y = &quot;&quot;) y lo podemos comparar con la versión continua (distribución beta). En cuanto a la estimación, la tabla de probabilidades nos da una estimación para los valores de los parámetros. Podemos calcular la media de \\(\\theta\\) como el promedio ponderado por las probabilidades: \\(\\bar{\\theta}=\\sum_{\\theta} \\theta p(\\theta|x)\\) head(dists_h) #&gt; theta inicial verosimilitud posterior #&gt; 1 0.05 0.05 4.186401e-15 1.142584e-12 #&gt; 2 0.10 0.05 7.290000e-12 1.989641e-09 #&gt; 3 0.15 0.05 5.312031e-10 1.449799e-07 #&gt; 4 0.20 0.05 1.048576e-08 2.861851e-06 #&gt; 5 0.25 0.05 1.005828e-07 2.745181e-05 #&gt; 6 0.30 0.05 6.076142e-07 1.658346e-04 sum(dists_h$posterior * dists_h$theta) #&gt; [1] 0.7500629 Ahora para intervalos de probabilidad, debido a que estamos usando masas discretas, la suma de las masas en un intervalo usualmente no será igual a \\(95\\%\\) y por tanto elegimos los puntos tales que la masa sea mayor a igual a \\(95\\%\\) y la masa total sea lo menor posible, en nuestro ejemplo podemos usar cuantiles. dist_cum &lt;- cumsum(dists_h$posterior) #vector de distribución acumulada lb &lt;- which.min(dist_cum &lt; 0.05) - 1 ub &lt;- which.min(dist_cum &lt; 0.975) dists_h$theta[lb] #&gt; [1] 0.5 dists_h$theta[ub] #&gt; [1] 0.9 Para el problema de predicción, la probabilidad predictiva para el siguiente valor \\(y\\) es simplemente la probabilidad de que ocurra dicho valor ponderado por la probabilidad posterior correspondiente: \\[p(y|x)=\\int p(y|\\theta)p(\\theta|x)d\\theta\\] \\[\\approx \\sum_{\\theta} p(y|\\theta)p(\\theta|x)\\] Calcula la probabilidad predictiva para \\(y=1\\) usando los datos del ejemplo. Finalmente, para la comparación de modelos la integral que define la evidencia \\[p(x|M)=\\int p(x|\\theta,M)p(\\theta|M)d\\theta\\] se convierte en una suma \\[p(x|M)\\approx \\sum_{\\theta} p(x|\\theta,M)p(\\theta|M)d\\theta\\] # calcula el factor de Bayes para el experimento Bernoulli Modelos M1 y M2 factorBayes &lt;- function(M, s){ evidencia &lt;- rbind(p_M1, p_M2) %&gt;% # base de datos horizontal group_by(modelo) %&gt;% mutate( Like = theta ^ s * (1 - theta) ^ (M - s), # verosimilitud posterior = (Like * prior) / sum(Like * prior) ) %&gt;% summarise(evidencia = sum(prior * Like)) print(evidencia) return(evidencia[1, 2] / evidencia[2, 2]) } factorBayes(50, 25) #&gt; # A tibble: 2 x 2 #&gt; modelo evidencia #&gt; &lt;chr&gt; &lt;dbl&gt; #&gt; 1 M1 4.44e-16 #&gt; 2 M2 2.75e-16 #&gt; evidencia #&gt; 1 1.6142 "],
["mcmc.html", "10.5 MCMC", " 10.5 MCMC Hay ocasiones en las que los métodos de inicial conjugada y aproximación por cuadrícula no funcionan, hay casos en los que la distribución beta no describe nuestras creencias iniciales. Por su parte, la aproximación por cuadrícula no es factible cuando tenemos varios parámetros. Es por ello que surge la necesidad de utilizar métodos de Monte Carlo vía Cadenas de Markov (MCMC). Introducción Metrópolis Para usar Metrópolis debemos poder calcular \\(p(\\theta)\\) para un valor particular de \\(\\theta\\) y el valor de la verosimilitud \\(p(x|\\theta)\\) para cualquier \\(x\\), \\(\\theta\\) dados. En realidad, el método únicamente requiere que se pueda calcular el producto de la inicial y la verosimilitud, y sólo hasta una constante de proporcionalidad. Lo que el método produce es una aproximación de la distribución posterior \\(p(\\theta|x)\\) mediante una muestra de valores \\(\\theta\\) obtenido de dicha distribución. Caminata aleatoria. Con el fin de entender el algoritmo comenzaremos estudiando el concepto de caminata aleatoria. Supongamos que un vendedor de yakult trabaja a lo largo de una cadena de islas: constantemente viaja entre las islas ofreciendo sus productos, al final de un día de trabajo decide si permanece en la misma isla o se transporta a una de las \\(2\\) islas vecinas. El vendedor ignora la distribución de la población en las islas y el número total de islas; sin embargo, una vez que se encuentra en una isla puede investigar la población de la misma y también de la isla a la que se propone viajar después. El objetivo del vendedor es visitar las islas de manera proporcional a la población de cada una. Con esto en mente el vendedor utiliza el siguiente proceso: Lanza un volado, si el resultado es águila se propone ir a la isla del lado izquierdo de su ubicación actual y si es sol a la del lado derecho. Si la isla propuesta en el paso anterior tiene población mayor a la población de la isla actual, el vendedor decide viajar a ella. Si la isla vecina tiene población menor, entonces visita la isla propuesta con una probabilidad que depende de la población de las islas. Sea \\(P_{prop}\\) la población de la isla propuesta y \\(P_{actual}\\) la población de la isla actual. Entonces el vendedor cambia de isla con probabilidad \\[p_{mover}=P_{prop}/P_{actual}\\] A la larga, si el vendedor sigue la heurística anterior la probabilidad de que el vendedor este en alguna de las islas coincide con la población relativa de la isla. islas &lt;- data.frame(islas = 1:10, pob = 1:10) caminaIsla &lt;- function(i){ # i: isla actual u &lt;- runif(1) # volado v &lt;- ifelse(u &lt; 0.5, i - 1, i + 1) # isla vecina (índice) if (v &lt; 1 | v &gt; 10) { # si estas en los extremos y el volado indica salir return(i) } u2 &lt;- runif(1) p_move = min(islas$pob[v] / islas$pob[i], 1) if (p_move &gt; u2) { return(v) # isla destino } else { return(i) # me quedo en la misma isla } } pasos &lt;- 100000 camino &lt;- numeric(pasos) camino[1] &lt;- sample(1:10, 1) # isla inicial for (j in 2:pasos) { camino[j] &lt;- caminaIsla(camino[j - 1]) } caminata &lt;- tibble(pasos = 1:pasos, isla = camino) plot_caminata &lt;- ggplot(caminata[1:1000, ], aes(x = pasos, y = isla)) + geom_point(size = 0.8) + geom_path(alpha = 0.5) + coord_flip() + labs(title = &quot;Caminata aleatoria&quot;) + scale_y_continuous(expression(theta), breaks = 1:10) + scale_x_continuous(&quot;Tiempo&quot;) plot_dist &lt;- ggplot(caminata, aes(x = isla)) + geom_histogram() + scale_x_continuous(expression(theta), breaks = 1:10) + labs(title = &quot;Distribución objetivo&quot;, y = expression(P(theta))) grid.arrange(plot_caminata, plot_dist, ncol = 1, heights = c(4, 2)) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Entonces: Para aproximar la distribución objetivo debemos permitir que el vendedor recorra las islas durante una sucesión larga de pasos y registramos sus visitas. Nuestra aproximación de la distribución es justamente el registro de sus visitas. Más aún, debemos tener cuidado y excluir la porción de las visitas que se encuentran bajo la influencia de la posición inicial. Esto es, debemos excluir el periodo de calentamiento. Una vez que tenemos un registro largo de los viajes del vendedor (excluyendo el calentamiento) podemos aproximar la distribución objetivo de cada valor de \\(\\theta\\) simplemente contando el número relativo de veces que el vendedor visitó dicha isla. t &lt;- c(1:10, 20, 50, 100, 200, 1000, 5000) plots_list &lt;- lapply(t, function(i){ ggplot(caminata[1:i, ], aes(x = isla)) + geom_histogram() + labs(y = &quot;&quot;, x = &quot;&quot;, title = paste(&quot;t = &quot;, i, sep = &quot;&quot;)) + scale_x_continuous(expression(theta), breaks = 1:10, limits = c(0, 11)) }) args.list &lt;- c(plots_list,list(nrow = 4, ncol = 4)) invoke(grid.arrange, args.list) Escribamos el algoritmo, para esto indexamos las islas por el valor \\(\\theta\\), es así que la isla del extremo oeste corresponde a \\(\\theta=1\\) y la población relativa de cada isla es \\(P(\\theta)\\): El vendedor se ubica en \\(\\theta_{actual}\\) y propone moverse a la izquierda o derecha con probabilidad \\(0.5\\). El rango de los posibles valores para moverse, y la probabilidad de proponer cada uno se conoce como distribución propuesta, en nuestro ejemplo sólo toma dos valores cada uno con probabilidad \\(0.5\\). Una vez que se propone un movimiento, decidimos si aceptarlo. La decisión de aceptar se basa en el valor de la distribución objetivo en la posición propuesta, relativo al valor de la distribución objetivo en la posición actual: \\[p_{mover}=min\\bigg\\{\\frac{P(\\theta_{propuesta})}{P(\\theta_{actual})},1\\bigg\\}\\] Notemos que la distribución objetivo \\(P(\\theta)\\) no necesita estar normalizada, esto es porque lo que nos interesa es el cociente \\(P(\\theta_{propuesta})/P(\\theta_{actual})\\). Una vez que propusimos un movimiento y calculamos la probabilidad de aceptar el movimiento aceptamos o rechazamos el movimiento generando un valor de una distribución uniforme, si dicho valor es menor a \\(p_{mover}\\) entonces hacemos el movimiento. Entonces, para utilizar el algoritmo necesitamos ser capaces de: Generar un valor de la distribución propuesta (para crear \\(\\theta_{propuesta}\\)). Evaluar la distribución objetivo en cualquier valor propuesto (para calcular \\(P(\\theta_{propuesta})/P(\\theta_{actual})\\)). Generar un valor uniforme (para movernos con probabilidad \\(p_{mover}\\)) Las \\(3\\) puntos anteriores nos permiten generar muestras aleatorias de la distribución objetivo, sin importar si esta está normalizada. Esta técnica es particularmente útil cuando cuando la distribución objetivo es una posterior proporcional a \\(p(x|\\theta)p(\\theta)\\). Para entender porque funciona el algoritmo de Metrópolis hace falta entender \\(2\\) puntos, primero que la distribución objetivo es estable: si la probabilidad actual de ubicarse en una posición coincide con la probabilidad en la distribución objetivo, entonces el algoritmo preserva las probabilidades. library(expm) transMat &lt;- function(P){ # recibe vector de probabilidades (o población) T &lt;- matrix(0, 10, 10) n &lt;- length(P - 1) # número de estados for (j in 2:n - 1) { # llenamos por fila T[j, j - 1] &lt;- 0.5 * min(P[j - 1] / P[j], 1) T[j, j] &lt;- 0.5 * (1 - min(P[j - 1] / P[j], 1)) + 0.5 * (1 - min(P[j + 1] / P[j], 1)) T[j, j + 1] &lt;- 0.5 * min(P[j + 1] / P[j], 1) } # faltan los casos j = 1 y j = n T[1, 1] &lt;- 0.5 + 0.5 * (1 - min(P[2] / P[1], 1)) T[1, 2] &lt;- 0.5 * min(P[2] / P[1], 1) T[n, n] &lt;- 0.5 + 0.5 * (1 - min(P[n - 1] / P[n], 1)) T[n, n - 1] &lt;- 0.5 * min(P[n - 1] / P[n], 1) T } T &lt;- transMat(islas$pob) w &lt;- c(0, 1, rep(0, 8)) t &lt;- c(1:10, 20, 50, 100, 200, 1000, 5000) expT &lt;- map_df(t, ~data.frame(t = ., w %*% (T %^% .))) expT_long &lt;- expT %&gt;% gather(theta, P, -t) %&gt;% mutate(theta = parse_number(theta)) ggplot(expT_long, aes(x = theta, y = P)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgray&quot;) + facet_wrap(~ t) + scale_x_continuous(expression(theta), breaks = 1:10, limits = c(0, 11)) El segundo punto es que el proceso converge a la distribución objetivo. Podemos ver, (en nuestro ejemplo sencillo) que sin importar el punto de inicio se alcanza la distribución objetivo. inicioP &lt;- function(i){ w &lt;- rep(0, 10) w[i] &lt;- 1 t &lt;- c(1, 10, 50, 100) expT &lt;- map_df(t, ~data.frame(t = ., inicio = i, w %*% (T %^% .))) %&gt;% gather(theta, P, -t, -inicio) %&gt;% mutate(theta = parse_number(theta)) expT } expT &lt;- map_df(c(1, 3, 5, 9), inicioP) ggplot(expT, aes(x = as.numeric(theta), y = P)) + geom_bar(stat = &quot;identity&quot;, fill = &quot;darkgray&quot;) + facet_grid(inicio ~ t) + scale_x_continuous(expression(theta), breaks = 1:10, limits = c(0, 11)) "],
["metrópolis.html", "10.6 Metrópolis", " 10.6 Metrópolis En la sección anterior implementamos el algoritmo de Metrópolis en un caso sencillo: las posiciones eran discretas, en una dimensión y la propuesta era únicamente mover a la izquierda o a la derecha. El algoritmo general aplica para valores continuos, en cualquier número de dimensiones y con distribuciones propuesta más generales. Lo esencial del método no cambia para el caso general: Metropolis Tenemos una distribución objetivo \\(p(\\theta)\\) de la cual buscamos generar muestras. Debemos ser capaces de calcular el valor de \\(p(\\theta)\\) para cualquier valor candidato \\(\\theta\\). La distribución objetivo \\(p(\\theta)\\) no tiene que estar normalizada, típicamente \\(p(\\theta)\\) es la distribución posterior de \\(\\theta\\) no normalizada, es decir, es el producto de la verosimilitud y la inicial. La muestra de la distribución objetivo se genera mediante una caminata aleatoria a través del espacio de parámetros. La caminata inicia en un lugar arbitrario (definido por el usuario). El punto inicial debe ser tal que \\(p(\\theta)&gt;0\\). La caminata avanza en cada tiempo proponiendo un movimiento a una nueva posición y después decidiendo si se acepta o no el valor propuesto. Las distribuciones propuesta pueden tener muchas formas, el objetivo es que la distribución propuesta explore el espacio de parámetros de manera eficiente. Una vez que tenemos un valor propuesto calculamos: \\[p_{mover}=min\\bigg( \\frac{P(\\theta_{propuesta})}{P(\\theta_{actual})},1\\bigg)\\] y aceptamos el valor propuesto con probabilidad \\(p_{mover}\\) Y al final obtenemos valores representativos de la distribución objetivo \\(\\{\\theta_1,...,\\theta_n\\}\\) Es importante recordar que debemos excluir las primeras observaciones pues estas siguen bajo la influencia del valor inicial. Retomemos el problema de inferencia Bayesiana y veamos como usar el algoritmo de Metrópolis cuando la distribución objetivo es la distribución posterior. 10.6.0.1 Ejemplo: Bernoulli Retomemos el ejemplo del experimento Bernoulli, iniciamos con una función de distribución que describa nuestro conocimiento inicial y tal que podamos calcular \\(p(\\theta)\\) con facilidad. En este caso elegimos una densidad beta y podemos usar función dbeta de R: # p(theta) con theta = 0.4, a = 2, b = 2 dbeta(0.4, 2, 2) #&gt; [1] 1.44 # Definimos la distribución inicial prior &lt;- function(a = 1, b = 1){ function(theta) dbeta(theta, a, b) } También necesitamos especificar la función de verosimilitud, en nuestro caso tenemos repeticiones de un experimento Bernoulli por lo que: \\[\\mathcal{L}(\\theta) \\propto \\theta^{z}(1-\\theta)^{N-z}\\] y en R: # Verosimilitid binomial likeBern &lt;- function(z, N){ function(theta){ theta ^ z * (1 - theta) ^ (N - z) } } Por tanto la distribución posterior \\(p(\\theta|x)\\) es, por la regla de Bayes, proporcional a \\(p(x|\\theta)p(\\theta)\\). Usamos este producto como la distribución objetivo en el algoritmo de Metrópolis. # posterior no normalizada postRelProb &lt;- function(theta){ mi_like(theta) * mi_prior(theta) } Implementemos el algoritmo con una inicial \\(Beta(1,1)\\) (uniforme) y observaciones \\(z = \\sum{x_i}=11\\) y \\(N = 14\\), es decir lanzamos \\(14\\) volados de los cuales \\(11\\) resultan en águila. # Datos observados N &lt;- 14 z &lt;- 11 # Defino mi inicial y la verosimilitud mi_prior &lt;- prior() # inicial uniforme mi_like &lt;- likeBern(z, N) # verosimilitud de los datos observados # para cada paso decidimos el movimiento de acuerdo a la siguiente función caminaAleat &lt;- function(theta){ # theta: valor actual salto_prop &lt;- rnorm(1, 0, sd = 0.1) # salto propuesto theta_prop &lt;- theta + salto_prop # theta propuesta if (theta_prop &lt; 0 | theta_prop &gt; 1) { # si el salto implica salir del dominio return(theta) } u &lt;- runif(1) p_move &lt;- min(postRelProb(theta_prop) / postRelProb(theta), 1) # prob mover if (p_move &gt; u) { return(theta_prop) # aceptar valor propuesto } else{ return(theta) # rechazar } } set.seed(47405) pasos &lt;- 6000 camino &lt;- numeric(pasos) # vector que guardará las simulaciones camino[1] &lt;- 0.1 # valor inicial # Generamos la caminata aleatoria for (j in 2:pasos){ camino[j] &lt;- caminaAleat(camino[j - 1]) } caminata &lt;- data.frame(pasos = 1:pasos, theta = camino) ggplot(caminata[1:3000, ], aes(x = pasos, y = theta)) + geom_point(size = 0.8) + geom_path(alpha = 0.5) + scale_y_continuous(expression(theta), limits = c(0, 1)) + scale_x_continuous(&quot;Tiempo&quot;) + geom_vline(xintercept = 600, color = &quot;red&quot;, alpha = 0.5) # excluímos las primeras observaciones (etapa de calentamiento) caminata_f &lt;- filter(caminata, pasos &gt; 600) ggplot(caminata_f, aes(x = theta)) + geom_density(adjust = 2, aes(color = &quot;posterior&quot;)) + labs(title = &quot;Distribución posterior&quot;, y = expression(p(theta)), x = expression(theta)) + stat_function(fun = mi_prior, aes(color = &quot;inicial&quot;)) + # inicial xlim(0, 1) Si la distribución objetivo es muy dispersa y la distribución propuesta muy estrecha, entonces se necesitarán muchos pasos para que la caminata aleatoria cubra la distribución con una muestra representativa. Por otra parte, si la distribución propuesta es muy dispersa podemos caer en rechazar demasiados valores propuestos. Imaginemos que \\(\\theta_{actual}\\) se ubica en una zona de densidad alta, entonces cuando los valores propuestos están lejos del valor actual se ubicarán en zonas de menor densidad y \\(p(\\theta_{propuesta})/p(\\theta_{actual})\\) tenderá a ser chico y el movimiento propuesto será aceptado en pocas ocasiones. ¿Qué porcentaje de los valores propuestos son aceptados? Si cambias la desviación estándar de la distribución propuesta a \\(\\sigma = 0.01\\) y \\(\\sigma = 2\\), ¿Cómo cambia el porcentaje de aceptación de valores propuestos? ¿De los \\(3\\) valores que usamos, qué desviación estándar crees que sea más conveniente? De la muestra de valores de \\(p(\\theta|x)\\) obtenidos usando el algoritmo de Metrópolis podemos estimar aspectos de la verdadera distribución \\(p(\\theta|x)\\). Por ejemplo, para resumir la tendencia central es fácil calcular la media y la mediana. mean(caminata_f$theta) #&gt; [1] 0.7483886 sd(caminata_f$theta) #&gt; [1] 0.1093274 En el caso de predicción: sims_y &lt;- rbinom(nrow(caminata_f), size = 1, prob = caminata_f$theta) mean(sims_y) # p(y = 1 | x) probabilidad predictiva #&gt; [1] 0.7503704 sd(sims_y) #&gt; [1] 0.4328387 Inferencia de dos proporciones binomiales Consideramos la situación en la que nos interesa estudiar dos proporciones \\(\\theta_1\\) y \\(\\theta_2\\) correspondientes a dos grupos. Queremos determinar que debemos creer de estas proporciones tras observar datos provenientes de ambos grupos. Esto es relevante en muchos casos, por ejemplo, en un análisis clínico nos podría interesar evaluar el efecto de una nueva medicina y queremos comoparar la tasa de éxito en el grupo control contra el grupo de tratamiento. En el marco Bayesiano comenzamos definiendo nuestras creencias iniciales. En este caso nuestras creencias describen combinaciones de parámetros, es decir debemos especificar \\(p(\\theta_1, \\theta_2)\\) para todas las combinaciones \\(\\theta_1, \\theta_2\\). Un caso sencillo es asumir que nuestras creencias de \\(\\theta_1\\) son independientes de nuestras creencias de \\(\\theta_2\\). Esto implica: \\[p(\\theta_1, \\theta_2) = p(\\theta_1)p(\\theta_2)\\] para todo valor de \\(\\theta_1\\) y de \\(\\theta_2\\) y donde \\(p(\\theta_1)\\) y \\(p(\\theta_2)\\) corresponden a las distribuciones marginales. Las creencias de los dos parámetros no tienen porque ser independientes, por ejemplo, puedo creer que dos monedas acuñadas en la misma casa de moneda tienen un sesgo similar, en este caso las manipulaciones matemáticas son más complicadas pero es posible describir estas creencias mediante una distribución bivariada. Además de las creencias iniciales tenemos datos observados. Suponemos que los lanzamientos dentro de cada grupo son independientes y que los lanzamientos entre los grupos también lo son. Es importante recalcar que siempre suponemos independencia en los datos, sin importar nuestros supuestos de independencia en las creencias. Para el primer grupo observamos la secuencia \\(\\{x_{1,1},...,x_{1,N_1}\\}\\) que contiene \\(z_1\\) águilas, y en el otro grupo observamos la sucesión \\(\\{x_{2,1},...,x_{2,N_2}\\}\\) que contiene \\(z_2\\) águilas. Es decir, \\[z_1 = \\sum_{i=1}^{N_1}x_{1,i}\\] Por simplicidad denotamos los datos por \\(x =\\{x_{1,1},...,x_{1,N_1}x_{2,1},...,x_{2,N_2}\\}\\) Debido a la independencia de los lanzamientos tenemos: \\[p(x|\\theta_1,\\theta_2)=\\prod_{i=1}^{N_1}p(x_{1,i}|\\theta_1,\\theta_2)\\cdot \\prod_{i=1}^{N_2}p(x_{2,i}|\\theta_1,\\theta_2)\\] \\[= \\theta_1^{z_1}(1-\\theta_1)^{N_1-z_1}\\cdot \\theta_2^{z_2}(1-\\theta_2)^{N_2-z_2}\\] Usamos la regla de Bayes para calcular la distribución posterior: \\[p(\\theta_1,\\theta_2|x)=p(x|\\theta_1,\\theta_2)p(\\theta_1, \\theta_2) / p(x)\\] \\[= \\frac{p(x|\\theta_1,\\theta_2)p(\\theta_1, \\theta_2)} { \\int\\int p(x|\\theta_1,\\theta_2)p(\\theta_1, \\theta_2)d\\theta_1d\\theta_2}\\] Distribuciones conjugadas Siguiendo el caso de la familia Beta-Bernoulli que estudiamos en el caso de una proporción y suponiendo independencia en las creencias, \\(p(\\theta_1, \\theta_2) = p(\\theta_1)p(\\theta_2)\\). Escribimos la densidad inicial como el producto de dos densidades beta, donde \\(\\theta_1\\) se distribuye \\(Beta(a_1, b_1)\\) y \\(\\theta_2\\) se distribuye \\(Beta(a_2, b_2)\\). \\[p(\\theta_1, \\theta_2)=\\frac{\\theta_1^{a_1-1}(1-\\theta)^{b_1-1}} {B(a_1, b_1)}\\frac{\\cdot \\theta_2^{a_2-1}(1-\\theta)^{b_2-1}}{B(a_2, b_2)}\\] donde \\(B(a, b) = \\Gamma(a)\\Gamma(b) / \\Gamma(a+b)\\). La posterior se escribe: \\[p(\\theta_1, \\theta_2|x)=\\frac{\\theta_1^{z_1+a_1-1}(1-\\theta)^{b_1-1}\\cdot \\theta_2^{z_2+a_2-1}(1-\\theta)^{b_2-1}}{p(x)\\cdot B(a_1, b_1) \\cdot B(a_2, b_2)}\\] Resumiendo, cuando la inicial es el producto de distribuciones beta independientes, la posterior también es el producto de distribuciones beta independientes. Veamos las gráficas. grid &lt;- expand.grid(x = seq(0.01, 1, 0.01), y = seq(0.01, 1, 0.01)) grid_inicial &lt;- grid %&gt;% mutate(z = round(dbeta(x, 3, 3) * dbeta(y, 3, 3), 1)) binom_2_inicial &lt;- ggplot(grid_inicial, aes(x = x, y = y, z = z)) + # geom_tile(aes(fill = z)) + geom_raster(aes(fill = z), show.legend = FALSE) + geom_contour(colour = &quot;white&quot;) + # stat_contour(binwidth = 0.6, aes(color = ..level..), show.legend = FALSE) + scale_x_continuous(expression(theta[1]), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), limits = c(0, 1)) + scale_color_gradient(expression(p(theta[1],theta[2])), limits = c(0, 8.6)) + scale_fill_gradient(expression(p(theta[1],theta[2])), limits = c(0, 8.6)) + coord_fixed() + labs(title = &quot;Inicial&quot;) # z_1=5, N_1=7, z_2=2, N_2=7, a_1=a_2=b_1=b_2=3 grid_v &lt;- grid %&gt;% mutate(z = round(dbeta(x, 6, 3) * dbeta(y, 3, 6), 1)) binom_2_verosimilitud &lt;- ggplot(grid_v, aes(x = x, y = y, z = z)) + geom_raster(aes(fill = z), show.legend = FALSE) + geom_contour(colour = &quot;white&quot;) + # stat_contour(binwidth = 0.6, aes(color = ..level..), show.legend = FALSE) + scale_x_continuous(expression(theta[1]), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), limits = c(0, 1)) + scale_color_gradient(expression(p(theta[1],theta[2])), limits = c(0, 8.6)) + scale_fill_gradient(expression(p(theta[1],theta[2])), limits = c(0, 8.6)) + coord_fixed() + labs(title = &quot;Verosimilitud&quot;) # z_1=5, N_1=7, z_2=2, N_2=7 grid_post &lt;- grid %&gt;% mutate(z = round(dbeta(x, 8, 5) * dbeta(y, 5, 8), 1)) binom_2_posterior &lt;- ggplot(grid_post, aes(x = x, y = y, z = z)) + geom_raster(aes(fill = z)) + geom_contour(colour = &quot;white&quot;, show.legend = FALSE) + # stat_contour(binwidth = 0.6, aes(color = ..level..), show.legend = FALSE) + scale_x_continuous(expression(theta[1]), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), limits = c(0, 1)) + # scale_color_gradient(expression(p(theta[1],theta[2])), limits = c(0, 8.6)) + scale_fill_gradient(expression(p(theta[1],theta[2])), limits = c(0, 8.6)) + coord_fixed() + labs(title = &quot;Posterior&quot;) grid.arrange(binom_2_inicial, binom_2_verosimilitud, binom_2_posterior, nrow = 1, widths = c(0.3, 0.3, 0.4)) library(plotly) #&gt; #&gt; Attaching package: &#39;plotly&#39; #&gt; The following object is masked from &#39;package:ggplot2&#39;: #&gt; #&gt; last_plot #&gt; The following object is masked from &#39;package:stats&#39;: #&gt; #&gt; filter #&gt; The following object is masked from &#39;package:graphics&#39;: #&gt; #&gt; layout grid_inicial_pl &lt;- grid_inicial %&gt;% spread(y, z) %&gt;% as.matrix() pl_inicial &lt;- plot_ly(z = grid_inicial_pl) %&gt;% add_surface(cmin = 0, cmax = 9) grid_v_pl &lt;- grid_v %&gt;% spread(y, z) %&gt;% as.matrix() pl_verosimilitud &lt;- plot_ly(z = grid_v_pl) %&gt;% add_surface(cmin = 0, cmax = 9) grid_post_pl &lt;- grid_post %&gt;% spread(y, z) %&gt;% as.matrix() pl_post &lt;- plot_ly(z = grid_post_pl) %&gt;% add_surface(cmin = 0, cmax = 9) pl_inicial pl_verosimilitud pl_post Metrópolis Al igual que en el caso univariado escribimos la verosimilitud y la distribución inicial: # Verosimilitid binomial likeBern2 &lt;- function(z_1, N_1, z_2, N_2){ function(theta){ theta[1] ^ z_1 * (1 - theta[1]) ^ (N_1 - z_1) * theta[2] ^ z_2 * (1 - theta[2]) ^ (N_2 - z_2) } } prior2 &lt;- function(a_1 = 3, b_1 = 3, a_2 = 3, b_2 = 3){ function(theta) dbeta(theta[1], a_1 , b_1) * dbeta(theta[2], a_2 , b_2) } # posterior no normalizada postRelProb2 &lt;- function(theta){ mi_like(theta) * mi_prior(theta) } Implementemos el algoritmo con una inicial \\(Beta(3,3)\\) y observaciones \\(z_1=5\\), \\(N = 7\\), \\(z_1=2\\) y \\(N = 7\\), es decir lanzamos \\(14\\) volados \\(7\\) de cada moneda. z_1=5; N_1=7; z_2=2; N_2=7; a_1=3; a_2=3; b_1=3; b_2=3 # Datos observados N = 14 z = 11 # Defino mi inicial y la verosimilitud mi_prior &lt;- prior2() # inicial uniforme mi_like &lt;- likeBern2(z_1 = z_1, N_1 = N_1, z_2 = z_2, N_2 = N_2) # verosimilitud Para proponer saltos usaremos una distribución normal bivariada. El movimiento se acepta de manera definitiva si la posterior es más densa que la posición actual y se acepta de manera probabilística si la posición actual es más densa. # para cada paso decidimos el movimiento de acuerdo a la siguiente función caminaAleat2 &lt;- function(theta){ # theta: valor actual salto_prop &lt;- MASS::mvrnorm(n = 1 , mu = rep(0, 2), Sigma = matrix(c(0.08, 0, 0, 0.08), byrow = TRUE, nrow = 2)) # salto propuesto theta_prop &lt;- theta + salto_prop # theta propuesta if (all(theta_prop &lt; 0) | all(theta_prop &gt; 1)) { # salir del dominio return(theta) } u &lt;- runif(1) p_move = min(postRelProb2(theta_prop) / postRelProb2(theta), 1) # prob mover if (p_move &gt; u) { return(theta_prop) # aceptar valor propuesto } else{ return(theta) # rechazar } } set.seed(47405) pasos &lt;- 6000 camino &lt;- matrix(0, nrow = pasos, ncol = 2) # vector que guardará las simulaciones camino[1, ] &lt;- c(0.1, 0.8) # valor inicial # Generamos la caminata aleatoria for (j in 2:pasos) { camino[j, ] &lt;- caminaAleat2(camino[j - 1, ]) } caminata &lt;- data.frame(pasos = 1:pasos, theta_1 = camino[, 1], theta_2 = camino[, 2]) ggplot(caminata, aes(x = theta_1, y = theta_2)) + geom_point(size = 0.8) + geom_path(alpha = 0.3) + scale_x_continuous(expression(theta[1]), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), limits = c(0, 1)) + coord_fixed() caminata_m &lt;- caminata %&gt;% gather(parametro, val, theta_1, theta_2) %&gt;% arrange(pasos) ggplot(caminata_m[1:2000, ], aes(x = pasos, y = val)) + geom_path(alpha = 0.5) + facet_wrap(~parametro, ncol = 1) + scale_y_continuous(&quot;&quot;, limits = c(0, 1)) "],
["muestreador-de-gibbs.html", "10.7 Muestreador de Gibbs", " 10.7 Muestreador de Gibbs El algoritmo de Metrópolis es muy general y se puede aplicar a una gran variedad de problemas. Sin embargo, afinar los parámetros de la distribución propuesta para que el algoritmo funcione correctamente puede ser complicado. Por otra parte, el muestredor de Gibbs no necesita de una distribución propuesta. Para implementar un muestreador de Gibbs se necesita ser capaz de generar muestras de la distribución posterior condicional a cada uno de los parámetros individuales. Esto es, el muestreador de Gibbs permite generar muestras de la posterior: \\[p(\\theta_1,...,\\theta_p|x)\\] siempre y cuando podamos generar valores de todas las distribuciones condicionales: \\[p(\\theta_k,|\\theta_1,...,\\theta_{k-1},\\theta_{k+1},...,\\theta_p,x)\\] El proceso del muestreador de Gibbs es una caminata aleatoria a lo largo del espacio de parámetros. La caminata inicia en un punto arbitrario y en cada tiempo el siguiente paso depende únicamente de la posición actual. Por tanto el muestredor de Gibbs es un proceso cadena de Markov vía Monte Carlo. La diferencia entre Gibbs y Metrópolis radica en como se deciden los pasos. Muestreador Gibbs En cada punto de la caminata se selecciona uno de los componentes del vector de parámetros (típicamente se cicla en orden): Supongamos que se selecciona el parámetro \\(\\theta_k\\), entonces obtenemos un nuevo valor para este parámetro generando una simulación de la distribución condicional \\[p(\\theta_k,|\\theta_1,...,\\theta_{k-1},\\theta_{k+1},...,\\theta_p,x)\\] El nuevo valor \\(\\theta_k\\) junto con los valores que aun no cambian \\(\\theta_1,...,\\theta_{k-1},\\theta_{k+1},...,\\theta_p\\) constituyen la nueva posición en la caminata aleatoria. Seleccionamos una nueva componente (\\(\\theta_{k+1}\\)) y repetimos el proceso. El muestreador de Gibbs es útil cuando no podemos determinar de manera analítica la distribución conjunta y no se puede simular directamente de ella, pero si podemos determinar todas las distribuciones condicionales y simular de ellas. Ejemplificaremos el muestreador de Gibbs con el ejemplo de las proporciones, a pesar de no ser necesario en este caso. Comenzamos identificando las distribuciones condicionales posteriores para cada parámetro: \\[p(\\theta_1|\\theta_2,x) = p(\\theta_1,\\theta_2|x) / p(\\theta_2|x)\\] \\[= \\frac{p(\\theta_1,\\theta_2|x)} {\\int p(\\theta_1,\\theta_2|x) d\\theta_1}\\] Usando iniciales \\(beta(a_1, b_1)\\) y \\(beta(a_2,b_2)\\), obtenemos: \\[p(\\theta_1|\\theta_2,x) = \\frac{beta(\\theta_1|z_1 + a_1, N_1 - z_1 + b_1) beta(\\theta_2|z_2 + a_2, N_2 - z_2 + b_2)}{\\int beta(\\theta_1|z_1 + a_1, N_1 - z_1 + b_1) beta(\\theta_2|z_2 + a_2, N_2 - z_2 + b_2) d\\theta_1}\\] \\[= \\frac{beta(\\theta_1|z_1 + a_1, N_1 - z_1 + b_1) beta(\\theta_2|z_2 + a_2, N_2 - z_2 + b_2)}{beta(\\theta_2|z_2 + a_2, N_2 - z_2 + b_2)}\\] \\[=beta(\\theta_1|z_1 + a_1, N_1 - z_1 + b_1)\\] Debido a que la posterior es el producto de dos distribuciones Beta independientes es claro que \\(p(\\theta_1|\\theta_2,x)=p(\\theta_1|x)\\). Una vez que determinamos las distribuciones condicionales, simplemente hay que encontrar una manera de obtener muestras de estas, en R podemos usar la función \\(rbeta\\). pasos &lt;- 12000 camino &lt;- matrix(0, nrow = pasos, ncol = 2) # vector que guardará las simulaciones camino[1, 1] &lt;- 0.1 # valor inicial camino[1, 2] &lt;- 0.1 # Generamos la caminata aleatoria for (j in 2:pasos) { if (j %% 2) { camino[j, 1] &lt;- rbeta(1, z_1 + a_1, N_1 - z_1 + b_1) camino[j, 2] &lt;- camino[j - 1, 2] } else{ camino[j, 2] &lt;- rbeta(1, z_2 + a_2, N_2 - z_2 + b_2) camino[j, 1] &lt;- camino[j - 1, 1] } } caminata &lt;- data.frame(pasos = 1:pasos, theta_1 = camino[, 1], theta_2 = camino[, 2]) ggplot(caminata[1:2000, ], aes(x = theta_1, y = theta_2)) + geom_point(size = 0.8) + geom_path(alpha = 0.5) + scale_x_continuous(expression(theta[1]), limits = c(0, 1)) + scale_y_continuous(expression(theta[2]), limits = c(0, 1)) + coord_fixed() caminata_g &lt;- filter(caminata, pasos %% 2 == 0) %&gt;% gather(parametro, val, theta_1, theta_2) %&gt;% mutate(pasos = rep(1:6000, 2)) %&gt;% arrange(pasos) ggplot(caminata_g[1:2000, ], aes(x = pasos, y = val)) + geom_path(alpha = 0.3) + facet_wrap(~parametro, ncol = 1) + scale_y_continuous(&quot;&quot;, limits = c(0, 1)) Si comparamos los resultados del muestreador de Gibbs con los de Metrópolis notamos que las estimaciones son muy cercanas # Metropolis caminata_m %&gt;% filter(pasos &gt; 1000) %&gt;% # eliminamos el calentamiento group_by(parametro) %&gt;% summarise( media = mean(val), mediana = median(val), std = sd(val) ) #&gt; # A tibble: 2 x 4 #&gt; parametro media mediana std #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 theta_1 0.614 0.620 0.127 #&gt; 2 theta_2 0.378 0.370 0.134 # Gibbs caminata_g %&gt;% filter(pasos &gt; 1000) %&gt;% group_by(parametro) %&gt;% summarise( media = mean(val), mediana = median(val), std = sd(val) ) #&gt; # A tibble: 2 x 4 #&gt; parametro media mediana std #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 theta_1 0.614 0.621 0.129 #&gt; 2 theta_2 0.384 0.378 0.129 También podemos comparar los sesgos de las dos monedas, esta es una pregunta más interesante. caminata &lt;- caminata %&gt;% mutate(dif = theta_1 - theta_2) ggplot(caminata, aes(x = dif)) + geom_histogram(fill = &quot;gray&quot;) + geom_vline(xintercept = 0, alpha = 0.8, color = &quot;red&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. La principal ventaja del muestreador de Gibbs sobre el algoritmo de Metrópolis es que no hay necesidad de seleccionar una distribución propuesta y no hay que lidiar con lo ineficiente de rechazar valores. A cambio, debemos ser capaces de derivar las probabilidades condicionales de cada parámetro y de generar muestras de estas. Ejemplo: Normal Retomemos el caso de observaciones normales, supongamos que tengo una muestra \\(x_1,...,x_N\\) de observaciones independientes e identicamente distribuidas, con \\(x_i \\sim N(\\mu, \\sigma^2)\\), veremos el caso de media desconocida, varianza desconocida y de ambas desconocidas. Normal con media desconocida. Supongamos que \\(\\sigma^2\\) es conocida, por lo que nuestro parámetro de interés es únicamente \\(\\mu\\) entonces si describo mi conocimiento inicial de \\(\\mu\\) a través de una distribución normal: \\[\\mu \\sim N(m, \\tau^2)\\] resulta en una distribución posterior: \\[\\mu|x \\sim N\\bigg(\\frac{\\sigma^2}{\\sigma^2 + N\\tau^2}m + \\frac{N\\tau^2}{\\sigma^2 + N \\tau^2}\\bar{x}, \\frac{\\sigma^2 \\tau^2}{\\sigma^2 + N\\tau^2}\\bigg)\\] Normal con varianza desconocida. Supongamos que \\(\\mu\\) es conocida, por lo que nuestro parámetro de interés es únicamente \\(\\sigma^2\\). En este caso una distribución conveniente para describir nuestro conocimiento inicial es la distribución Gamma Inversa. La distribución Gamma Inversa es una distribución continua con dos parámetros y que toma valores en los positivos. Como su nombre lo indica, esta distribución corresponde al recírpoco de una variable cuya distribución es Gamma, recordemos que si \\(x\\sim Gamma(\\alpha, \\beta)\\) entonces: \\[p(x)=\\frac{1}{\\beta^{\\alpha}\\Gamma(\\alpha)}x^{\\alpha-1}e^{-x/\\beta}\\] donde \\(x&gt;0\\). Ahora si \\(y\\) es la variable aleatoria recírpoco de \\(x\\) entonces: \\[p(y)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}y^{-\\alpha - 1} exp{-\\beta/y}\\] con media \\[\\frac{\\beta}{\\alpha-1}\\] y varianza \\[\\frac{\\beta^2}{(\\alpha-1)^2(\\alpha-2)}.\\] Debido a la relación entre las distribuciones Gamma y Gamma Inversa, podemos utilizar la función rgamma de R para generar valores con distribución gamma inversa. # 1. simulamos valores porvenientes de una distribución gamma x_gamma &lt;- rgamma(2000, shape = 5, rate = 1) # 2. invertimos los valores simulados x_igamma &lt;- 1 / x_gamma # También podemos usar las funciones de MCMCpack library(MCMCpack) x_igamma &lt;- data.frame(x_igamma) ggplot(x_igamma, aes(x = x_igamma)) + geom_histogram(aes(y = ..density..), binwidth = 0.05, fill = &quot;gray&quot;) + stat_function(fun = dinvgamma, args = list(shape = 5, scale = 1), color = &quot;red&quot;) Volviendo al problema de inferir acerca del parámetros \\(\\sigma^2\\), si resumimos nuestro conocimiento inicial a través de una distribución Gamma Inversa tenemos \\[p(\\sigma^2)=\\frac{\\beta^\\alpha}{\\Gamma(\\alpha)}\\frac{1}{(\\sigma^2)^{\\alpha + 1}} e^{-\\beta/\\sigma^2}\\] la verosimiltud: \\[p(x|\\mu, \\sigma^2)=\\frac{1}{(2\\pi\\sigma^2)^{N/2}}exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{N}(x_j-\\mu)^2\\right)\\] y calculamos la posterior: \\[p(\\sigma^2) \\propto p(x|\\mu,\\sigma^2)p(\\sigma^2)\\] obtenemos que \\(\\sigma^2|x \\sim GI(N/2+\\alpha, \\beta + 1/2 \\sum(x_i - \\mu)^2)\\). Por tanto tenemos que la inicial Gamma con verosimilitud Normal es una familia conjugada. Ejemplo: Normal con media y varianza desconocidas Sea \\(\\theta=(\\mu, \\sigma^2)\\) especificamos la siguiente inicial para \\(\\theta\\): \\[p(\\theta) = N(\\mu|m, \\tau^2)\\cdot IG(\\sigma^2|\\alpha, \\beta)\\] suponemos hiperparámetros \\(m,\\tau^2, \\alpha, \\beta\\) conocidos. Entonces, la distribución posterior es: \\[ p(\\theta|x) \\propto p(x|\\theta) p(\\theta)\\] \\[= \\frac{1}{(\\sigma^2)^{N/2}} exp\\bigg(-\\frac{1}{2\\sigma^2}\\sum_{i=1}^N (x_i-\\mu)^2 \\bigg) exp\\bigg(-\\frac{1}{2\\tau^2}(\\mu-m)^2)\\bigg) \\frac{1}{(\\sigma^2)^{\\alpha +1}} exp\\bigg(-\\frac{\\beta}{\\sigma^2}\\bigg)\\] en esta última distribución no reconocemos el núcleo de niguna distribución conocida (existe una distribución normal-gamma inversa) pero si nos concenteramos únicamente en los términos que involucran a \\(\\mu\\) tenemos: \\[exp\\left(-\\frac{1}{2}\\left( \\mu^2 \\left( \\frac{N}{\\sigma^2} + \\frac{1}{\\tau^2} \\right) - 2\\mu\\left(\\frac{\\sum_{i= 1}^n x_i}{\\sigma^2} + \\frac{m}{\\tau^2}\\right) \\right)\\right)\\] esta expresión depende de \\(\\mu\\) y \\(\\sigma^2\\), sin embargo condicional a \\(\\sigma^2\\) observamos el núcleo de una distribución normal, \\[\\mu|\\sigma^2,x \\sim N\\left(\\frac{n\\tau^2}{n\\tau^2 + \\sigma^2}\\bar{x} + \\frac{\\sigma^2}{N\\tau^2 + \\sigma^2}m, \\frac{\\tau^2\\sigma^2}{n\\tau^2 + \\sigma^2} \\right)\\] Si nos fijamos únicamente en los tárminos que involucran a \\(\\sigma^2\\) tenemos: \\[\\frac{1}{(\\sigma^2)^{N/2+\\alpha+1}}exp\\left(- \\frac{1}{\\sigma^2} \\left(\\sum_{i=1}^N \\frac{(x_i-\\mu)^2}{2} + \\beta \\right) \\right)\\] y tenemos \\[\\sigma^2|\\mu,x \\sim GI\\left(\\frac{N}{2} + \\alpha, \\sum_{i=1}^n \\frac{(x_i-\\mu)^2}{2} + \\beta \\right)\\] Obtenemos así las densidades condicionales completas \\(p(\\mu|\\sigma^2, x)\\) y \\(p(\\sigma^2|\\mu, x)\\) cuyas distribuciones conocemos y de las cuales podemos simular. Implementaremos un muestreador de Gibbs. Comenzamos definiendo las distrbuciones iniciales: \\(\\mu \\sim N(1.5, 16)\\), esto es \\(m = 1.5\\) y \\(\\tau^2 = 16\\). \\(\\sigma^2 \\sim GI(3, 3)\\), esto es \\(\\alpha = \\beta = 3\\). Ahora supongamos que observamos \\(20\\) realizaciones provenientes de la distribución de interés: N &lt;- 50 # Observamos 20 realizaciones set.seed(122) x &lt;- rnorm(N, 2, 2) x #&gt; [1] 4.62140176 0.24829384 2.39904749 2.93190885 -1.60411353 4.89748691 #&gt; [7] 2.59770769 2.72362329 -0.01388084 1.48600171 1.73574244 0.31673052 #&gt; [13] 2.54850449 -2.92518071 -2.30679198 4.31835150 3.37948021 3.76050265 #&gt; [19] 0.11325957 3.43814572 0.92434912 0.95470268 -0.10584378 2.20303449 #&gt; [25] 5.72700211 1.96078181 -0.15661507 2.34520855 3.06610819 5.90452895 #&gt; [31] 4.82270939 3.20273052 0.17200480 5.16085186 1.06016874 5.20367778 #&gt; [37] 2.74547989 2.06775571 2.20820677 -2.03674850 0.68398061 -1.21700489 #&gt; [43] -0.68818260 1.60665220 4.75132535 2.34663000 1.12144484 -1.19064581 #&gt; [49] 0.51144488 5.63041621 Escribimos el muestreador de Gibbs. m &lt;- 1.5; tau2 &lt;- 16; alpha &lt;- 3; beta &lt;- 3 # parámetros de iniciales pasos &lt;- 20000 camino &lt;- matrix(0, nrow = pasos + 1, ncol = 2) # vector guardará las simulaciones camino[1, 1] &lt;- 0 # valor inicial media # Generamos la caminata aleatoria for (j in 2:(pasos + 1)) { # sigma^2 mu &lt;- camino[j - 1, 1] a &lt;- N / 2 + alpha b &lt;- sum((x - mu) ^ 2) / 2 + beta camino[j - 1, 2] &lt;- 1/rgamma(1, shape = a, rate = b) # Actualizar sigma2 # mu sigma2 &lt;- camino[j - 1, 2] media &lt;- (N * tau2 * mean(x) + sigma2 * m) / (N * tau2 + sigma2) var &lt;- sigma2 * tau2 / (N * tau2 + sigma2) camino[j, 1] &lt;- rnorm(1, media, sd = sqrt(var)) # actualizar mu } caminata &lt;- data.frame(pasos = 1:pasos, mu = camino[1:pasos, 1], sigma2 = camino[1:pasos, 2]) caminata_g &lt;- caminata %&gt;% gather(parametro, val, mu, sigma2) %&gt;% arrange(pasos) ggplot(filter(caminata_g, pasos &gt; 15000), aes(x = pasos, y = val)) + geom_path(alpha = 0.3) + facet_wrap(~parametro, ncol = 1, scales = &quot;free&quot;) + scale_y_continuous(&quot;&quot;) En la gráfica de arriba podemos ver que se actualiza un parámetro a la vez, por lo que cada paso de la caminata aleatoria es paralelo al eje de uno de los parámetros. Una alternativa es conservar únicamente ciclos completos de la caminata, esto es lo que hace JAGS y otros programas que implementan Gibbs, sin embargo ambas cadenas (cadanas completas y conservando únicamente ciclos completos) convergen a la misma distribución posterior. ggplot(filter(caminata_g, pasos &gt; 5000), aes(x = val)) + geom_histogram(fill = &quot;gray&quot;) + facet_wrap(~parametro, ncol = 1, scales = &quot;free&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. Algunos resúmenes de la posterior: caminata_g %&gt;% filter(pasos &gt; 1000) %&gt;% # eliminamos la etapa de calentamiento group_by(parametro) %&gt;% summarise( mean(val), sd(val), median(val) ) #&gt; # A tibble: 2 x 4 #&gt; parametro `mean(val)` `sd(val)` `median(val)` #&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 mu 1.91 0.305 1.91 #&gt; 2 sigma2 4.74 0.933 4.62 Predicción. Para predecir el valor de una realización futura \\(y\\) recordemos que: \\[p(y) =\\int p(y|\\theta)p(\\theta|x)d\\theta\\] Por tanto podemos aproximar la distribución predictiva posterior como: caminata_f &lt;- filter(caminata, pasos &gt; 5000) caminata_f$y_sims &lt;- rnorm(1:nrow(caminata_f), caminata_f$mu, caminata_f$sigma2) ggplot(caminata_f, aes(x = y_sims)) + geom_histogram(fill = &quot;gray&quot;) + geom_vline(aes(xintercept = mean(y_sims)), color = &quot;red&quot;) #&gt; `stat_bin()` using `bins = 30`. Pick better value with `binwidth`. ¿Cuál es la probabilidad de que una observación futura sea mayor a \\(8\\)? En estadística bayesiana es común parametrizar la distribución Normal en términos de precisión (el inverso de la varianza). Si parametrizamos de esta manera \\(\\nu = 1/\\sigma^2\\) podemos repetir el proceso anterior con la diferencia de utilizar la distribución Gamma en lugar de Gamma inversa. Conclusiones y observaciones Metrópolis y Gibbs Una generalización del algoritmo de Metrópolis es Metrópolis-Hastings. El algoritmo de Metropolis es como sigue: Generamos un punto inicial tal que \\(p(\\theta)&gt;0\\). Para \\(t = 1,2,...\\) Se propone un nuevo valor \\(\\theta_{propuesto}\\) con una distribución propuesta \\(g(\\theta_{propuesta}|\\theta_{actual})\\) es común que \\(g(\\theta_{propuesta}|\\theta_{actual})\\) sea una normal centrada en \\(\\theta_{actual}\\). Calculamos \\[p_{mover}=min\\bigg( \\frac{p(\\theta_{propuesta})}{p(\\theta_{actual})},1\\bigg)\\] y aceptamos \\[\\theta_{propuesta}\\] con probabilidad \\(p_{mover}\\). Es así que el algorito requiere que podamos calcular el cociente en \\(p_{mover}\\) para todo \\(\\theta_{actual}\\) y \\(\\theta_{propuesta}\\), así como simular de la distribución propuesta \\(g(\\theta_{propuesta}|\\theta_{actual})\\), adicionalmente debemos poder generar valores uniformes para decidir si aceptar/rechazar. En el caso de Metropolis un requerimiento adicional es que la distribución propuesta \\(g(\\theta_{a}|\\theta_b)\\) debe ser simétrica, es decir \\(g(\\theta_{a}|\\theta_b) = g(\\theta_{b}|\\theta_a)\\) para todo \\(\\theta_{a}\\), \\(\\theta_{b}\\). Metropolis-Hastings generaliza Metropolis, eliminando la restricción de simetría en la distribución propuesta \\(g(\\theta_{a}|\\theta_b)\\), sin embargo para corregir por esta asimetría debemos calcular \\(p_{mover}\\) como sigue: \\[p_{mover}=min\\bigg\\{ \\frac{p(\\theta_{propuesta})/g(\\theta_{propuesta}|\\theta_{actual})}{p(\\theta_{actual})/g(\\theta_{actual}|\\theta_{propuesta})},1\\bigg\\}\\] La generalización de Metrópolis-Hastings puede resultar en algoritmos más veloces. Se puede ver Gibbs como una generalización de Metropolis-Hastings, cuando estamos actualizando un componente de los parámetros, la distribución propuesta es la distribución posterior para ese parámetro, por tanto siempre es aceptado. Comparado con Metrópolis Gibbs tiene la ventaja de que no se necesita afinar los parámetros de una distribución propuesta (o seleccionar siquiera una distribución propuesta). Además que no hay pérdida de simulaciones debido a rechazo. Por su parte, la desventaja debemos conocer las distribuciones condicionales y poder simular de ellas. En el caso de modelos complicados se utilizan combinaciones de Gibbs y Metropolis. Cuando se consideran estos dos algoritmos Gibbs es un método más simple y es la primera opción para modelos condicionalmente conjugados. Sí solo podemos simular de un subconjunto de las distribuciones condicionales posteriores, entonces podemos usar Gibbs siempre que se pueda y Metropolis unidimensional para el resto, o de manera más general separamos en bloques, un bloque se actualiza con Gibbs y otro con Metrópolis. El algoritmo de Gibbs puede atorarse cuando hay correlación alta entre los parámetros, reparametrizar puede ayudar, o se pueden usar otros algoritmos que veremos más adelante. JAGS (Just Another Gibbs Sampler), WinBUGS y OpenBUGS son programas que implementan métodos MCMC para generar simulaciones de distribuciones posteriores. Los paquetes rjags y R2jags permiten ajustar modelos en JAGS desde R. Es muy fácil utilizar estos programas pues uno simplemente debe especificar las distribuciones iniciales, la verosimilitud y los datos observados. Para aprender a usar JAGS se puede revisar la sección correspondiente en las notas de 2018, ahora nos concentraremos en el uso de Stan. "],
["hmc-y-stan.html", "10.8 HMC y Stan", " 10.8 HMC y Stan It appears to be quite a general principle that, whenever there is a randomized way of doinf something, then there is a nonrandomized way that delivers better performance but requires more thought. -E.T. Jaynes Stan es un programa para generar muestras de una distribución posterior de los parámetros de un modelo, el nombre del programa hace referencia a Stanislaw Ulam (1904-1984) que fue pionero en los métodos de Monte Carlo. A diferencia de JAGS y BUGS, los pasos de la cadena de Markov se generan con un método llamado Monte Carlo Hamiltoniano (HMC). HMC es computacionalmente más costoso que Metropolis o Gibbs, sin embargo, sus propuestas suelen ser más eficientes, y por consiguiente no necesita muestras tan grandes. En particular cuando se ajustan modelos grandes y complejos (por ejemplo, con variables con correlación alta) HMC supera a otros. Muestreo HMC El uso de HMC en estadística es reciente, sin embargo, gracias a Stan se ha expandido rápidamente tanto en academia como industria. Desafortunadamente, la teoría de HMC está desarrollada en términos de geometría diferencial, lo que hace que su construcción formal requiera de matemáticas avanzadas. En estas notas se presentan las ideas detrás de HMC siguiendo Kruschke (2015), una referencia con mayor detalle es Betancourt (2017) y para el uso de Stan vale la pena tener siempre a la mano el manual. Stan genera muestras de la posterior usando una variación del algoritmo de Metrópolis. Recordemos como funciona el algoritmo de Metrópolis que vimos en clase: Tenemos una distribución objetivo \\(p(\\theta)\\) de la cual buscamos generar muestras. Debemos ser capaces de calcular el valor de \\(p(\\theta)\\) para cualquier valor candidato \\(\\theta\\). La distribución objetivo \\(p(\\theta)\\) no tiene que estar normalizada, típicamente \\(p(\\theta)\\) es la distribución posterior de \\(\\theta\\) no normalizada, es decir, es el producto de la verosimilitud y la inicial. La muestra de la distribución objetivo se genera mediante una caminata aleatoria a través del espacio de parámetros. La caminata inicia en un lugar arbitrario (definido por el usuario). El punto inicial debe ser tal que \\(p(\\theta)&gt;0\\). La caminata avanza en cada tiempo proponiendo un movimiento a una nueva posición y después decidiendo si se acepta o no el valor propuesto. Las distribuciones propuesta pueden tener muchas formas, el objetivo es que la distribución propuesta explore el espacio de parámetros de manera eficiente. Una vez que tenemos un valor propuesto decidimos si aceptar calculando: \\[p_{mover}=min\\bigg( \\frac{p(\\theta_{propuesta})}{p(\\theta_{actual})},1\\bigg)\\] Y al final obtenemos valores representativos de la distribución objetivo \\(\\{\\theta_1,...,\\theta_n\\}\\). Notemos que en la versión de Metrópolis que estudiamos, la forma de la distribución propuesta está centrada de manera simétrica en la posición actual. Es decir, en un espacio paramétrico multidimensional, la distribución propuesta podría ser una Normal multivariada, con la matriz de varianzas y covarianzas seleccionada para mejorar la eficiencia en la aplicación particular. La normal multivariada siempre esta centrada en la posición actual y siempre tiene la misma forma, sin importar en que sección del espacio paramétrico estemos ubicados. Esto puede llevar a ineficiencias, por ejemplo, si nos ubicamos en las colas de la distribución posterior el paso propuesto con la misma probabilidad nos aleja o acerca de la moda de la posterior. Otro ejemplo es si la distriución posterior se curva a lo largo del espacio paramétrico, una distribución propuesta (de forma fija) puede ser eficiente para una parte de la posterior y poco eficiente para otra parte de la misma. Por su parte HMC, usa una distribución propuesta que cambia dependiendo de la posición actual. HMC utiliza el gradiente de la posterior y envuelve la distribución propuesta hacia el gradiente, como se ve en la siguiente figura. knitr::include_graphics(&quot;img/hmc_proposals.png&quot;) HMC genera un movimiento propuesta de manera análoga a rodar una canica en la distribución posterior volteada (también conocida como potencial). El potencial es el negativo del logaritmo de la densidad posterior, en las regiones donde la posterior es alta el potencial es bajo, y en las la regiones donde la posterior es plana el potencial es alto. La propuesta se genera dando un golpecito la canica en una dirección aleatoria y dejándola rodar cierto tiempo. En el caso del ejemplo de un solo parámetro la dirección del golpecito inicial es hacia la izquierda o derecha, y la magnitud se genera de manera aleatoria muestreando de una distrubución Gaussiana de media cero. El golpecito impone un momento inicial a la canica, y al terminar el tiempo se propone al algoritmo de Metrópilis la posición final de la canica. Es fácil imaginar que la posición propuesta tenderá a estar en regiones de mayor probabilidad posterior. La última fila de la figura de arriba nos muestra un histograma de los pasos propuestos, notemos que no está centrado en la posición actual sino que hay una inclinación hacia la moda de la posterior. Para distribuciones posteriores de dimensión alta con valles diagonales o curveados, la dinámica de HMC generará valores propuesta mucho más prometedores que una distribución propuesta simétrica (como la versión de Metrópolis que implementamos) y mejores que un muestreador de Gibbs que puede atorarse en paredes diagonales. Es así que para pasar del algoritmo de Metrópolis que estudiamos a HMC se modifica la probabilidad de aceptación para tener en cuenta no sola la denisdad posterior relativa, sino también el momento (denotado por \\(\\phi\\)) en las posiciones actual y propuesta. \\[p_{aceptar}=min\\bigg\\{\\frac{p(\\theta_{propuesta}|x)p(\\phi_{propuesta})}{p(\\theta_{actual}|x)p(\\phi_{actual})}, 1 \\bigg\\}\\] En un sistema continuo ideal la suma de la energía potencial y cinética (que corresponden a \\(-log(p(\\theta|x))\\) y \\(-log(p(\\phi))\\)) es constante y por tanto aceptaríamos todas las propuestas. Sin embargo, en la práctica las dinámicas continuas se dicretizan a intervalos en el tiempo y los cálculos son solo aproximados conllevando a que se rechacen algunas propuestas. Si los pasos discretizados son pequeños entonces la aproximación será buena pero se necesitarán más pasos para alejarse de una posición dada, y lo contrario si los pasos son muy grandes. Por tanto se debe ajustar el tamaño del paso (\\(\\varepsilon\\)) y el número de pasos. La duración de la trayectoria es el producto del tamaño del paso y el número de pasos. Es usual buscar una tasa de aceptación alrededor del \\(65\\%\\), moviendo el tamaño de epsilon y compensando con el número de pasos. Es así que el tamaño del paso controla la suavidad de la trayectoria. También es importante ajustar el número de pasos (es decir la duración del movimiento) pues no queremos alejarnos demasiado o rodar de regreso a la posición actual. La siguiente figura muestra varias trayectorias y notamos que muchas rebotan al lugar inicial. knitr::include_graphics(&quot;img/hmc_proposals_2.png&quot;) Para evitar las ineficiencias de dar vueltas en U, Stan incorpora un algoritmo que generaliza la nación de vueltas en U a espacios de dimensión alta, y así estima cuando parar las trayectorias antes de que reboten hacia la posición inical. El algoritmo se llama No U-turn Sampler (NUTS). Adicional a ajustar el tamaño del paso y número de pasos debemos ajustar la desviación estándar del momento inicial. Si la desviación estandar del momento es grande también lo será la desviación estándar de las propuestas. Nuevamente, lo más eficiente será una desviación estándar ni muy grande ni muy chica. En Stan la desviación estándar del momento se establece de manera adaptativa para que coincida con la desviación estándar de la posterior. knitr::include_graphics(&quot;img/hmc_proposals_3.png&quot;) Por último, para calcular la trayectoria propuesta debemos ser capaces de calcular el gradiente de la densidad posterior en cualquier valor de los parámetros. Para realizar esto de manera eficiente en espacios de dimensión alta se debe derivar analíticamente, en el caso de modelos complejos las fórmulas se derivan usando algoritmos avanzados. El paper A Conceptual Introduction to Hamiltonian Monte Carlo de Michael Betancourt explica conceptos e intuición detrás de HMC, y el porqué es apropiado en problemas de alta dimensión. Stan Para instalar Stan sigue las instrucciones de aquí. Nosotros usaremos el paquete rstan, Guo, Gabry, and Goodrich (2019). library(rstan) # opcional para correr en paralelo rstan_options(auto_write = TRUE) options(mc.cores = parallel::detectCores()) En Stan los programas se organizan mediante secuencias de bloques, cada uno de estos bloques inicia con declaración de variables y después le siguen enunciados. El siguiente esqueleto ejemplifica los bloques disponibles, sin embargo, veremos que no siempre se utilizan todos los bloques. functions { // ... function declarations and definitions ... } data { // ... declarations ... } transformed data { // ... declarations ... statements ... } parameters { // ... declarations ... } transformed parameters { // ... declarations ... statements ... } model { // ... declarations ... statements ... } generated quantities { // ... declarations ... statements ... } Comenzamos con el ejemplo sencillo de estimar el sesgo de una moneda. El primer paso es especificar el modelo en el lenguaje de Stan. Lo podemos guardar en un archivo de texto separado o simplemente asignarlo a una variable en R. modelo_bernoulli.stan &lt;- &#39; data { int&lt;lower=0&gt; N; int y[N]; } parameters { real&lt;lower=0,upper=1&gt; theta; } model { theta ~ beta(1,1) ; y ~ bernoulli(theta) ; } &#39; # notemos que los modelos de Stan deben terminar con una línea en blanco cat(modelo_bernoulli.stan, file = &quot;src/stan_files/modelo_bernoulli.stan&quot;) Notemos que Stan permite operaciones vectorizadas, por lo que podemos poner: y ~ bernoulli(theta) ; sin necesidad de hacer el ciclo for: for ( i in 1:N ) { y[i] ~ dbern(theta) } Una vez que especificamos el modelo lo siguiente es traducir el modelo a código de C++ y compilarlo. Para esto usamos la función stan_model() que puede recibir el archivo de texto con la especificación del modelo. stan_cpp &lt;- stan_model(&quot;src/stan_files/modelo_bernoulli.stan&quot;) O el objeto de R, stan_cpp &lt;- stan_model(model_code = modelo_bernoulli.stan) El paso de compilación puede tardar, pues Stan está calculando los gradientes para las dinámicas Hamiltonianas. Ahora cargamos los datos y usamos la función sampling para obtener las simulaciones de la distribución posterior. N = 50 ; z = 10 ; y = c(rep(1, z), rep(0, N - z)) set.seed(8979) data_list &lt;- list(y = y, N = N ) stan_fit &lt;- sampling(object = stan_cpp, data = data_list, chains = 3 , iter = 1000 , warmup = 200, thin = 1 ) La función summary() nos da resúmenes de la distribución posterior de los parámetros combinando las simulaciones de todas las cadenas y por cadena. stan_fit #&gt; Inference for Stan model: modelo_bernoulli. #&gt; 3 chains, each with iter=1000; warmup=200; thin=1; #&gt; post-warmup draws per chain=800, total post-warmup draws=2400. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; theta 0.21 0.00 0.06 0.11 0.17 0.21 0.25 0.33 864 1 #&gt; lp__ -27.35 0.02 0.75 -29.49 -27.52 -27.07 -26.88 -26.83 915 1 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Wed Nov 27 08:03:06 2019. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). Una alternativa a usar las funciones stan_model() y sampling(), es compilar y correr las cadenas de manera simultanea con la función stan(). stan_fit_2 &lt;- stan(file = &#39;src/stan_files/modelo_bernoulli.stan&#39;, data = data_list, chains = 3, iter = 1000 , warmup = 200, thin = 1) Podemos graficar las cadenas con la función traceplot(), esta gráfica nos permire inspecconar la conducta del muestreador y evaluar si las cadenas se han mezclado y olvidado el valor inicial. traceplot(stan_fit) Ejemplo normal Recordemos ahora el ejemplo normal con media y varianza desconocidas, para este problema escribimos un muestreador de Gibbs, y ahora veremos como lo haríamos con Stan y compararemos los resultados. Escribimos el modelo en Stan: modelo_normal.stan &lt;- &#39; data { int&lt;lower=0&gt; N; vector[N] y; } parameters { real mu; real&lt;lower=0&gt; sigma2; } model { y ~ normal(mu, sqrt(sigma2)); mu ~ normal(1.5, 4); sigma2 ~ inv_gamma(3, 3); } &#39; cat(modelo_normal.stan, file = &quot;src/stan_files/modelo_normal.stan&quot;) Especificamos la verosimilitud normal con media \\(\\mu\\) y varianza \\(\\sigma^2\\), notemos que en Stan los parámetros son la media y desviación estándar. y ~ normal(mu, sqrt(sigma2)); Y al igual que en el ejemplo del muestreador de Gibbs usaremos iniciales Normal para \\(\\mu\\) y Gamma inversa para \\(\\sigma^2\\). mu ~ normal(1.5, 4); sigma2 ~ inv_gamma(3, 3); Pasamos al paso de compilación: stan_norm_cpp &lt;- stan_model(&quot;src/stan_files/modelo_normal.stan&quot;) El modelo ya esta especificado, pero aun falta indicar los datos observados. N &lt;- 50 # Observamos 20 realizaciones set.seed(122) y &lt;- rnorm(N, 2, 2) data_list &lt;- list(y = y, N = N ) norm_fit &lt;- sampling(object = stan_norm_cpp, data = data_list, chains = 3 , iter = 1000 , warmup = 500) Y podemos ver intervalos de la distribución posterior de los parámetros. norm_fit #&gt; Inference for Stan model: modelo_normal. #&gt; 3 chains, each with iter=1000; warmup=500; thin=1; #&gt; post-warmup draws per chain=500, total post-warmup draws=1500. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; mu 1.91 0.01 0.31 1.30 1.70 1.92 2.12 2.53 1071 1.00 #&gt; sigma2 4.72 0.03 0.92 3.26 4.07 4.62 5.26 6.81 981 1.00 #&gt; lp__ -71.04 0.05 0.99 -73.64 -71.39 -70.76 -70.33 -70.07 455 1.01 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Wed Nov 27 08:03:54 2019. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). Podemos realizar graficas de las cadenas con más detalle (y con facilidad), usando el paquete bayesplot. library(bayesplot) #&gt; Error in library(bayesplot): there is no package called &#39;bayesplot&#39; norm_posterior_inc_warmup &lt;- extract(norm_fit, inc_warmup = TRUE, permuted = FALSE) color_scheme_set(&quot;viridis&quot;) #&gt; Error in color_scheme_set(&quot;viridis&quot;): could not find function &quot;color_scheme_set&quot; mcmc_trace(norm_posterior_inc_warmup, pars = c(&quot;mu&quot;, &quot;sigma2&quot;), n_warmup = 300, facet_args = list(nrow = 2, labeller = label_parsed)) + facet_text(size = 15) #&gt; Error in mcmc_trace(norm_posterior_inc_warmup, pars = c(&quot;mu&quot;, &quot;sigma2&quot;), : could not find function &quot;mcmc_trace&quot; Y podemos graficar la distribución posterior de los parámetros, con intervalos de probabilidad. norm_posterior &lt;- as.array(norm_fit) mcmc_areas(norm_posterior, pars = c(&quot;mu&quot;, &quot;sigma2&quot;), prob = 0.8, point_est = &quot;median&quot;, adjust = 1.4) #&gt; Error in mcmc_areas(norm_posterior, pars = c(&quot;mu&quot;, &quot;sigma2&quot;), prob = 0.8, : could not find function &quot;mcmc_areas&quot; Realiza un histograma de la distribución predictiva posterior. Construye un intervalo de \\(95\\%\\) de probabilidad para una predicción. Tip: usa la función extract() para extraer las simulaciones del objeto stanfit. Referencias "],
["diagnósticos-generales.html", "10.9 Diagnósticos generales", " 10.9 Diagnósticos generales Cuando generamos una muestra de la distribución posterior usando MCMC, sin importar el método (Metrópolis, Gibbs, HMC), buscamos que: Los valores simulados sean representativos de la distribución posterior, esto implica que no deben estar influenciados por el valor inicial (arbitrario) y deben explorar todo el rango de la posterior. Debemos tener suficientes simulaciones de tal manera que las estimaciones sean precisas y estables. Queremos tener un método eficiente para generar las simulaciones. En la práctica intentamos cumplir lo más posible estos objetivos, pues aunque en principio los métodos MCMC garantizan que cadena infinitamente largas lograran una representación perfecta, siempre debemos tener un criterio para cortar la cadena y evaluar la calidad de las simulaciones. Representatividad Para determinar la convergencia de la cadena es conveniente realizar más de una cadena, buscamos ver si realmente se ha olvidado el estado inicial y revisar que algunas cadenas no hayan quedado atoradas en regiones inusuales del espacio de parámetros. set.seed(337687) norm_fit_1 &lt;- sampling(object = stan_norm_cpp, data = data_list, chains = 3 , iter = 40 , warmup = 20) #&gt; Warning: The largest R-hat is 1.14, indicating chains have not mixed. #&gt; Running the chains for more iterations may help. See #&gt; http://mc-stan.org/misc/warnings.html#r-hat #&gt; Warning: Bulk Effective Samples Size (ESS) is too low, indicating posterior means and medians may be unreliable. #&gt; Running the chains for more iterations may help. See #&gt; http://mc-stan.org/misc/warnings.html#bulk-ess #&gt; Warning: Tail Effective Samples Size (ESS) is too low, indicating posterior variances and tail quantiles may be unreliable. #&gt; Running the chains for more iterations may help. See #&gt; http://mc-stan.org/misc/warnings.html#tail-ess norm_posterior_inc_warmup &lt;- extract(norm_fit_1, inc_warmup = TRUE, permuted = FALSE) mcmc_trace(norm_posterior_inc_warmup, pars = c(&quot;mu&quot;, &quot;sigma2&quot;), n_warmup = 20, facet_args = list(nrow = 2, labeller = label_parsed)) + facet_text(size = 15) #&gt; Error in mcmc_trace(norm_posterior_inc_warmup, pars = c(&quot;mu&quot;, &quot;sigma2&quot;), : could not find function &quot;mcmc_trace&quot; La gráfica anterior nos puede ayudar a determinar si elegimos un periodo de calentamiento adecuado o si alguna cadena está alejada del resto. Además de realizar gráficas podemos usar la medida de convergencia \\(\\hat{R}\\) que la función rstan() calcula por omisión: norm_fit_1 #&gt; Inference for Stan model: modelo_normal. #&gt; 3 chains, each with iter=40; warmup=20; thin=1; #&gt; post-warmup draws per chain=20, total post-warmup draws=60. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; mu 1.84 0.05 0.31 1.18 1.73 1.88 2.00 2.41 40 1.04 #&gt; sigma2 4.76 0.11 0.89 3.24 4.06 4.71 5.23 6.56 61 0.97 #&gt; lp__ -71.06 0.20 1.24 -74.81 -71.56 -70.47 -70.30 -70.06 38 1.07 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Wed Nov 27 08:03:56 2019. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). La medida \\(\\hat{R}\\) se conoce como el factor de reducción potencial de escala o diagnóstico de convergencia de Gelman-Rubin, esta es una estimación de la posible reducción en la longitud de un intervalo de confianza si las simulaciones continuran infinitamente. \\(\\hat{R}\\) es aproximadamente la raíz cuadrada de la varianza de todas las cadenas juntas dividida entre la varianza dentro de cada cadena. Si \\(\\hat{R}\\) es mucho mayor a 1 esto indica que las cadenas no se han mezclado bien. Una regla usual es iterar hasta alcanzar un valor \\(\\hat{R} \\leq 1.1\\) para todos los parámetros. \\[\\hat{R} = \\sqrt{\\frac{\\hat{d}+3}{\\hat{d}+1}\\frac{\\hat{V}}{W}}\\] donde \\(B\\) es la varianza entre las cadenas, \\(W\\) es la varianza dentro de las cadenas \\[B = \\frac{N}{M-1}\\sum_m (\\hat{\\theta_m} - \\hat{\\theta})^2\\] \\[W = \\frac{1}{M}\\sum_m \\hat{\\sigma_m}^2\\] Y \\(\\hat{V}\\) es una estimación del varianza de posteriro de \\(\\theta\\): \\[\\hat{V} = \\frac{N-1}{N}W + \\frac{M+1}{MN}B\\] Precisión Una vez que tenemos una muestra representativa de la distribución posterior, nuestro objetivo es asegurarnos de que la muestra es lo suficientemente grande para producir estimaciones estables y precisas de la distribución. Para ello usaremos otra salida de la función jags: n.eff que es el tamaño efectivo de muestra, si las simulaciones fueran independientes n.eff sería el número total de simulaciones; sin embargo, las simulaciones de MCMC suelen estar correlacionadas, el tamaño efectivo nos dice que tamaño de muestra de observaciones independientes nos daría la misma información que las simulaciones de la cadena. \\[NEM = \\frac{N}{1+2\\sum_{k=1}^\\infty ACF(k)} \\] Usualmente nos gustaría obtener un tamaño efectivo de al menos \\(100\\). norm_fit #&gt; Inference for Stan model: modelo_normal. #&gt; 3 chains, each with iter=1000; warmup=500; thin=1; #&gt; post-warmup draws per chain=500, total post-warmup draws=1500. #&gt; #&gt; mean se_mean sd 2.5% 25% 50% 75% 97.5% n_eff Rhat #&gt; mu 1.91 0.01 0.31 1.30 1.70 1.92 2.12 2.53 1071 1.00 #&gt; sigma2 4.72 0.03 0.92 3.26 4.07 4.62 5.26 6.81 981 1.00 #&gt; lp__ -71.04 0.05 0.99 -73.64 -71.39 -70.76 -70.33 -70.07 455 1.01 #&gt; #&gt; Samples were drawn using NUTS(diag_e) at Wed Nov 27 08:03:54 2019. #&gt; For each parameter, n_eff is a crude measure of effective sample size, #&gt; and Rhat is the potential scale reduction factor on split chains (at #&gt; convergence, Rhat=1). Eficiencia Hay varias maneras para mejorar la eficiencia de un proceso MCMC: Paralelizar, no disminuimos el número de pasos en las simulaciones pero podemos disminuir el tiempo que tarda en correr. Cambiar la parametrización del modelo o transformar los datos. Adelgazar la muestra: esto nos puede ayudar a disminuir el uso de memoria, consiste en guardar únicamente los \\(k\\)-ésimos pasos de la cadena y resulta en cadenas con menos autocorrelación (en el caso de la función sampling() el adelgazamiento está controlado por el parámetro thin). Recomendaciones generales Gelman and Hill (2007) recomienda los siguientes pasos cuando uno esta simulando de la posterior: Cuando definimos un modelo por primera vez establecemos un valor bajo para iter. La razón es que la mayor parte de las veces los modelos no funcionan a la primera por lo que sería pérdida de tiempo dejarlo correr mucho tiempo antes de descubrir el problema. Si las simulaciones no han alcanzado convergencia aumentamos las iteraciones a \\(500\\) ó \\(1000\\) de tal forma que las corridas tarden segundos o unos cuantos minutos. Si tarda más que unos cuantos minutos (para problemas del tamaño que veremos en la clase) y aún así no alcanza convergencia entonces juega un poco con el modelo (por ejemplo intenta transformaciones lineales), para JAGS Gelman sugiere más técnicas para acelerar la convergencia en el capitulo \\(19\\) del libro Data Analysis Using Regression and Multilevel/Hierarchical models. En el caso de Stan veremos ejemplos de reparametrización, y se puede leer más en la guía. Otra técnica conveniente cuando se trabaja con bases de datos grandes (sobretodo para la parte exploratoria) es trabajar con un subconjunto de los datos, quizá la mitad o una quinta parte. 10.9.1 Iniciales Las siguientes recomendaciones de iniciales son de los desarrolladores de Stan, puedes leer más en https://github.com/stan-dev/stan/wiki/Prior-Choice-Recommendations. Podemos clasificar las iniciales en 5 niveles de acuerdo a que tan informativas son: Iniciales planas: usualmente impropias. Muy vagas pero propias: \\(N(0, (1e6) ^ 2)\\). Iniciales genéricas muy debilmente informativas: \\(Normal(0, 10^2)\\). Iniciales genéricas debilmente informativas: \\(Normal(0, 1)\\). Iniciales específicas e informativas: \\(Normal(0.4, 0.2^2)\\) Cuando se usa Stan tenemos los siguientes principios generales: En Stan no importa usar conjugadas (cuando se usan muestreadores de Gibbs puede ser conveniente usar conjugadas, aquí no). No interesan: invarianza, Jeffreys o entropía. Las iniciales debilmente informativas deben contener suficiente información para regularizar: las iniciales deben dejar fuera valores de los parámetros que no sean razonables, pero no dejar fuera valores que podrían tener sentido. Se prefieren las iniciales debilmente informativas sobre las informativas, esto es porque las consecuencias de perder precisión por elegir una inicial muy débil (comparado con la verdadera distribución poblacional de los parámetros o comparado con conocimiento experto) es menos importante que la ganancia en robustez que se deriva de incluir parte del espacio paramétrico que pueda ser relevante. Cuando se usen distribuciones iniciales informativas se debe ser explícito en porque se tomaron las decisiones. No utilices iniciales uniformes, o en general no restrinjas el espacio paramétrico, a menos que las fronteras representen restricciones verdaderas (por ejemplo parámetros de escala restringidos a los positivos, o correlaciones entre el -1 y el 1). Algunos ejemplos: Si crees que un parámetro se ubica entre el cero y el un, en lugar de usar Uniforme(0,1), intenta Normal(0.5, 0.5). Un parámetro de escala esta restringido a ser positivo y quieres una inicial vaga, propones Uniforme(0,100) (o Uniforme(0,1000), es mejor no especificar inicial, que en Stan equivale a una inicial uniforme no informativa, o usar una Esxponencial con valor esperado 10, o una media-normal(0,10). 10.9.2 Recursos de Stan y paquetes con R: Stan Manual. Stan User’s Guide. El paquete rstanarm, Gabry and Goodrich (2019), facilita el uso de Stan desde R, cuenta con funciones para llevar a cabo estimación Bayesiana sin necesidad de escribir código en el lenguaje de Stan, de acuerdo a los autores el objetivo del paquete es que la estimación bayesiana sea rutina para los modelos de regresión más comunes. El paquete bayesplot, Gabry and Mahr (2019), cuenta con funciones para graficar estimaciones de los parámetros, realizar diagnósticos visuales de las cadenas (convergencia), y analizar el ajuste del modelo (graficando de la predictiva posterior). El paquete shinystan, (???), contiene resumenes gráficos y numéricos de parámetros de un modelo y de diagnósticos de convergencia. Referencias "],
["tareas.html", "Tareas", " Tareas Las tareas se envían por correo a teresa.ortiz.mancera@gmail.com con título: EstComp-TareaXX (donde XX corresponde al número de tarea, 01..). Las tareas deben incluir código y resultados (si conocen Rmarkdown es muy conveniente para este propósito). "],
["instalación-y-visualización.html", "1. Instalación y visualización", " 1. Instalación y visualización 1. Instala los siguientes paquetes (o colecciones): tidyverse de CRAN (install.packages(&quot;tidyverse&quot;)) devtools de CRAN (install.packages(&quot;devtools&quot;)) gapminder de CRAN (install.packages(&quot;gapminder&quot;)) estcomp de GitHUB (debes haber instalado devtools y correr devtools::install_github(&quot;tereom/estcomp&quot;)) mxmaps instalarlo es opcional de GitHub 2. Visualización Elige un base de datos, recuerda que en la ayuda puedes encontrar más información de las variables (?gapminder): gapminder (paquete gapminder en CRAN). election_2012 ó election_sub_2012 (paquete estcomp). df_edu (paquete estcomp). enlacep_2013 o un subconjuto de este (paquete estcomp). Escribe algunas preguntas que consideres interesantes de los datos. Realiza \\(3\\) gráficas buscando explorar las preguntas de arriba y explica las relaciones que encuentres. Debes usar lo que revisamos en estas notas y al menos una de las gráficas debe ser de paneles (usando facet_wrap() o facet_grid). 4. Prueba (en clase)! Ejercicios basados en ejercicios de Wickham and Grolemund (2017). Socrative: https://b.socrative.com/login/student/ Room: ESTCOMP library(tidyverse,warn.conflicts = FALSE, quietly = TRUE) library(gridExtra) # 1. one &lt;- ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy)) # 2. two &lt;- ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy), se = FALSE) # 3. three &lt;- ggplot(data = mpg) + geom_smooth(mapping = aes(x = displ, y = hwy, group = drv)) # 4. four &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;blue&quot;), show.legend = FALSE) # 5. five &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;blue&quot;, show.legend = FALSE) # 6. six &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy), color = &quot;class&quot;, show.legend = FALSE) # 7. seven &lt;- ggplot(data = mpg) + geom_point(mapping = aes(x = displ, y = hwy, color = &quot;class&quot;), show.legend = FALSE) eight &lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy, color = drv)) + geom_point() + geom_smooth() nine &lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(aes(color = drv)) + geom_smooth(data = select(mpg, displ, hwy), aes(x = displ, y = hwy)) ten &lt;- ggplot(data = mpg, mapping = aes(x = displ, y = hwy)) + geom_point(aes(color = drv)) + geom_smooth() eleven &lt;- ggplot(data = mpg) + geom_point(aes(x = displ, y = hwy, color = drv)) + geom_smooth(aes(x = displ, y = hwy, color = drv)) Referencias "],
["transformación-de-datos-1.html", "2. Transformación de datos", " 2. Transformación de datos Vuelve a instalar el paquete estcomp para asegurar que tengas todos los datos y su documentación: devtools::install_github(&quot;tereom/estcomp&quot;) Usaremos los datos df_edu, ve la ayuda para entender sus variables: library(estcomp) ?df_edu ¿Cuál es el municipo con mayor escolaridad promedio (valor de schoolyrs)? Tip: usa filter para quedarte únicamente con sex correspondiente a Total. Crea una data.frame que contenga una línea por cada estado y por sexo, con la siguiente información: la escolaridad promedio por estado y sexo (ponderada por la población pop_15) la población de cada sexo (mayor a 15 años) Crea una variable que indique el porcentaje de la población que cursó al menos educación básica. Enuncia al menos una pregunta que se pueda responder transformando y graficando estos datos. Crea tu(s) gráfica(s). "],
["unión-de-tablas-y-limpieza-de-datos.html", "3. Unión de tablas y limpieza de datos", " 3. Unión de tablas y limpieza de datos Pueden encontrar la versión de las notas de datos limpuis usando gather() y spread() [aquí](https://tereom.github.io/tutoriales/datos_limpios.html. Trabajaremos con los datos df_marital, ¿Están limpios los datos? en caso de que no ¿qué principio no cumplen? library(estcomp) df_marital #&gt; # A tibble: 29,484 x 14 #&gt; state_code municipio_code region state_name state_abbr municipio_name sex #&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; #&gt; 1 01 001 01001 Aguascali… AGS Aguascalientes Total #&gt; 2 01 001 01001 Aguascali… AGS Aguascalientes Total #&gt; 3 01 001 01001 Aguascali… AGS Aguascalientes Total #&gt; 4 01 001 01001 Aguascali… AGS Aguascalientes Total #&gt; 5 01 001 01001 Aguascali… AGS Aguascalientes Homb… #&gt; 6 01 001 01001 Aguascali… AGS Aguascalientes Homb… #&gt; 7 01 001 01001 Aguascali… AGS Aguascalientes Homb… #&gt; 8 01 001 01001 Aguascali… AGS Aguascalientes Homb… #&gt; 9 01 001 01001 Aguascali… AGS Aguascalientes Muje… #&gt; 10 01 001 01001 Aguascali… AGS Aguascalientes Muje… #&gt; # … with 29,474 more rows, and 7 more variables: age_group &lt;chr&gt;, pop &lt;dbl&gt;, #&gt; # single &lt;dbl&gt;, married &lt;dbl&gt;, living_w_partner &lt;dbl&gt;, separated &lt;dbl&gt;, #&gt; # other &lt;dbl&gt; Limpia los datos y muestra las primeras y últimas líneas (usa head() y tail()). Filtra para eliminar los casos a total en las variables sexo y edad, calcula a nivel nacional cuál es la proporción en cada situación conyugal por grupo de edad y sexo. ¿Cómo puedes graficar o presentar los resultados? Regresando a los datos que obtuviste en 2, une la tabla de datos con df_edu, ¿qué variables se usarán para unir? "],
["programación-funcional-y-distribución-muestral.html", "4. Programación funcional y distribución muestral", " 4. Programación funcional y distribución muestral Descarga la carpeta specdata, ésta contiene 332 archivos csv que almacenan información de monitoreo de contaminación en 332 ubicaciones de EUA. Cada archivo contiene información de una unidad de monitoreo y el número de identificación del monitor es el nombre del archivo. En este ejercicio nos interesa unir todas las tablas en un solo data.frame que incluya el identificador de las estaciones. La siguiente instrucción descarga los datos si trabajas con proyectos de RStudio, también puedes descargar el zip manualmente. library(usethis) use_directory(&quot;data&quot;) use_zip(&quot;https://d396qusza40orc.cloudfront.net/rprog%2Fdata%2Fspecdata.zip&quot;, destdir = &quot;data&quot;) Crea un vector con las direcciones de los archivos. Lee uno de los archivos usando la función read_csv() del paquete readr. Tip: especifica el tipo de cada columna usando el parámetro col_types. Utiliza la función map_df() para iterar sobre el vector con las direcciones de los archivos csv y crea un data.frame con todos los datos, recuerda añadir una columna con el nombre del archivo para poder identificar la estación. Consideramos los datos de ENLACE edo. de México (enlace), y la columna de calificaciones de español 3o de primaria (esp_3). library(estcomp) enlace &lt;- enlacep_2013 %&gt;% janitor::clean_names() %&gt;% mutate(id = 1:n()) %&gt;% select(id, cve_ent, turno, tipo, esp_3 = punt_esp_3, esp_6 = punt_esp_6, n_eval_3 = alum_eval_3, n_eval_6 = alum_eval_6) %&gt;% na.omit() %&gt;% filter(esp_3 &gt; 0, esp_6 &gt; 0, n_eval_3 &gt; 0, n_eval_6 &gt; 0, cve_ent == &quot;15&quot;) Selecciona una muestra de tamaño \\(n = 10, 100, 1000\\). Para cada muestra calcula media y el error estándar de la media usando el principio del plug-in: \\(\\hat{\\mu}=\\bar{x}\\), y \\(\\hat{se}(\\bar{x})=\\hat{\\sigma}_{P_n}/\\sqrt{n}\\). Tip: Usa la función sample_n() del paquete deplyr para generar las muestras. Ahora aproximareos la distribución muestral, para cada tamaño de muestra \\(n\\): simula \\(10,000\\) muestras aleatorias, ii) calcula la media en cada muestra, Realiza un histograma de la distribución muestral de las medias (las medias del paso anterior) iv) aproxima el error estándar calculando la desviación estándar de las medias del paso ii. Tip: Escribe una función que dependa del tamaño de muestra y usa la función rerun() del paquete purrr para hacer las \\(10,000\\) simulaciones. simula_media &lt;- function(n) { } medias_10 &lt;- rerun(10000, simula_media(n = 10)) %&gt;% flatten_dbl() Calcula el error estándar de la media para cada tamaño de muestra usando la información poblacional (ésta no es una aproximación), usa la fórmula: \\(se_P(\\bar{x}) = \\sigma_P/ \\sqrt{n}\\). ¿Cómo se comparan los errores estándar correspondientes a los distintos tamaños de muestra? Solución + bootstrap Presentamos la solución del ejercicio y agregamos como haríamos con bootsrtap. Suponemos que me interesa hacer inferencia del promedio de las calificaciones de los estudiantes de tercero de primaria en el Estado de México. En este ejercicio planteamos \\(3\\) escenarios (que simulamos): 1) que tengo una muestra de tamaño \\(10\\), 2) que tengo una muestra de tamaño \\(100\\), y 3) que tengo una muestra de tamaño \\(1000\\). Selección de muestras: set.seed(373783326) muestras &lt;- tibble(tamanos = c(10, 100, 1000)) %&gt;% mutate(muestras = map(tamanos, ~sample(enlace$esp_3, size = .))) Ahora procedemos de manera usual en estadística (usando fórmulas y no simulación), estimo la media de la muestra con el estimador plug-in \\[\\bar{x}={1/n\\sum x_i}\\] y el error estándar de \\(\\bar{x}\\) con el estimador plug-in \\[\\hat{se}(\\bar{x}) =\\bigg\\{\\frac{1}{n^2}\\sum_{i=1}^n(x_i-\\bar{x})^2\\bigg\\}^{1/2}\\] Estimadores plug-in: se_plug_in &lt;- function(x){ x_bar &lt;- mean(x) n_x &lt;- length(x) var_x &lt;- 1 / n_x ^ 2 * sum((x - x_bar) ^ 2) sqrt(var_x) } muestras_est &lt;- muestras %&gt;% mutate( medias = map_dbl(muestras, mean), e_estandar_plug_in = map_dbl(muestras, se_plug_in) ) muestras_est #&gt; # A tibble: 3 x 4 #&gt; tamanos muestras medias e_estandar_plug_in #&gt; &lt;dbl&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 10 &lt;dbl [10]&gt; 602. 19.3 #&gt; 2 100 &lt;dbl [100]&gt; 553. 6.54 #&gt; 3 1000 &lt;dbl [1,000]&gt; 552. 1.90 Ahora, recordemos que la distribución muestral es la distribución de una estadística, considerada como una variable aleatoria. Usando esta definción podemos aproximarla, para cada tamaño de muestra, simulando: simulamos muestras de tamaño \\(n\\) de la población, calculamos la estadística de interés (en este caso \\(\\bar{x}\\)), vemos la distribución de la estadística a lo largo de simulaciones. Histogramas de distribución muestral y aproximación de errores estándar con simulación muestras_sims &lt;- muestras_est %&gt;% mutate( sims_muestras = map(tamanos, ~rerun(10000, sample(enlace$esp_3, size = ., replace = TRUE))), sims_medias = map(sims_muestras, ~map_dbl(., mean)), e_estandar_aprox = map_dbl(sims_medias, sd) ) sims_medias &lt;- muestras_sims %&gt;% select(tamanos, sims_medias) %&gt;% unnest(sims_medias) ggplot(sims_medias, aes(x = sims_medias)) + geom_histogram(binwidth = 2) + facet_wrap(~tamanos, nrow = 1) + theme_minimal() Notamos que la variación en la distribución muestral decrece conforme aumenta el tamaño de muestra, esto es esperado pues el error estándar de una media es \\(\\sigma_P / \\sqrt{n}\\), y dado que en este ejemplo estamos calculando la media para la misma población el valor poblacional \\(\\sigma_P\\) es constante y solo cambia el denominador. Nuestros valores de error estándar con simulación están en la columna e_estandar_aprox: muestras_sims %&gt;% select(tamanos, medias, e_estandar_plug_in, e_estandar_aprox) #&gt; # A tibble: 3 x 4 #&gt; tamanos medias e_estandar_plug_in e_estandar_aprox #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 10 602. 19.3 18.9 #&gt; 2 100 553. 6.54 5.92 #&gt; 3 1000 552. 1.90 1.87 En este ejercicio estamos simulando para examinar las distribuciones muestrales y para ver que podemos aproximar el error estándar de la media usando simulación; sin embargo, dado que en este caso hipotético conocemos la varianza poblacional y la fórmula del error estándar de una media, por lo que podemos calcular el verdadero error estándar para una muestra de cada tamaño. Calcula el error estándar de la media para cada tamaño de muestra usando la información poblacional: muestras_sims_est &lt;- muestras_sims %&gt;% mutate(e_estandar_pob = sd(enlace$esp_3) / sqrt(tamanos)) muestras_sims_est %&gt;% select(tamanos, e_estandar_plug_in, e_estandar_aprox, e_estandar_pob) #&gt; # A tibble: 3 x 4 #&gt; tamanos e_estandar_plug_in e_estandar_aprox e_estandar_pob #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 10 19.3 18.9 18.7 #&gt; 2 100 6.54 5.92 5.93 #&gt; 3 1000 1.90 1.87 1.87 En la tabla de arriba podemos comparar los \\(3\\) errores estándar que calculamos, recordemos que de estos \\(3\\) el plug-in es el único que podríamos obtener en un escenario real pues los otros dos los calculamos usando la población. Una alternativa al estimador plug-in del error estándar es usar bootstrap (en muchos casos no podemos calcular el error estándar plug-in por falta de fórmulas) pero podemos usar bootstrap: utilizamos una estimación de la distribución poblacional y calculamos el error estándar bootstrap usando simulación. Hacemos el mismo procedimiento que usamos para calcular e_estandar_apox pero sustituimos la distribución poblacional por la distriución empírica. Hagámoslo usando las muestras que sacamos en el primer paso: muestras_sims_est_boot &lt;- muestras_sims_est %&gt;% mutate( sims_muestras_boot = map2(muestras, tamanos, ~rerun(10000, sample(.x, size = .y, replace = TRUE))), sims_medias_boot = map(sims_muestras_boot, ~map_dbl(., mean)), e_estandar_boot = map_dbl(sims_medias_boot, sd) ) muestras_sims_est_boot #&gt; # A tibble: 3 x 11 #&gt; tamanos muestras medias e_estandar_plug… sims_muestras sims_medias #&gt; &lt;dbl&gt; &lt;list&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;list&gt; &lt;list&gt; #&gt; 1 10 &lt;dbl [1… 602. 19.3 &lt;list [10,00… &lt;dbl [10,0… #&gt; 2 100 &lt;dbl [1… 553. 6.54 &lt;list [10,00… &lt;dbl [10,0… #&gt; 3 1000 &lt;dbl [1… 552. 1.90 &lt;list [10,00… &lt;dbl [10,0… #&gt; # … with 5 more variables: e_estandar_aprox &lt;dbl&gt;, e_estandar_pob &lt;dbl&gt;, #&gt; # sims_muestras_boot &lt;list&gt;, sims_medias_boot &lt;list&gt;, e_estandar_boot &lt;dbl&gt; Graficamos los histogramas de la distribución bootstrap para cada muestra. sims_medias_boot &lt;- muestras_sims_est_boot %&gt;% select(tamanos, sims_medias_boot) %&gt;% unnest(sims_medias_boot) ggplot(sims_medias_boot, aes(x = sims_medias_boot)) + geom_histogram(binwidth = 4) + facet_wrap(~tamanos, nrow = 1) + theme_minimal() Y la tabla con todos los errores estándar quedaría: muestras_sims_est_boot %&gt;% select(tamanos, e_estandar_boot, e_estandar_plug_in, e_estandar_aprox, e_estandar_pob) #&gt; # A tibble: 3 x 5 #&gt; tamanos e_estandar_boot e_estandar_plug_in e_estandar_aprox e_estandar_pob #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 10 19.3 19.3 18.9 18.7 #&gt; 2 100 6.53 6.54 5.92 5.93 #&gt; 3 1000 1.89 1.90 1.87 1.87 Observamos que el estimador bootstrap del error estándar es muy similar al estimador plug-in del error estándar, esto es esperado pues se calcularon con la misma muestra y el error estándar bootstrap converge al plug-in conforme incrementamos el número de muestras bootstrap. "],
["bootstrap-conteo.html", "5. Bootstrap conteo", " 5. Bootstrap conteo Conteo rápido En México, las elecciones tienen lugar un domingo, los resultados oficiales del proceso se presentan a la población una semana después. A fin de evitar proclamaciones de victoria injustificadas durante ese periodo el INE organiza un conteo rápido. El conteo rápido es un procedimiento para estimar, a partir de una muestra aleatoria de casillas, el porcentaje de votos a favor de los candidatos en la elección. En este ejercicio deberás crear intervalos de confianza para la proporción de votos que recibió cada candidato en las elecciones de 2006. La inferencia se hará a partir de una muestra de las casillas similar a la que se utilizó para el conteo rápido de 2006. El diseño utilizado es muestreo estratificado simple, lo que quiere decir que: se particionan las casillas de la pablación en estratos (cada casilla pertenece a exactamente un estrato), y dentro de cada estrato se usa muestreo aleatorio para seleccionar las casillas que estarán en la muestra. En este ejercicio (similar al conteo rápido de 2006): Se seleccionó una muestra de \\(7,200\\) casillas La muestra se repartió a lo largo de 300 estratos. La tabla strata_sample_2006 contiene en la columna \\(N\\) el número total de casillas en el estrato y en \\(n\\) el número de casillas que se seleccionaron en la muestra, para cada estrato: library(estcomp) strata_sample_2006 #&gt; # A tibble: 300 x 3 #&gt; stratum n N #&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 1 20 369 #&gt; 2 2 23 420 #&gt; 3 3 24 440 #&gt; 4 4 31 570 #&gt; 5 5 29 528 #&gt; 6 6 37 664 #&gt; 7 7 26 474 #&gt; 8 8 21 373 #&gt; 9 9 25 457 #&gt; 10 10 24 430 #&gt; # … with 290 more rows La tabla sample_2006 en el paquete estcomp (vuelve a instalar de ser necesario) contiene para cada casilla: el estrato al que pertenece: stratum el número de votos que recibió cada partido/coalición: pan, pri_pvem, panal, prd_pt_convergencia, psd y la columna otros indica el número de votos nulos o por candidatos no registrados. el total de votos registrado en la casilla: total. sample_2006 #&gt; # A tibble: 7,200 x 11 #&gt; polling_id stratum edo_id rural pri_pvem pan panal prd_pt_conv psd otros #&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; #&gt; 1 74593 106 16 1 47 40 0 40 0 9 #&gt; 2 109927 194 27 0 131 10 0 147 1 8 #&gt; 3 112039 199 28 0 51 74 2 57 2 2 #&gt; 4 86392 141 20 1 145 64 2 139 1 14 #&gt; 5 101306 176 24 0 51 160 0 64 14 1 #&gt; 6 86044 140 20 1 150 20 0 166 1 11 #&gt; 7 56057 57 15 1 117 119 2 82 0 24 #&gt; 8 84186 128 19 0 118 205 8 73 9 13 #&gt; 9 27778 283 9 0 26 65 5 249 7 2 #&gt; 10 29892 289 9 0 27 32 0 338 14 7 #&gt; # … with 7,190 more rows, and 1 more variable: total &lt;int&gt; Una de las metodolgías de estimación, que se usa en el conteo rápido, es estimador de razón y se contruyen intervalos de 95% de confianza usando el método normal con error estándar bootstrap. En este ejercicio debes construir intervalos usando este procedimiento. Para cada candidato: Calcula el estimador de razón combinado, para muestreo estratificado la fórmula es: \\[\\hat{p}=\\frac{\\sum_h \\frac{N_h}{n_h} \\sum_i Y_{hi}}{\\sum_h \\frac{N_h}{n_h} \\sum_i X_{hi}}\\] donde: \\(\\hat{p}\\) es la estimación de la proporción de votos que recibió el candidato en la elección. \\(Y_{hi}\\) es el número total de votos que recibió el candidato en la \\(i\\)-ésima casillas, que pertence al \\(h\\)-ésimo estrato. \\(X_{hi}\\) es el número total de votos en la \\(i\\)-ésima casilla, que pertence al \\(h\\)-ésimo estrato. \\(N_h\\) es el número total de casillas en el \\(h\\)-ésimo estrato. \\(n_h\\) es el número de casillas del \\(h\\)-ésimo estrato que se seleccionaron en la muestra. Utiliza bootstrap para calcular el error estándar, y reporta tu estimación del error. Genera 1000 muestras bootstrap. Recuerda que las muestras bootstrap tienen que tomar en cuenta la metodología que se utilizó en la selección de la muestra original, en este caso, lo que implica es que debes tomar una muestra aleatoria independient dentro de cada estrato. Construye un intervalo del 95% de confianza utilizando el método normal. Repite para todos los partidos (y la categoría otros). Reporta tus intervalos en una tabla. "],
["respuesta-ejercicios-clase.html", "Respuesta ejercicios clase", " Respuesta ejercicios clase Considera el coeficiente de correlación muestral entre la calificación de \\(y=\\)esp_3 y la de \\(z=\\)esp_6. ¿Qué tan precisa es esta estimación? Calcula el estimador plug-in y el error estándar bootstrap. library(estcomp) # universo: creamos datos de ENLACE estado de México enlace &lt;- enlacep_2013 %&gt;% janitor::clean_names() %&gt;% mutate(id = 1:n()) %&gt;% select(id, cve_ent, turno, tipo, esp_3 = punt_esp_3, esp_6 = punt_esp_6, n_eval_3 = alum_eval_3, n_eval_6 = alum_eval_6) %&gt;% na.omit() %&gt;% filter(esp_3 &gt; 0, esp_6 &gt; 0, n_eval_3 &gt; 0, n_eval_6 &gt; 0, cve_ent == &quot;15&quot;) glimpse(enlace) set.seed(16021) n &lt;- 300 # muestra enlace_muestra &lt;- sample_n(enlace, n) # estimador plug-in theta_hat &lt;- cor(enlace_muestra$esp_3, enlace_muestra$esp_6) boot_reps &lt;- function(){ muestra_boot &lt;- sample_frac(enlace_muestra, size = 1, replace = TRUE) cor(muestra_boot$esp_3, muestra_boot$esp_6) } # error estandar bootstrap replicaciones &lt;- rerun(1000, boot_reps()) %&gt;% flatten_dbl() sd(replicaciones) Varianza sesgada con datos spatial. library(bootstrap) rep_bootstrap &lt;- function() { boot_sample &lt;- sample(spatial$A, replace = TRUE) boot_replication &lt;- var_sesgada(boot_sample) } theta_hat &lt;- var_sesgada(spatial$A) reps_boot &lt;- rerun(5000, rep_bootstrap()) %&gt;% flatten_dbl() qplot(reps_boot) sd_spatial &lt;- sd(reps_boot) # Normal theta_hat - 2 * sd_spatial theta_hat + 2 * sd_spatial # t theta_hat + qt(0.025, 25) * sd_spatial theta_hat + qt(0.975, 25) * sd_spatial # Percentiles quantile(reps_boot, probs = c(0.025, 0.975)) "],
["más-bootstrap.html", "6. Más bootstrap", " 6. Más bootstrap Consideramos la siguiente muestra de los datos de ENLACE: library(estcomp) set.seed(1983) enlace_sample &lt;- enlacep_2013 %&gt;% janitor::clean_names() %&gt;% mutate(id = 1:n()) %&gt;% select(id, cve_ent, turno, tipo, mat_3 = punt_mat_3, n_eval_3 = alum_eval_3) %&gt;% na.omit() %&gt;% filter(mat_3 &gt; 0, n_eval_3 &gt; 0) %&gt;% group_by(cve_ent) %&gt;% sample_frac(size = 0.1) %&gt;% ungroup() Selecciona el subconjunto de datos de Chiapas (clave de entidad 07): Calcula el estimador plug-in para la mediana de las calificaciones de matemáticas (en Chiapas). Calcula el estimador bootstrap del error estándar y construye un intrvalo de confianza normal. Debes 1) tomar muestras bootstrap con reemplazo del subconjunto de datos de Chiapas, 2) calcular la mediana en cada una de las muestras y 3) calcular la desviación estándar de las medianas de 2). Repite los pasos anteriores para la Ciudad de México (clave de entidad 09). Compara los intervalos de confianza. Intervalos de confianza. En este ejercicio compararemos distintos intervalos de confianza para las medias de una exponencial Simula una muestra de tamaño 40 de una distribución exponencial(1/2). Calcula el estimador plug-in. Calcula intervalos: normal, de percentiles y \\(BC_a\\), presentalos en una tabla (para los \\(BC_a\\) usa la función boot.ci() del paquete boot. Repite los pasos anteriores 200 veces y grafica los intervalos, ¿cómo se comparan? library(boot) sim_exp &lt;- rexp(40, 1/2) my_mean &lt;- function(x, ind) mean(x[ind]) boot_sim_exp &lt;- boot(sim_exp, my_mean, R = 10000) ints &lt;- boot.ci(boot_sim_exp, type = c(&quot;norm&quot;, &quot;perc&quot;, &quot;bca&quot;)) "],
["simulación-de-variables-aleatorias-1.html", "7. Simulación de variables aleatorias", " 7. Simulación de variables aleatorias Simulación de una Gamma Usa el método de aceptación y rechazo para generar 1000 observaciones de una variable aleatoria con distribución gamma(3/2,1). Simulación de una Normal Implementa el algoritmo de simulación de una variable aleatoria normal estándar visto en clase (simula 1000 observaciones de una normal estándar): Nuestro objetivo es primero, simular una variable aleatoria normal estándar Z, para ello comencemos notando que el valor absoluto de Z tiene función de densidad: \\[f(x)=\\frac{2}{\\sqrt{2\\pi}}e^{-x^2/2}\\] con soporte en los reales positivos. Generaremos observaciones de la densidad anterior usando el método de aceptación y rechazo con \\(g\\) una densidad exponencial con media 1: \\[g(x)= e^{-x}\\] Ahora, \\(\\frac{f(x)}{g(x)}=\\sqrt{2/\\pi}e^{x - x^2/2}\\) y por tanto el máximo valor de \\(f(x)/g(x)\\) ocurre en el valor \\(x\\) que maximiza \\(x - x^2/2\\), esto ocurre en \\(x=1\\), y podemos tomar \\(c=\\sqrt{2e/\\pi}\\), \\[\\frac{f(x)}{cg(x)}=exp\\bigg\\{x - \\frac{x^2}{2}-{1}{2}\\bigg\\}\\] \\[=exp\\bigg\\{\\frac{(x-1)^2}{2}\\bigg\\}\\] y por tanto podemos generar el valor absoluto de una variable aleatoria con distribución normal estándar de la siguiente manera: Genera \\(Y\\) una variable aleatoria exponencial con tasa 1. Genera un número aleatorio \\(U\\). Si \\(U \\leq exp\\{-(Y-1)^2/2\\}\\) define \\(X=Y\\), en otro caso vuelve a 1. Para generar una variable aleatoria con distribución normal estándar \\(Z\\) simplemente elegimos \\(X\\) o \\(-X\\) con igual probabilidad. "],
["distribuciones-multivariada-y-simulación.html", "8. Distribuciones multivariada y simulación", " 8. Distribuciones multivariada y simulación Retomando el ejemplo de asegurados visto en clase, supongamos ahora que una compañía de seguros divide a la gente en dos clases: propensos a accidente (30% de las personas) y no propensos a accidente. En un año dado aquellos propensos a accidentes sufren un accidente con probabilidad 0.4, mientras que los del otro grupo sufren un accidente con probabilidad 0.2. ¿Cuál es la probabilidad de que un asegurado tenga un accidente en su segundo año condicional a que sufrió un accidente en el primer año? Supongamos que una compañía cambia la tecnología usada para producir una cámara, un estudio estima que el ahorro en la producción es de $5 por unidad con un error estándar de $4. Más aún, una proyección estima que el tamaño del mercado (esto es, el número de cámaras que se venderá) es de 40,000 con un error estándar de 10,000. Suponiendo que las dos fuentes de incertidumbre son independientes, usa simulación de variables aleatorias normales para estimar el total de dinero que ahorrará la compañía, calcula un intervalo de confianza. "],
["inferencia-visual-y-simulación-e-modelos.html", "9. Inferencia visual y simulación e modelos", " 9. Inferencia visual y simulación e modelos 1. Análisis discriminante Existen métodos de clasificación (supervisados o no supervisados) para formar grupos en términos de variables que describen a los individuos. Estos métodos (análisis discriminante, o k-means, por ejemplo), pretenden formar grupos compactos, bien separados entre ellos. Cuando aplicamos el método, obtenemos clasificadores basados en las variables de entrada. La pregunta es: ¿los grupos resultantes son producto de patrones que se generalizan a la población, o capitalizaron en variación aleatoria para formarse? Especialmente cuando tenemos muchas mediciones de los individuos, y una muestra relativamente chica, Es relativamente fácil encontrar combinaciones de variables que separan los grupos, aunque estas combinaciones y diferencias están basadas en ruido y no generalizan a la población. En el siguiente ejemplo, tenemos 4 grupos de avispas (50 individuos en total), y para cada individuo se miden expresiones de 42 genes distintos. La pregunta es: ¿Podemos separar a los grupos de avispas dependiendo de sus mediciones? Usaremos análisis discriminante para buscar proyecciones de los datos en dimensión baja de forma que los grupos sean lo más compactos y separados posibles. Para probar qué tan bien funciona este método, podemos seguir el protocolo lineup. El siguiente código es como se procedería en análisis discriminante en R usando los datos: data(wasps) # incluídos en nullabor wasp_lda &lt;- MASS::lda(Group~., data = wasps[,-1]) wasp_ld &lt;- predict(wasp_lda, dimen = 2)$x true &lt;- data.frame(wasp_ld, Group = wasps$Group) ggplot(true, aes(x = LD1, y = LD2, colour = Group)) + geom_point() + theme(aspect.ratio = 1) Para generar los conjuntos de datos nulos debes permutar lo columna Group de la tabla de datos y repetir los pasos de arriba, realiza esto 19 veces y realiza una gráfica de páneles en la que escondas los datos verdaderos entre los datos nulos, ¿cuál es tu conclusión? 2. Simulación de un modelo de regresión Los datos beauty consisten en evaluaciones de estudiantes a profesores, los estudiantes calificaron belleza y calidad de enseñanza para distintos cursos en la Universidad de Texas. Las evaluaciones de curso se realizaron al final del semestre y tiempo después \\(6\\) estudiantes que no llevaron el curso realizaron los juicios de belleza. Ajustamos el siguiente modelo de regresión lineal usando las variables edad (age), belleza (btystdave), sexo (female) e inglés no es primera lengua (nonenglish) para predecir las evaluaciones del curso (courseevaluation). beauty &lt;- readr::read_csv(&quot;https://raw.githubusercontent.com/tereom/est-computacional-2018/master/data/beauty.csv&quot;) fit_score &lt;- lm(courseevaluation ~ age + btystdave + female + nonenglish, data = beauty) La instructora \\(A\\) es una mujer de \\(50\\) años, el inglés es su primera lengua y tiene un puntaje de belleza de \\(-1\\). El instructor B es un hombre de \\(60\\) años, su primera lengua es el inglés y tiene un puntaje de belleza de \\(-0.5\\). Simula \\(1000\\) generaciones de la evaluación del curso de estos dos instructores. En tus simulaciones debes incorporar la incertidumbre en los parámetros y en la predicción. Para hacer las simulaciones necesitarás la distribución del vector de coeficientes \\(\\beta\\), este es normal con media: coef(fit_score) y matriz de varianzas y covarianzas \\(\\sigma^2 V\\), donde \\(V\\) es: summary(fit_score)$cov.unscaled y \\(\\sigma\\) se calcula como \\(\\sigma=\\hat{\\sigma}\\sqrt{(df)/X}\\), donde X es una generación de una distribución \\(\\chi ^2\\) con \\(df\\) (\\(458\\)) grados de libertad \\(\\hat{\\sigma}\\) es: summary(fit_score)$sigma y \\(df\\) (los grados de libertad) se obtienen: summary(fit_score)$df[2] Una vez que obtengas una simulación del vector \\(\\beta\\) generas simulaciones para los profesores usando el modelo de regresión lineal y las simulaciones de los parámetros (también puedes usar la función sim del paquete arm. Realiza un histograma de la diferencia entre la evaluación del curso para \\(A\\) y \\(B\\). ¿Cuál es la probabilidad de que \\(A\\) obtenga una calificación mayor? En el inciso anterior obtienes simulaciones de la distribución conjunta \\(p(\\tilde{y},\\beta,\\sigma^2)\\) donde \\(\\beta\\) es el vector de coeficientes de la regresión lineal. Para este ejercicio nos vamos a enfocar en el coeficiente de belleza (\\(\\beta_3\\)), realiza \\(6000\\) simulaciones del modelo (como en el inciso anterior) y guarda las realizaciones de \\(\\beta_3\\). Genera un histograma con las simulaciones de \\(\\beta_3\\). Calcula la media y desviación estándar de las simulaciones y comparalas con la estimación y desviación estándar del coeficiente obtenidas usando summary. "],
["simulación-muestra-y-bootstrap-paramétrico.html", "10. Simulación muestra y bootstrap paramétrico", " 10. Simulación muestra y bootstrap paramétrico Simulación para calcular tamaños de muestra Supongamos que queremos hacer una encuesta para estimar la proporción de hogares donde se consume refresco de manera regular, para ello se diseña un muestreo por conglomerados donde los conglomerados están dados por conjuntos de hoagres de tal manera que todos los conglomerados tienen el mismo número de hogares. La selección de la muestra se hará en dos etapas: Seleccionamos \\(J\\) conglomerados de manera aleatoria. En cada conglomerado seleccionames \\(n/J\\) hogares para entrevistar. El estimador será simplemente el porcentaje de hogares del total de la muestra que consume refresco. Suponemos que la verdadera proporción es cercana a \\(0.50\\) y que la media de la proporción de interés tiene una desviación estándar de \\(0.1\\) a lo largo de los conglomerados . Supongamos que la muestra total es de \\(n=1000\\). ¿Cuál es la estimación del error estándar para la proporción estimada si \\(J=1,10,100,1000\\)? El obejtivo es estimar la propoción que consume refresco en la población con un error estándar de a lo más \\(2\\%\\). ¿Que valores de \\(J\\) y \\(n\\) debemos elegir para cumplir el objetivo al menor costo? Los costos del levantamiento son: + \\(50\\) pesos por encuesta. + \\(500\\) pesos por conglomerado Solución muestreo &lt;- function(J, n_total = 1000) { n_cong &lt;- floor(n_total / J) medias &lt;- rnorm(J, 0.5, 0.1) medias &lt;- case_when( medias &lt; 0 ~ 0, medias &gt; 1 ~ 1, TRUE ~ medias) resp &lt;- rbinom(J, n_cong, medias) sum(resp) / n_total } errores &lt;- tibble(J = c(1, 10, 100, 1000)) %&gt;% mutate(sims = map(J, ~(rerun(1000, muestreo(.)) %&gt;% flatten_dbl())), error_est = map_dbl(sims, sd) %&gt;% round(3)) errores #&gt; # A tibble: 4 x 3 #&gt; J sims error_est #&gt; &lt;dbl&gt; &lt;list&gt; &lt;dbl&gt; #&gt; 1 1 &lt;dbl [1,000]&gt; 0.098 #&gt; 2 10 &lt;dbl [1,000]&gt; 0.035 #&gt; 3 100 &lt;dbl [1,000]&gt; 0.02 #&gt; 4 1000 &lt;dbl [1,000]&gt; 0.016 tamano_muestra &lt;- function(J) { n_total &lt;- max(100, J) ee &lt;- rerun(500, muestreo(J = J, n_total = n_total)) %&gt;% flatten_dbl() %&gt;% sd() while(ee &gt; 0.02){ n_total = n_total + 20 ee &lt;- rerun(500, muestreo(J = J, n_total = n_total)) %&gt;% flatten_dbl() %&gt;% sd() %&gt;% round(3) } list(ee = ee, n_total = n_total, costo = 500 * J + 50 * n_total) } tamanos &lt;- c(20, 30, 40, 50, 100, 150) costos &lt;- map_df(tamanos, tamano_muestra) costos$J &lt;- tamanos costos #&gt; # A tibble: 6 x 4 #&gt; ee n_total costo J #&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; #&gt; 1 0.02 23000 1160000 20 #&gt; 2 0.02 1900 110000 30 #&gt; 3 0.02 1100 75000 40 #&gt; 4 0.02 940 72000 50 #&gt; 5 0.02 580 79000 100 #&gt; 6 0.019 550 102500 150 ggplot(costos, aes(x = J, y = costo / 1000)) + geom_line() + scale_y_log10() + theme_minimal() + labs(y = &quot;miles de pesos&quot;, title = &quot;Costos&quot;) Bootstrap paramétrico Sean \\(X_1,...,X_n \\sim N(\\mu, 1)\\). Sea \\(\\theta = e^{\\mu}\\), simula una muestra de \\(n=100\\) observaciones usando \\(\\mu=5\\). Usa el método delta para estimar \\(\\hat{se}\\) de \\(\\hat{\\theta}\\) y crea un intervalo del \\(95\\%\\) de confianza. Pista: \\(se(\\hat{\\mu}) = 1/\\sqrt{n}\\) Repite el inciso anterior usando boostrap paramétrico. Finalmente usa bootstrap no paramétrico y compara tus respuestas. Realiza un histograma de replicaciones bootstrap para cada método, estas son estimaciones de la distribución de \\(\\hat{\\theta}\\). El método delta también nos da una aproximación a esta distribución: \\(Normal(\\hat{\\theta},\\hat{se}^2)\\). Comparalos con la verdadera distribución de \\(\\hat{\\theta}\\) (que puedes obtener vía simulación). ¿Cuál es la aproximación más cercana a la verdadera distribución? "],
["familias-conjugadas.html", "11-Familias conjugadas", " 11-Familias conjugadas 1. Modelo Beta-Binomial Una compañía farmacéutica afirma que su nueva medicina incrementa la probabilidad de concebir un niño (sexo masculino), pero aún no publican estudios. Supón que conduces un experimento en el cual \\(50\\) parejas se seleccionan de manera aleatoria de la población, toman la medicina y conciben. Nacen 30 niños y 20 niñas. Quieres estimar la probabilidad de concebir un niño para parejas que toman la medicina. ¿Cuál es una inicial apropiada? No tiene que estar centrada en \\(0.5\\) pues esta corresponde a personas que no toman la medicina, y la inicial debe reflejar tu incertidumbre sobre el efecto de la droga. Usando tu inicial de a) grafica la posterior y decide si es creíble que las parejas que toman la medicina tienen una probabilidad de \\(0.5\\) de concebir un niño. Supón que la farmacéutica asevera que la probabilidad de concebir un niño cuando se toma la medicina es cercana al \\(60\\%\\) con alta certeza. Representa esta postura con una distribución inicial \\(Beta(60,40)\\). Comparala con la inicial de un escéptico que afirma que la medicina no hace diferencia, representa esta creencia con una inicial \\(Beta(50,50)\\). ¿Cómo se compara la probabilidad posterior de concebir un niño (usando las distintas iniciales)? 2. Otra familia conjugada Supongamos que nos interesa analizar el IQ de una muestra de estudiantes del ITAM y suponemos que el IQ de un estudiante tiene una distribución normal \\(x \\sim N(\\theta, \\sigma^2)\\) con \\(\\sigma ^ 2\\) conocida. Considera que observamos el IQ de un estudiante \\(x\\). La verosimilitud del modelo es: \\[p(x|\\theta)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}}exp\\left(-\\frac{1}{2\\sigma^2}(x-\\theta)^2\\right)\\] Realizaremos un análisis bayesiano por lo que hace falta establer una distribución inicial, elegimos \\(p(\\theta)\\) que se distribuya \\(N(\\mu, \\tau^2)\\) donde elegimos los parámetros \\(\\mu, \\tau\\) que mejor describan nuestras creencias iniciales, por ejemplo si tengo mucha certeza de que el \\(IQ\\) promedio se ubica en \\(150\\), elegiría \\(\\mu=150\\) y una desviación estándar chica por ejemplo \\(\\tau = 5\\). Entonces la distribución inicial es: \\[p(\\theta)=\\frac{1}{\\sqrt{2\\pi\\tau^2}}exp\\left(-\\frac{1}{2\\tau^2}(\\theta-\\mu)^2\\right)\\] Calcula la distribución posterior \\(p(\\theta|x) \\propto p(x|\\theta)p(\\theta)\\), usando la inicial y verosimilitud que definimos arriba. Una vez que realices la multiplicación debes identificar el núcleo de una distribución Normal, ¿cuáles son sus parámetros (media y varianza)? "],
["metropolis-1.html", "12-Metropolis", " 12-Metropolis Sigue las instrucciones de aquí para instalar Stan, lo usaremos la próxima clase. Regresamos al ejercicio de IQ de la tarea anterior, en ésta hiciste cálculos para el caso de una sola observación. En este ejercicio consideramos el caso en que observamos una muestra \\(x=\\{x_1,...,x_N\\}\\), y utilizaremos Metrópolis para obtener una muestra de la distribución posterior. Crea una función \\(prior\\) que reciba los parámetros \\(\\mu\\) y \\(\\tau\\) que definen tus creencias del parámetro desconocido \\(\\theta\\) y devuelva \\(p(\\theta)\\), donde \\(p(\\theta)\\) tiene distriución \\(N(\\mu, \\sigma^2)\\) prior &lt;- function(mu, tau{ function(theta){ ... # llena esta parte } } Utiliza la función que acabas de escribir para definir una distribución inicial con parámetros \\(\\mu = 150\\) y \\(\\tau = 15\\), llámala mi_prior. Ya que tenemos la distribución inicial debemos escribir la verosimilitud, en este caso la verosimilitud es: \\[p(x|\\theta, \\sigma^2)=\\frac{1}{(2\\pi\\sigma^2)^{N/2}}exp\\left(-\\frac{1}{2\\sigma^2}\\sum_{j=1}^{N}(x_j-\\theta)^2\\right)\\] \\[=\\frac{1}{(2\\pi\\sigma^2)^{N/2}}exp\\left(-\\frac{1}{2\\sigma^2}\\bigg(\\sum_{j=1}^{N}x_j^2-2\\theta\\sum_{j=1}^{N} x_j + N\\theta^2 \\bigg) \\right)\\] Recuerda que estamos suponiendo varianza conocida, supongamos que la desviación estándar es \\(\\sigma=20\\). \\[p(x|\\theta)=\\frac{1}{(2\\pi (20^2))^{N/2}}exp\\left(-\\frac{1}{2 (20^2)}\\bigg(\\sum_{j=1}^{N}x_j^2-2\\theta\\sum_{j=1}^{N} x_j + N\\theta^2 \\bigg) \\right)\\] Crea una función \\(likeNorm\\) en R que reciba la desviación estándar, la suma de los valores observados \\(\\sum x_i\\), la suma de los valores al cuadrado \\(\\sum x_i^2\\) y el número de observaciones \\(N\\) la función devolverá la función de verosimilitud (es decir va a regresar una función que depende únicamente de \\(\\theta\\)). # S: sum x_i, S2: sum x_i^2, N: número obs. likeNorm &lt;- function(S, S2, N){ function(theta){ ... # llena esta parte } } Supongamos que aplicamos un test de IQ a 100 alumnos y observamos que la suma de los puntajes es 13300, es decir \\(\\sum x_i=13,000\\) y \\(\\sum x_i^2=1,700,000\\). Utiliza la función que acabas de escribir para definir la función de verosimilitud condicional a los datos observados, llámala mi_like. La distribución posterior no normalizada es simplemente el producto de la inicial y la posterior: postRelProb &lt;- function(theta){ mi_like(theta) * mi_prior(theta) } Utiliza Metropolis para obtener una muestra de valores representativos de la distribución posterior de \\(\\theta\\). Para proponer los saltos utiliza una Normal(0, 5). Grafica los valores de la cadena para cada paso. Elimina los valores correspondientes a la etapa de calentamiento y realiza un histograma de la distribución posterior. Si calcularas la posterior de manera analítica obtendrías que \\(p(x|\\theta)\\) es normal con media: \\[\\frac{\\sigma^2}{\\sigma^2 + N\\tau^2}\\mu + \\frac{N\\tau^2}{\\sigma^2 + N \\tau^2}\\bar{x}\\] y varianza \\[\\frac{\\sigma^2 \\tau^2}{\\sigma^2 + N\\tau^2}\\] donde \\(\\bar{x}=1/N\\sum_{i=1}^N x_i\\) es la media de los valores observados. Realiza simulaciones de la distribución posterior calculada de manera analítica y comparalas con el histograma de los valores generados con Metropolis. ¿Cómo utilizarías los parámetros \\(\\mu, \\tau^2\\) para describir un escenario donde sabes poco del verdadero valor de la media \\(\\theta\\)? "],
["referencias.html", "Referencias", " Referencias "]
]
